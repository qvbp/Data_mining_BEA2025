import os
import json
import torch
import torch.nn as nn
import torch.nn.functional as F
import wandb
import random
import argparse
import numpy as np
from tqdm import tqdm
from transformers import AutoModel, AutoTokenizer, AdamW, AutoConfig
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


class BAE2025Dataset(Dataset):
    def __init__(
            self,
            data_path,
            label_type="Actionability",  # æ ¹æ®éœ€è¦å¯ä»¥æ˜¯ "Mistake_Identification", "Mistake_Location", "Providing_Guidance", "Actionability"
            labels={
                "Yes": 0,
                "To some extent": 1, 
                "No": 2,
            }
    ):
        self.data_path = data_path
        self.label_type = label_type
        self.labels = labels
        self._get_data()
    
    def _get_data(self):
        with open(self.data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        self.data = []
        for item in data:
            sent1 = item['conversation_history']
            sent2 = item['response']
            
            # æ£€æŸ¥itemä¸­æ˜¯å¦ç›´æ¥åŒ…å«æˆ‘ä»¬éœ€è¦çš„æ ‡ç­¾
            if self.label_type in item and item[self.label_type] in self.labels:
                self.data.append(((sent1, sent2), self.labels[item[self.label_type]]))
    
    def __len__(self):
        return len(self.data)
    
    def get_labels(self):
        return self.labels

    def __getitem__(self, idx):
        return self.data[idx]


class BAE2025DataLoader:
    def __init__(
        self,
        dataset,
        batch_size=16,
        max_length=512,
        shuffle=True,
        drop_last=True,
        device=None,
        tokenizer_name='/mnt/cfs/huangzhiwei/BAE2025/models/deberta-v3-base'
    ):
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
        self.tokenizer.truncation_side = 'left'  # è®¾ç½®æˆªæ–­æ–¹å‘ä¸ºå·¦ä¾§
        print("å½“å‰ä½¿ç”¨çš„ tokenizer ç±»å‹ï¼š", type(self.tokenizer))
        
        self.dataset = dataset
        self.batch_size = batch_size
        self.max_length = max_length
        self.shuffle = shuffle
        self.drop_last = drop_last

        if device is None:
            self.device = torch.device(
                'cuda' if torch.cuda.is_available() else 'cpu'
            )
        else:
            self.device = device

        self.loader = DataLoader(
            dataset=self.dataset,
            batch_size=self.batch_size,
            collate_fn=self.collate_fn,
            shuffle=self.shuffle,
            drop_last=self.drop_last
        )

    def collate_fn(self, data):
        sents = [i[0] for i in data]
        labels = [i[1] for i in data]

        # å¤„ç†ä¸¤ä¸ªå¥å­çš„æƒ…å†µ
        data = self.tokenizer.batch_encode_plus(
            batch_text_or_text_pairs=[(sent[0], sent[1]) for sent in sents],
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt',
            return_length=True
        )
        input_ids = data['input_ids'].to(self.device)
        attention_mask = data['attention_mask'].to(self.device)
        labels = torch.LongTensor(labels).to(self.device)

        return input_ids, attention_mask, labels

    def __iter__(self):
        for data in self.loader:
            yield data

    def __len__(self):
        return len(self.loader)


class MLPExpert(nn.Module):
    """MLPä¸“å®¶æ¨¡å‹"""
    def __init__(self, input_size, hidden_size, num_layers=2, dropout=0.1):
        super().__init__()
        layers = []
        current_size = input_size
        
        # æ„å»ºå¤šå±‚MLP
        for i in range(num_layers):
            if i == num_layers - 1:  # æœ€åä¸€å±‚
                layers.append(nn.Linear(current_size, hidden_size))
            else:
                layers.append(nn.Linear(current_size, hidden_size))
                layers.append(nn.LayerNorm(hidden_size))
                layers.append(nn.GELU())
                layers.append(nn.Dropout(dropout))
                current_size = hidden_size
                
        self.mlp = nn.Sequential(*layers)
        
    def forward(self, x):
        # x: [batch_size, seq_len, input_size] æˆ– [batch_size, input_size]
        if len(x.shape) == 3:
            # å¦‚æœæ˜¯åºåˆ—è¾“å…¥ï¼Œä½¿ç”¨å¹³å‡æ± åŒ–
            x = torch.mean(x, dim=1)  # [batch_size, input_size]
        return self.mlp(x)  # [batch_size, hidden_size]


class BertClassificationHead(nn.Module):
    def __init__(self, hidden_size=1024, num_classes=3, dropout_prob=0.3):
        super().__init__()
        self.dense = nn.Linear(hidden_size, hidden_size)
        self.dropout = nn.Dropout(dropout_prob)
        self.out_proj = nn.Linear(hidden_size, num_classes)
    
    def forward(self, features):
        # æå– [CLS] æ ‡è®°çš„è¡¨ç¤º
        x = features[:, 0, :]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ ‡è®°([CLS])
        x = self.dropout(x)
        x = self.dense(x)
        x = torch.tanh(x)
        x = self.dropout(x)
        x = self.out_proj(x)
        return x


class DynamicMoERouter(nn.Module):
    """åŠ¨æ€ä¸“å®¶è·¯ç”±å™¨ï¼Œæ”¯æŒtopKé€‰æ‹©"""
    def __init__(self, input_size, num_experts, top_k=2):
        super().__init__()
        self.num_experts = num_experts
        self.top_k = min(top_k, num_experts)  # ç¡®ä¿top_kä¸è¶…è¿‡ä¸“å®¶æ•°é‡
        self.router = nn.Linear(input_size, num_experts)
        
    def forward(self, x):
        # x: [batch_size, input_size]
        batch_size = x.size(0)
        
        # è®¡ç®—è·¯ç”±logits
        router_logits = self.router(x)  # [batch_size, num_experts]
        
        # è®¡ç®—æ‰€æœ‰ä¸“å®¶çš„softmaxæ¦‚ç‡ï¼ˆç”¨äºè´Ÿè½½å‡è¡¡æŸå¤±ï¼‰
        all_probs = F.softmax(router_logits, dim=-1)  # [batch_size, num_experts]
        
        # é€‰æ‹©topKä¸ªä¸“å®¶
        top_k_logits, top_k_indices = torch.topk(router_logits, self.top_k, dim=-1)
        
        # å¯¹é€‰ä¸­çš„ä¸“å®¶åº”ç”¨softmax
        top_k_probs = F.softmax(top_k_logits, dim=-1)  # [batch_size, top_k]
        
        # åˆ›å»ºç¨€ç–çš„è·¯ç”±æƒé‡çŸ©é˜µ
        routing_weights = torch.zeros_like(router_logits)  # [batch_size, num_experts]
        routing_weights.scatter_(1, top_k_indices, top_k_probs)
        
        return routing_weights, top_k_indices, all_probs  # [batch_size, num_experts], [batch_size, top_k], [batch_size, num_experts]


class DeBERTaMoEClassifier(nn.Module):
    def __init__(
        self, 
        pretrained_model_name, 
        num_classes=3, 
        freeze_pooler=0,
        expert_hidden_size=256,
        dropout=0.3,
        num_experts=16,  # æ–°å¢ï¼šä¸“å®¶æ•°é‡
        top_k=4,  # æ–°å¢ï¼šæ¯æ¬¡é€‰æ‹©çš„ä¸“å®¶æ•°é‡
        expert_mlp_layers=2,  # æ–°å¢ï¼šä¸“å®¶MLPå±‚æ•°
        load_balance_weight=0.01,  # æ–°å¢ï¼šè´Ÿè½½å‡è¡¡æŸå¤±æƒé‡
    ):
        super().__init__()
        
        # ä½¿ç”¨ AutoModel åŠ è½½ DeBERTa æ¨¡å‹
        self.bert = AutoModel.from_pretrained(pretrained_model_name)
        
        # è·å– bert éšè—å±‚å¤§å°
        self.bert_hidden_size = self.bert.config.hidden_size
        self.num_experts = num_experts
        self.top_k = top_k
        self.load_balance_weight = load_balance_weight  # è´Ÿè½½å‡è¡¡æŸå¤±æƒé‡
        
        # ä¿ç•™åŸæœ‰çš„åˆ†ç±»å¤´
        self.original_classifier = BertClassificationHead(
            hidden_size=self.bert_hidden_size,
            num_classes=num_classes,
            dropout_prob=dropout
        )
        
        # åˆ›å»ºå¤šä¸ªMLPä¸“å®¶
        self.experts = nn.ModuleList([
            MLPExpert(
                input_size=self.bert_hidden_size,
                hidden_size=expert_hidden_size,
                num_layers=expert_mlp_layers,
                dropout=dropout
            ) for _ in range(num_experts)
        ])
        
        # åˆ›å»ºåŠ¨æ€è·¯ç”±å™¨
        self.router = DynamicMoERouter(
            input_size=self.bert_hidden_size, 
            num_experts=num_experts,
            top_k=top_k
        )
        
        # ä¸“å®¶è¾“å‡ºæ˜ å°„å±‚
        self.expert_output_proj = nn.Linear(expert_hidden_size, num_classes)
        
        # æœ€ç»ˆçš„èåˆå±‚
        # åŸå§‹åˆ†ç±»å¤´ + MoEè¾“å‡º = 2 * num_classes
        combined_dim = num_classes * 2
        self.final_classifier = nn.Sequential(
            nn.Linear(combined_dim, combined_dim // 2),
            nn.LayerNorm(combined_dim // 2),
            nn.Dropout(dropout),
            nn.ReLU(),
            nn.Linear(combined_dim // 2, num_classes)
        )
    
    def compute_load_balance_loss(self, all_probs, top_k_indices):
        """
        è®¡ç®—è´Ÿè½½å‡è¡¡æŸå¤±
        
        Args:
            all_probs: [batch_size, num_experts] - æ‰€æœ‰ä¸“å®¶çš„softmaxæ¦‚ç‡
            top_k_indices: [batch_size, top_k] - é€‰ä¸­çš„ä¸“å®¶ç´¢å¼•
            
        Returns:
            load_balance_loss: æ ‡é‡å¼ é‡
        """
        batch_size = all_probs.size(0)
        
        # 1. è®¡ç®—æ¯ä¸ªä¸“å®¶çš„å¹³å‡é€‰æ‹©æ¦‚ç‡ (importance)
        importance = torch.mean(all_probs, dim=0)  # [num_experts]
        
        # 2. è®¡ç®—æ¯ä¸ªä¸“å®¶è¢«é€‰ä¸­çš„é¢‘ç‡ (frequency)
        # åˆ›å»ºone-hotç¼–ç çŸ©é˜µè¡¨ç¤ºå“ªäº›ä¸“å®¶è¢«é€‰ä¸­
        selection_mask = torch.zeros_like(all_probs)  # [batch_size, num_experts]
        
        # å°†é€‰ä¸­çš„ä¸“å®¶ä½ç½®è®¾ä¸º1
        for i in range(self.top_k):
            expert_indices = top_k_indices[:, i]  # [batch_size]
            selection_mask.scatter_(1, expert_indices.unsqueeze(1), 1.0)
        
        # è®¡ç®—æ¯ä¸ªä¸“å®¶è¢«é€‰ä¸­çš„é¢‘ç‡
        frequency = torch.mean(selection_mask, dim=0)  # [num_experts]
        
        # 3. è®¡ç®—è´Ÿè½½å‡è¡¡æŸå¤± (Switch Transformer style)
        # auxiliary_loss = Î± * num_experts * Î£(f_i * P_i)
        # å…¶ä¸­ f_i æ˜¯ä¸“å®¶iè¢«é€‰æ‹©çš„é¢‘ç‡ï¼ŒP_i æ˜¯ä¸“å®¶içš„å¹³å‡é‡è¦æ€§
        auxiliary_loss = self.num_experts * torch.sum(frequency * importance)
        
        # 4. å¯é€‰ï¼šæ·»åŠ æ–¹å·®æŸå¤±æ¥è¿›ä¸€æ­¥é¼“åŠ±å‡åŒ€åˆ†å¸ƒ
        target_frequency = 1.0 / self.num_experts
        frequency_variance = torch.var(frequency)
        importance_variance = torch.var(importance)
        
        # æ€»è´Ÿè½½å‡è¡¡æŸå¤±
        load_balance_loss = auxiliary_loss + frequency_variance + importance_variance
        
        return load_balance_loss
        
    def forward(self, input_ids, attention_mask, return_loss=True):
        # DeBERTa ç¼–ç 
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask
        )
        
        # è·å–åºåˆ—éšè—çŠ¶æ€
        hidden_states = outputs.last_hidden_state  # [batch_size, seq_len, hidden_size]
        batch_size = hidden_states.size(0)
        
        # è·å–åŸå§‹åˆ†ç±»å¤´ç»“æœ
        original_logits = self.original_classifier(hidden_states)  # [batch_size, num_classes]
        
        # è·å–è·¯ç”±æƒé‡å’Œé€‰ä¸­çš„ä¸“å®¶ç´¢å¼•
        cls_embedding = hidden_states[:, 0]  # [batch_size, hidden_size]
        routing_weights, top_k_indices, all_probs = self.router(cls_embedding)  # è·å–æ‰€æœ‰æ¦‚ç‡ç”¨äºè´Ÿè½½å‡è¡¡æŸå¤±
        
        # MoEå‰å‘ä¼ æ’­
        moe_output = torch.zeros(batch_size, self.expert_output_proj.out_features, device=hidden_states.device)
        
        # åªå¯¹é€‰ä¸­çš„ä¸“å®¶è¿›è¡Œè®¡ç®—
        for i in range(self.top_k):
            # è·å–å½“å‰ä½ç½®æ‰€æœ‰æ ·æœ¬é€‰ä¸­çš„ä¸“å®¶ç´¢å¼•
            expert_idx = top_k_indices[:, i]  # [batch_size]
            
            # ä¸ºæ¯ä¸ªæ ·æœ¬è®¡ç®—å¯¹åº”ä¸“å®¶çš„è¾“å‡º
            for batch_idx in range(batch_size):
                current_expert_idx = expert_idx[batch_idx].item()
                current_weight = routing_weights[batch_idx, current_expert_idx]
                
                # è®¡ç®—ä¸“å®¶è¾“å‡º
                expert_hidden = self.experts[current_expert_idx](hidden_states[batch_idx:batch_idx+1])  # [1, expert_hidden_size]
                expert_logits = self.expert_output_proj(expert_hidden)  # [1, num_classes]
                
                # åŠ æƒç´¯åŠ 
                moe_output[batch_idx] += current_weight * expert_logits.squeeze(0)
        
        # æ‹¼æ¥åŸå§‹åˆ†ç±»å¤´å’ŒMoEè¾“å‡º
        combined_logits = torch.cat([original_logits, moe_output], dim=1)  # [batch_size, 2*num_classes]
        
        # é€šè¿‡æœ€ç»ˆåˆ†ç±»å™¨è¾“å‡ºæœ€ç»ˆç»“æœ
        final_logits = self.final_classifier(combined_logits)
        
        # è®¡ç®—è´Ÿè½½å‡è¡¡æŸå¤±
        if return_loss and self.training:
            load_balance_loss = self.compute_load_balance_loss(all_probs, top_k_indices)
            return {
                'logits': final_logits,
                'load_balance_loss': load_balance_loss,
                'routing_weights': routing_weights,
                'expert_utilization': torch.mean(all_probs, dim=0)  # ä¸“å®¶å¹³å‡åˆ©ç”¨ç‡
            }
        else:
            return final_logits
    
    def get_expert_utilization(self, input_ids, attention_mask):
        """è·å–ä¸“å®¶ä½¿ç”¨ç»Ÿè®¡ï¼Œç”¨äºåˆ†æè´Ÿè½½å‡è¡¡"""
        with torch.no_grad():
            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
            hidden_states = outputs.last_hidden_state
            cls_embedding = hidden_states[:, 0]
            routing_weights, top_k_indices, all_probs = self.router(cls_embedding)
            
            # ç»Ÿè®¡æ¯ä¸ªä¸“å®¶è¢«é€‰ä¸­çš„æ¬¡æ•°
            expert_counts = torch.zeros(self.num_experts)
            for i in range(self.top_k):
                expert_idx = top_k_indices[:, i]
                for idx in expert_idx:
                    expert_counts[idx.item()] += 1
            
            # è®¡ç®—è´Ÿè½½å‡è¡¡æŸå¤±
            load_balance_loss = self.compute_load_balance_loss(all_probs, top_k_indices)
                    
            return {
                'expert_counts': expert_counts,
                'routing_weights': routing_weights, 
                'top_k_indices': top_k_indices,
                'all_probs': all_probs,
                'load_balance_loss': load_balance_loss,
                'expert_utilization': torch.mean(all_probs, dim=0)  # ä¸“å®¶å¹³å‡åˆ©ç”¨ç‡
            }


def argparser():
    """å‘½ä»¤è¡Œå‚æ•°è§£æ"""
    parser = argparse.ArgumentParser(description='DeBERTa MoE Training')
    
    # æ¨¡å‹ç›¸å…³å‚æ•°
    parser.add_argument('--model_name', type=str, 
                       default='/mnt/cfs/huangzhiwei/BAE2025/models/deberta-v3-base',
                       help='é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„')
    parser.add_argument('--num_classes', type=int, default=3, help='åˆ†ç±»ç±»åˆ«æ•°')
    parser.add_argument('--dropout', type=float, default=0.1, help='Dropoutç‡')
    parser.add_argument('--freeze_pooler', type=int, default=0, help='æ˜¯å¦å†»ç»“poolerå±‚')
    
    # MoEç›¸å…³å‚æ•°
    parser.add_argument('--num_experts', type=int, default=16, help='ä¸“å®¶æ•°é‡')
    parser.add_argument('--top_k', type=int, default=4, help='é€‰æ‹©çš„ä¸“å®¶æ•°é‡')
    parser.add_argument('--expert_mlp_layers', type=int, default=2, help='ä¸“å®¶MLPå±‚æ•°')
    parser.add_argument('--expert_hidden_size', type=int, default=768, help='ä¸“å®¶éšè—å±‚å¤§å°')
    parser.add_argument('--load_balance_weight', type=float, default=0.01, help='è´Ÿè½½å‡è¡¡æŸå¤±æƒé‡')
    
    # è®­ç»ƒç›¸å…³å‚æ•°
    parser.add_argument('--batch_size', type=int, default=8, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--max_length', type=int, default=512, help='æœ€å¤§åºåˆ—é•¿åº¦')
    parser.add_argument('--lr', type=float, default=3e-5, help='å­¦ä¹ ç‡')
    parser.add_argument('--epochs', type=int, default=50, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--patience', type=int, default=6, help='æ—©åœè€å¿ƒå€¼')
    parser.add_argument('--weight_decay', type=float, default=0.01, help='æƒé‡è¡°å‡')
    parser.add_argument('--warmup_ratio', type=float, default=0.1, help='é¢„çƒ­æ¯”ä¾‹')
    
    # æ•°æ®ç›¸å…³å‚æ•°
    parser.add_argument('--data_path', type=str, default='../data_new/all.json', help='è®­ç»ƒæ•°æ®è·¯å¾„')
    parser.add_argument('--val_data_path', type=str, default='../data_new/val.json', help='éªŒè¯æ•°æ®è·¯å¾„')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
    parser.add_argument('--device', type=str, default=None, help='è®¾å¤‡')
    parser.add_argument('--name', type=str, default=None, help='å®éªŒåç§°')
    parser.add_argument('--checkpoint_dir', type=str, default='checkpoints_track4', help='æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•')
    
    return parser.parse_args()





def train(configs):
    # è®¾ç½®éšæœºç§å­
    random.seed(configs.seed)
    np.random.seed(configs.seed)
    torch.manual_seed(configs.seed)
    
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    # åˆ›å»ºæ£€æŸ¥ç‚¹ç›®å½•
    checkpoint_dir = os.path.join(configs.checkpoint_dir, configs.exp_name)
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    # ä¸ºä¿å­˜æ··æ·†çŸ©é˜µåˆ›å»ºç›®å½• - åˆ†åˆ«ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†åˆ›å»º
    train_plot_dir = os.path.join(checkpoint_dir, 'plots', 'train')
    val_plot_dir = os.path.join(checkpoint_dir, 'plots', 'val')
    os.makedirs(train_plot_dir, exist_ok=True)
    os.makedirs(val_plot_dir, exist_ok=True)
    
    # åŠ è½½æ•°æ®é›†
    train_dataset = BAE2025Dataset(configs.data_path)
    val_dataset = BAE2025Dataset(configs.val_data_path)    

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataloader = BAE2025DataLoader(
        dataset=train_dataset,
        batch_size=configs.batch_size,
        max_length=configs.max_length,
        shuffle=True,
        drop_last=True,
        device=configs.device,
        tokenizer_name=configs.model_name
    )

    val_dataloader = BAE2025DataLoader(
        dataset=val_dataset,
        batch_size=configs.batch_size,
        max_length=configs.max_length,
        shuffle=False,
        drop_last=False,
        device=configs.device,
        tokenizer_name=configs.model_name
    )
    
    # åˆ›å»ºMoEæ¨¡å‹
    model = DeBERTaMoEClassifier(
        pretrained_model_name=configs.model_name,
        num_classes=configs.num_classes,
        freeze_pooler=configs.freeze_pooler,
        expert_hidden_size=configs.expert_hidden_size,
        dropout=configs.dropout,
        num_experts=configs.num_experts,
        top_k=configs.top_k,
        expert_mlp_layers=configs.expert_mlp_layers,
        load_balance_weight=configs.load_balance_weight
    ).to(configs.device)

    criterion = nn.CrossEntropyLoss()

    # å®šä¹‰ä¼˜åŒ–å™¨
    optimizer = AdamW(
        filter(lambda p: p.requires_grad, model.parameters()),
        lr=configs.lr,
        weight_decay=configs.weight_decay
    )
    
    # åˆå§‹åŒ–æœ€ä½³éªŒè¯æŸå¤±å’Œæ—©åœè®¡æ•°å™¨
    best_val_acc = 0.0
    best_val_f1 = 0.0  # æ·»åŠ F1åˆ†æ•°ä½œä¸ºè¯„ä¼°æŒ‡æ ‡
    best_val_loss = float('inf')
    patience_counter = 0
    
    # åˆå§‹åŒ–æœ€ä½³æŒ‡æ ‡è®°å½•
    best_metrics = {
        'epoch': 0,
        'val_f1': 0.0,
        'val_acc': 0.0,
        'val_loss': float('inf')
    }

    # å®šä¹‰ç±»åˆ«åç§°
    class_names = ['Yes', 'To some extent', 'No']
    
    print(f"å¼€å§‹è®­ç»ƒMoEæ¨¡å‹...")
    print(f"ä¸“å®¶æ•°é‡: {configs.num_experts}, Top-K: {configs.top_k}")
    print(f"è´Ÿè½½å‡è¡¡æƒé‡: {configs.load_balance_weight}")
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(configs.epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        train_load_balance_loss = 0.0
        train_acc = 0.0
        train_preds = []
        train_labels_list = []
        
        with tqdm(
            train_dataloader,
            total=len(train_dataloader),
            desc=f'Epoch {epoch + 1}/{configs.epochs}',
            unit='batch',
            ncols=120
        ) as pbar:
            for input_ids, attention_mask, labels in pbar:
                optimizer.zero_grad()
                
                # å‰å‘ä¼ æ’­ - è·å–MoEè¾“å‡º
                outputs = model(input_ids, attention_mask, return_loss=True)
                
                if isinstance(outputs, dict):
                    # è®­ç»ƒæ¨¡å¼ï¼ŒåŒ…å«è´Ÿè½½å‡è¡¡æŸå¤±
                    logits = outputs['logits']
                    load_balance_loss = outputs['load_balance_loss']
                    expert_utilization = outputs['expert_utilization']
                else:
                    # æ¨ç†æ¨¡å¼ï¼Œåªæœ‰logits
                    logits = outputs
                    load_balance_loss = torch.tensor(0.0, device=logits.device)
                
                # è®¡ç®—åˆ†ç±»æŸå¤±
                labels = labels.long()
                classification_loss = criterion(logits, labels)
                
                # æ€»æŸå¤± = åˆ†ç±»æŸå¤± + è´Ÿè½½å‡è¡¡æŸå¤±
                total_loss = classification_loss + model.load_balance_weight * load_balance_loss
                
                # åå‘ä¼ æ’­
                total_loss.backward()
                optimizer.step()
                
                preds = logits.argmax(dim=1)
                accuracy = (preds == labels).float().mean()
                accuracy_all = (preds == labels).float().sum()
                
                # æ”¶é›†é¢„æµ‹ç»“æœå’ŒçœŸå®æ ‡ç­¾ï¼Œç”¨äºè®¡ç®—F1
                train_preds.extend(preds.cpu().numpy())
                train_labels_list.extend(labels.cpu().numpy())
                
                train_loss += classification_loss.item()
                train_load_balance_loss += load_balance_loss.item()
                train_acc += accuracy_all.item()
                
                # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤º
                pbar.set_postfix(
                    cls_loss=f'{classification_loss.item():.3f}',
                    lb_loss=f'{load_balance_loss.item():.4f}',
                    acc=f'{accuracy.item():.3f}'
                )
        
        train_loss = train_loss / len(train_dataloader)
        train_load_balance_loss = train_load_balance_loss / len(train_dataloader)
        train_acc = train_acc / len(train_dataset)
        
        # è®¡ç®—è®­ç»ƒé›†çš„F1åˆ†æ•° - ä½¿ç”¨macroå¹³å‡ä»¥å¤„ç†å¤šåˆ†ç±»
        train_f1 = f1_score(train_labels_list, train_preds, average='macro')
        
        print(f'Training - Classification Loss: {train_loss:.4f}, Load Balance Loss: {train_load_balance_loss:.4f}')
        print(f'Training - Accuracy: {train_acc:.4f}, F1 Score: {train_f1:.4f}')
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        val_corrects = 0.0
        val_preds = []
        val_labels_list = []

        with torch.no_grad():
            for input_ids, attention_mask, labels in val_dataloader:
                # ç¡®ä¿labelsæ˜¯é•¿æ•´å‹
                labels = labels.long()
                
                # å‰å‘ä¼ æ’­ - æ¨ç†æ¨¡å¼
                logits = model(input_ids, attention_mask, return_loss=False)
                
                loss = criterion(logits, labels)
                val_loss += loss.item()
                preds = logits.argmax(dim=1)
                accuracy = (preds == labels).float().sum()
                val_corrects += accuracy
                
                # æ”¶é›†é¢„æµ‹ç»“æœå’ŒçœŸå®æ ‡ç­¾ï¼Œç”¨äºè®¡ç®—F1å’Œæ··æ·†çŸ©é˜µ
                val_preds.extend(preds.cpu().numpy())
                val_labels_list.extend(labels.cpu().numpy())
        
        val_loss = val_loss / len(val_dataloader)
        val_acc = val_corrects.double() / len(val_dataset)
        
        # è®¡ç®—éªŒè¯é›†çš„F1åˆ†æ•°
        val_f1 = f1_score(val_labels_list, val_preds, average='macro')
        
        print('Validation - Loss: {:.4f}, Acc: {:.4f}, F1: {:.4f}'.format(val_loss, val_acc, val_f1))

        # åˆ†æä¸“å®¶ä½¿ç”¨æƒ…å†µï¼ˆå¯é€‰ï¼Œä»…åœ¨æŸäº›epochæ‰§è¡Œä»¥èŠ‚çœæ—¶é—´ï¼‰
        if epoch % 5 == 0:  # æ¯5ä¸ªepochåˆ†æä¸€æ¬¡
            # ä½¿ç”¨éªŒè¯é›†çš„ä¸€ä¸ªbatchæ¥åˆ†æä¸“å®¶ä½¿ç”¨
            sample_input_ids, sample_attention_mask, _ = next(iter(val_dataloader))
            expert_stats = model.get_expert_utilization(sample_input_ids, sample_attention_mask)
            expert_counts = expert_stats['expert_counts']
            expert_utilization = expert_stats['expert_utilization']
            
            print(f"ä¸“å®¶ä½¿ç”¨ç»Ÿè®¡ (Epoch {epoch+1}):")
            print(f"  ä¸“å®¶é€‰æ‹©æ¬¡æ•°: {expert_counts.numpy()}")
            print(f"  ä¸“å®¶å¹³å‡åˆ©ç”¨ç‡: {expert_utilization.cpu().numpy()}")
            print(f"  åˆ©ç”¨ç‡æ ‡å‡†å·®: {torch.std(expert_utilization).item():.4f}")
        
        # æ£€æŸ¥æ˜¯å¦ä¿å­˜æ¨¡å‹å¹¶åˆ¤æ–­æ˜¯å¦éœ€è¦æ—©åœ
        # ä½¿ç”¨F1åˆ†æ•°ä½œä¸ºä¸»è¦æŒ‡æ ‡
        if val_f1 > best_val_f1:
            best_val_f1 = val_f1
            best_val_acc = val_acc
            best_val_loss = val_loss
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            state_dict = model.state_dict()
            torch.save(state_dict, os.path.join(checkpoint_dir, 'best_model_f1.pt'))
            print(f'ğŸ‰ New best model saved! F1: {best_val_f1:.4f}, Acc: {best_val_acc:.4f}')
            
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= configs.patience:
                print(f'ğŸ›‘ Early stopping triggered after {epoch+1} epochs.')
                break

        model.train()

    print("è®­ç»ƒå®Œæˆ!")
    print(f"æœ€ä½³ç»“æœ - F1: {best_val_f1:.4f}, Acc: {best_val_acc:.4f}, Loss: {best_val_loss:.4f}")


# ä¸»å‡½æ•°
if __name__ == '__main__':
    # ä½¿ç”¨argparseè§£æå‘½ä»¤è¡Œå‚æ•°
    configs = argparser()
    
    # è®¾ç½®å®éªŒåç§°
    if configs.name is None:
        configs.exp_name = \
            f'{os.path.basename(configs.model_name)}' + \
            f'{"_fp" if configs.freeze_pooler else ""}' + \
            f'_moe{configs.num_experts}k{configs.top_k}' + \
            f'_b{configs.batch_size}_e{configs.epochs}' + \
            f'_len{configs.max_length}_lr{configs.lr}'
    else:
        configs.exp_name = configs.name
    
    # è®¾ç½®è®¾å¤‡
    if configs.device is None:
        configs.device = torch.device(
            'cuda' if torch.cuda.is_available() else 'cpu'
        )
    
    print(f"ä½¿ç”¨è®¾å¤‡: {configs.device}")
    print(f"å®éªŒåç§°: {configs.exp_name}")
    
    # è°ƒç”¨è®­ç»ƒå‡½æ•°
    train(configs)