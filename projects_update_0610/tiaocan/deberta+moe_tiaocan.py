"""
DeBERTa MoEå•ä»»åŠ¡å­¦ä¹  - è´å¶æ–¯è°ƒå‚ç‰ˆæœ¬

ä½¿ç”¨Optunaè¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–ï¼Œè‡ªåŠ¨æœç´¢æœ€ä½³å‚æ•°ç»„åˆ
è°ƒå‚å‚æ•°: num_experts, top_k, expert_mlp_layers, expert_hidden_size, 
         load_balance_weight, batch_size, lr, weight_decay, seed

è¿è¡Œæ–¹æ³•:
python train_single_task_bayesian.py
"""

import os
import json
import torch
import torch.nn as nn
import torch.nn.functional as F
import random
import numpy as np
import optuna
import pickle
from datetime import datetime
from tqdm import tqdm
from transformers import AutoModel, AutoTokenizer, AdamW
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import f1_score, accuracy_score

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# å›ºå®šé…ç½®
FIXED_CONFIG = {
    'model_name': '/mnt/cfs/huangzhiwei/BAE2025/models/deberta-v3-base',
    'num_classes': 3,
    'freeze_pooler': 0,
    'main_task': 'Actionability',  # å¯ä»¥æ”¹ä¸ºå…¶ä»–ä»»åŠ¡ï¼š'Mistake_Identification', 'Mistake_Location', 'Providing_Guidance'
    'max_length': 512,
    'epochs': 40,  # è°ƒå‚æ—¶å‡å°‘epochæ•°ä»¥èŠ‚çœæ—¶é—´
    'patience': 8,
    'warmup_ratio': 0.1,
    'data_path': '/mnt/cfs/huangzhiwei/Data_mining_BAE2025/data_new/train.json',
    'val_data_path': '/mnt/cfs/huangzhiwei/Data_mining_BAE2025/data_new/val.json',
    'checkpoint_dir': '/mnt/cfs/huangzhiwei/Data_mining_BAE2025/projects_update_0610/tiaocan/single_task_bayesian_optimization_results',
    'device': device
}


class BAE2025Dataset(Dataset):
    def __init__(
            self,
            data_path,
            label_type="Actionability",
            labels={
                "Yes": 0,
                "To some extent": 1, 
                "No": 2,
            }
    ):
        self.data_path = data_path
        self.label_type = label_type
        self.labels = labels
        self._get_data()
    
    def _get_data(self):
        with open(self.data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        self.data = []
        for item in data:
            if 'conversation_history' in item and 'response' in item:
                sent1 = item['conversation_history']
                sent2 = item['response']
                
                # æ£€æŸ¥itemä¸­æ˜¯å¦ç›´æ¥åŒ…å«æˆ‘ä»¬éœ€è¦çš„æ ‡ç­¾
                if self.label_type in item and item[self.label_type] in self.labels:
                    self.data.append(((sent1, sent2), self.labels[item[self.label_type]]))
    
    def __len__(self):
        return len(self.data)
    
    def get_labels(self):
        return self.labels

    def __getitem__(self, idx):
        return self.data[idx]


class BAE2025DataLoader:
    def __init__(
        self,
        dataset,
        batch_size=16,
        max_length=512,
        shuffle=True,
        drop_last=True,
        device=None,
        tokenizer_name='/mnt/cfs/huangzhiwei/BAE2025/models/deberta-v3-base'
    ):
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
        self.tokenizer.truncation_side = 'left'  # è®¾ç½®æˆªæ–­æ–¹å‘ä¸ºå·¦ä¾§
        
        self.dataset = dataset
        self.batch_size = batch_size
        self.max_length = max_length
        self.shuffle = shuffle
        self.drop_last = drop_last

        if device is None:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = device

        self.loader = DataLoader(
            dataset=self.dataset,
            batch_size=self.batch_size,
            collate_fn=self.collate_fn,
            shuffle=self.shuffle,
            drop_last=self.drop_last
        )

    def collate_fn(self, data):
        sents = [i[0] for i in data]
        labels = [i[1] for i in data]

        # å¤„ç†ä¸¤ä¸ªå¥å­çš„æƒ…å†µ
        data = self.tokenizer.batch_encode_plus(
            batch_text_or_text_pairs=[(sent[0], sent[1]) for sent in sents],
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt',
            return_length=True
        )
        input_ids = data['input_ids'].to(self.device)
        attention_mask = data['attention_mask'].to(self.device)
        labels = torch.LongTensor(labels).to(self.device)

        return input_ids, attention_mask, labels

    def __iter__(self):
        for data in self.loader:
            yield data

    def __len__(self):
        return len(self.loader)


class MLPExpert(nn.Module):
    """MLPä¸“å®¶æ¨¡å‹"""
    def __init__(self, input_size, hidden_size, num_layers=2, dropout=0.1):
        super().__init__()
        layers = []
        current_size = input_size
        
        # æ„å»ºå¤šå±‚MLP
        for i in range(num_layers):
            if i == num_layers - 1:  # æœ€åä¸€å±‚
                layers.append(nn.Linear(current_size, hidden_size))
            else:
                layers.append(nn.Linear(current_size, hidden_size))
                layers.append(nn.LayerNorm(hidden_size))
                layers.append(nn.GELU())
                layers.append(nn.Dropout(dropout))
                current_size = hidden_size
                
        self.mlp = nn.Sequential(*layers)
        
    def forward(self, x):
        # x: [batch_size, seq_len, input_size] æˆ– [batch_size, input_size]
        if len(x.shape) == 3:
            # å¦‚æœæ˜¯åºåˆ—è¾“å…¥ï¼Œä½¿ç”¨å¹³å‡æ± åŒ–
            x = torch.mean(x, dim=1)  # [batch_size, input_size]
        return self.mlp(x)  # [batch_size, hidden_size]


class BertClassificationHead(nn.Module):
    def __init__(self, hidden_size=1024, num_classes=3, dropout_prob=0.3):
        super().__init__()
        self.dense = nn.Linear(hidden_size, hidden_size)
        self.dropout = nn.Dropout(dropout_prob)
        self.out_proj = nn.Linear(hidden_size, num_classes)
    
    def forward(self, features):
        # æå– [CLS] æ ‡è®°çš„è¡¨ç¤º
        x = features[:, 0, :]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ ‡è®°([CLS])
        x = self.dropout(x)
        x = self.dense(x)
        x = torch.tanh(x)
        x = self.dropout(x)
        x = self.out_proj(x)
        return x


class DynamicMoERouter(nn.Module):
    """åŠ¨æ€ä¸“å®¶è·¯ç”±å™¨ï¼Œæ”¯æŒtopKé€‰æ‹©"""
    def __init__(self, input_size, num_experts, top_k=2):
        super().__init__()
        self.num_experts = num_experts
        self.top_k = min(top_k, num_experts)  # ç¡®ä¿top_kä¸è¶…è¿‡ä¸“å®¶æ•°é‡
        self.router = nn.Linear(input_size, num_experts)
        
    def forward(self, x):
        # x: [batch_size, input_size]
        batch_size = x.size(0)
        
        # è®¡ç®—è·¯ç”±logits
        router_logits = self.router(x)  # [batch_size, num_experts]
        
        # è®¡ç®—æ‰€æœ‰ä¸“å®¶çš„softmaxæ¦‚ç‡ï¼ˆç”¨äºè´Ÿè½½å‡è¡¡æŸå¤±ï¼‰
        all_probs = F.softmax(router_logits, dim=-1)  # [batch_size, num_experts]
        
        # é€‰æ‹©topKä¸ªä¸“å®¶
        top_k_logits, top_k_indices = torch.topk(router_logits, self.top_k, dim=-1)
        
        # å¯¹é€‰ä¸­çš„ä¸“å®¶åº”ç”¨softmax
        top_k_probs = F.softmax(top_k_logits, dim=-1)  # [batch_size, top_k]
        
        # åˆ›å»ºç¨€ç–çš„è·¯ç”±æƒé‡çŸ©é˜µ
        routing_weights = torch.zeros_like(router_logits)  # [batch_size, num_experts]
        routing_weights.scatter_(1, top_k_indices, top_k_probs)
        
        return routing_weights, top_k_indices, all_probs  # [batch_size, num_experts], [batch_size, top_k], [batch_size, num_experts]


class DeBERTaMoEClassifier(nn.Module):
    def __init__(
        self, 
        pretrained_model_name, 
        num_classes=3, 
        freeze_pooler=0,
        expert_hidden_size=256,
        dropout=0.3,
        num_experts=16,  # æ–°å¢ï¼šä¸“å®¶æ•°é‡
        top_k=4,  # æ–°å¢ï¼šæ¯æ¬¡é€‰æ‹©çš„ä¸“å®¶æ•°é‡
        expert_mlp_layers=2,  # æ–°å¢ï¼šä¸“å®¶MLPå±‚æ•°
        load_balance_weight=0.01,  # æ–°å¢ï¼šè´Ÿè½½å‡è¡¡æŸå¤±æƒé‡
    ):
        super().__init__()
        
        # ä½¿ç”¨ AutoModel åŠ è½½ DeBERTa æ¨¡å‹
        self.bert = AutoModel.from_pretrained(pretrained_model_name)
        
        # è·å– bert éšè—å±‚å¤§å°
        self.bert_hidden_size = self.bert.config.hidden_size
        self.num_experts = num_experts
        self.top_k = min(top_k, num_experts)  # ç¡®ä¿top_kä¸è¶…è¿‡num_experts
        self.load_balance_weight = load_balance_weight  # è´Ÿè½½å‡è¡¡æŸå¤±æƒé‡
        
        # ä¿ç•™åŸæœ‰çš„åˆ†ç±»å¤´
        self.original_classifier = BertClassificationHead(
            hidden_size=self.bert_hidden_size,
            num_classes=num_classes,
            dropout_prob=dropout
        )
        
        # åˆ›å»ºå¤šä¸ªMLPä¸“å®¶
        self.experts = nn.ModuleList([
            MLPExpert(
                input_size=self.bert_hidden_size,
                hidden_size=expert_hidden_size,
                num_layers=expert_mlp_layers,
                dropout=dropout
            ) for _ in range(num_experts)
        ])
        
        # åˆ›å»ºåŠ¨æ€è·¯ç”±å™¨
        self.router = DynamicMoERouter(
            input_size=self.bert_hidden_size, 
            num_experts=num_experts,
            top_k=self.top_k
        )
        
        # ä¸“å®¶è¾“å‡ºæ˜ å°„å±‚
        self.expert_output_proj = nn.Linear(expert_hidden_size, num_classes)
        
        # æœ€ç»ˆçš„èåˆå±‚
        # åŸå§‹åˆ†ç±»å¤´ + MoEè¾“å‡º = 2 * num_classes
        combined_dim = num_classes * 2
        self.final_classifier = nn.Sequential(
            nn.Linear(combined_dim, combined_dim // 2),
            nn.LayerNorm(combined_dim // 2),
            nn.Dropout(dropout),
            nn.ReLU(),
            nn.Linear(combined_dim // 2, num_classes)
        )
    
    def compute_load_balance_loss(self, all_probs, top_k_indices):
        """
        è®¡ç®—è´Ÿè½½å‡è¡¡æŸå¤±
        
        Args:
            all_probs: [batch_size, num_experts] - æ‰€æœ‰ä¸“å®¶çš„softmaxæ¦‚ç‡
            top_k_indices: [batch_size, top_k] - é€‰ä¸­çš„ä¸“å®¶ç´¢å¼•
            
        Returns:
            load_balance_loss: æ ‡é‡å¼ é‡
        """
        batch_size = all_probs.size(0)
        
        # 1. è®¡ç®—æ¯ä¸ªä¸“å®¶çš„å¹³å‡é€‰æ‹©æ¦‚ç‡ (importance)
        importance = torch.mean(all_probs, dim=0)  # [num_experts]
        
        # 2. è®¡ç®—æ¯ä¸ªä¸“å®¶è¢«é€‰ä¸­çš„é¢‘ç‡ (frequency)
        # åˆ›å»ºone-hotç¼–ç çŸ©é˜µè¡¨ç¤ºå“ªäº›ä¸“å®¶è¢«é€‰ä¸­
        selection_mask = torch.zeros_like(all_probs)  # [batch_size, num_experts]
        
        # å°†é€‰ä¸­çš„ä¸“å®¶ä½ç½®è®¾ä¸º1
        for i in range(self.top_k):
            expert_indices = top_k_indices[:, i]  # [batch_size]
            selection_mask.scatter_(1, expert_indices.unsqueeze(1), 1.0)
        
        # è®¡ç®—æ¯ä¸ªä¸“å®¶è¢«é€‰ä¸­çš„é¢‘ç‡
        frequency = torch.mean(selection_mask, dim=0)  # [num_experts]
        
        # 3. è®¡ç®—è´Ÿè½½å‡è¡¡æŸå¤± (Switch Transformer style)
        # auxiliary_loss = Î± * num_experts * Î£(f_i * P_i)
        # å…¶ä¸­ f_i æ˜¯ä¸“å®¶iè¢«é€‰æ‹©çš„é¢‘ç‡ï¼ŒP_i æ˜¯ä¸“å®¶içš„å¹³å‡é‡è¦æ€§
        auxiliary_loss = self.num_experts * torch.sum(frequency * importance)
        
        # 4. å¯é€‰ï¼šæ·»åŠ æ–¹å·®æŸå¤±æ¥è¿›ä¸€æ­¥é¼“åŠ±å‡åŒ€åˆ†å¸ƒ
        target_frequency = 1.0 / self.num_experts
        frequency_variance = torch.var(frequency)
        importance_variance = torch.var(importance)
        
        # æ€»è´Ÿè½½å‡è¡¡æŸå¤±
        load_balance_loss = auxiliary_loss + frequency_variance + importance_variance
        
        return load_balance_loss
        
    def forward(self, input_ids, attention_mask, return_loss=True):
        # DeBERTa ç¼–ç 
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask
        )
        
        # è·å–åºåˆ—éšè—çŠ¶æ€
        hidden_states = outputs.last_hidden_state  # [batch_size, seq_len, hidden_size]
        batch_size = hidden_states.size(0)
        
        # è·å–åŸå§‹åˆ†ç±»å¤´ç»“æœ
        original_logits = self.original_classifier(hidden_states)  # [batch_size, num_classes]
        
        # è·å–è·¯ç”±æƒé‡å’Œé€‰ä¸­çš„ä¸“å®¶ç´¢å¼•
        cls_embedding = hidden_states[:, 0]  # [batch_size, hidden_size]
        routing_weights, top_k_indices, all_probs = self.router(cls_embedding)  # è·å–æ‰€æœ‰æ¦‚ç‡ç”¨äºè´Ÿè½½å‡è¡¡æŸå¤±
        
        # MoEå‰å‘ä¼ æ’­
        moe_output = torch.zeros(batch_size, self.expert_output_proj.out_features, device=hidden_states.device)
        
        # åªå¯¹é€‰ä¸­çš„ä¸“å®¶è¿›è¡Œè®¡ç®—
        for i in range(self.top_k):
            # è·å–å½“å‰ä½ç½®æ‰€æœ‰æ ·æœ¬é€‰ä¸­çš„ä¸“å®¶ç´¢å¼•
            expert_idx = top_k_indices[:, i]  # [batch_size]
            
            # ä¸ºæ¯ä¸ªæ ·æœ¬è®¡ç®—å¯¹åº”ä¸“å®¶çš„è¾“å‡º
            for batch_idx in range(batch_size):
                current_expert_idx = expert_idx[batch_idx].item()
                current_weight = routing_weights[batch_idx, current_expert_idx]
                
                # è®¡ç®—ä¸“å®¶è¾“å‡º
                expert_hidden = self.experts[current_expert_idx](hidden_states[batch_idx:batch_idx+1])  # [1, expert_hidden_size]
                expert_logits = self.expert_output_proj(expert_hidden)  # [1, num_classes]
                
                # åŠ æƒç´¯åŠ 
                moe_output[batch_idx] += current_weight * expert_logits.squeeze(0)
        
        # æ‹¼æ¥åŸå§‹åˆ†ç±»å¤´å’ŒMoEè¾“å‡º
        combined_logits = torch.cat([original_logits, moe_output], dim=1)  # [batch_size, 2*num_classes]
        
        # é€šè¿‡æœ€ç»ˆåˆ†ç±»å™¨è¾“å‡ºæœ€ç»ˆç»“æœ
        final_logits = self.final_classifier(combined_logits)
        
        # è®¡ç®—è´Ÿè½½å‡è¡¡æŸå¤±
        if return_loss and self.training:
            load_balance_loss = self.compute_load_balance_loss(all_probs, top_k_indices)
            return {
                'logits': final_logits,
                'load_balance_loss': load_balance_loss,
                'routing_weights': routing_weights,
                'expert_utilization': torch.mean(all_probs, dim=0)  # ä¸“å®¶å¹³å‡åˆ©ç”¨ç‡
            }
        else:
            return final_logits
    
    def get_expert_utilization(self, input_ids, attention_mask):
        """è·å–ä¸“å®¶ä½¿ç”¨ç»Ÿè®¡ï¼Œç”¨äºåˆ†æè´Ÿè½½å‡è¡¡"""
        with torch.no_grad():
            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
            hidden_states = outputs.last_hidden_state
            cls_embedding = hidden_states[:, 0]
            routing_weights, top_k_indices, all_probs = self.router(cls_embedding)
            
            # ç»Ÿè®¡æ¯ä¸ªä¸“å®¶è¢«é€‰ä¸­çš„æ¬¡æ•°
            expert_counts = torch.zeros(self.num_experts)
            for i in range(self.top_k):
                expert_idx = top_k_indices[:, i]
                for idx in expert_idx:
                    expert_counts[idx.item()] += 1
            
            # è®¡ç®—è´Ÿè½½å‡è¡¡æŸå¤±
            load_balance_loss = self.compute_load_balance_loss(all_probs, top_k_indices)
                    
            return {
                'expert_counts': expert_counts,
                'routing_weights': routing_weights, 
                'top_k_indices': top_k_indices,
                'all_probs': all_probs,
                'load_balance_loss': load_balance_loss,
                'expert_utilization': torch.mean(all_probs, dim=0)  # ä¸“å®¶å¹³å‡åˆ©ç”¨ç‡
            }


def train_single_trial(params):
    """å•æ¬¡è®­ç»ƒå‡½æ•°ï¼Œè¿”å›æœ€ä½³éªŒè¯F1åˆ†æ•°"""
    
    # è®¾ç½®éšæœºç§å­
    random.seed(params['seed'])
    np.random.seed(params['seed'])
    torch.manual_seed(params['seed'])
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    # åŠ è½½æ•°æ®é›†
    train_dataset = BAE2025Dataset(
        FIXED_CONFIG['data_path'], 
        label_type=FIXED_CONFIG['main_task']
    )
    val_dataset = BAE2025Dataset(
        FIXED_CONFIG['val_data_path'], 
        label_type=FIXED_CONFIG['main_task']
    )

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataloader = BAE2025DataLoader(
        dataset=train_dataset,
        batch_size=params['batch_size'],
        max_length=FIXED_CONFIG['max_length'],
        shuffle=True,
        drop_last=True,
        device=FIXED_CONFIG['device'],
        tokenizer_name=FIXED_CONFIG['model_name']
    )

    val_dataloader = BAE2025DataLoader(
        dataset=val_dataset,
        batch_size=params['batch_size'],
        max_length=FIXED_CONFIG['max_length'],
        shuffle=False,
        drop_last=False,
        device=FIXED_CONFIG['device'],
        tokenizer_name=FIXED_CONFIG['model_name']
    )
    
    # åˆ›å»ºæ¨¡å‹
    model = DeBERTaMoEClassifier(
        pretrained_model_name=FIXED_CONFIG['model_name'],
        num_classes=FIXED_CONFIG['num_classes'],
        freeze_pooler=FIXED_CONFIG['freeze_pooler'],
        expert_hidden_size=params['expert_hidden_size'],
        dropout=params['dropout'],
        num_experts=params['num_experts'],
        top_k=params['top_k'],
        expert_mlp_layers=params['expert_mlp_layers'],
        load_balance_weight=params['load_balance_weight']
    ).to(FIXED_CONFIG['device'])

    criterion = nn.CrossEntropyLoss()
    optimizer = AdamW(
        filter(lambda p: p.requires_grad, model.parameters()),
        lr=params['lr'],
        weight_decay=params['weight_decay']
    )
    
    best_val_f1 = 0.0
    best_val_acc = 0.0
    best_epoch = 0
    patience_counter = 0
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(FIXED_CONFIG['epochs']):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        train_preds = []
        train_labels_list = []
        
        for input_ids, attention_mask, labels in train_dataloader:
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­ - è·å–MoEè¾“å‡º
            outputs = model(input_ids, attention_mask, return_loss=True)
            
            if isinstance(outputs, dict):
                # è®­ç»ƒæ¨¡å¼ï¼ŒåŒ…å«è´Ÿè½½å‡è¡¡æŸå¤±
                logits = outputs['logits']
                load_balance_loss = outputs['load_balance_loss']
            else:
                # æ¨ç†æ¨¡å¼ï¼Œåªæœ‰logits
                logits = outputs
                load_balance_loss = torch.tensor(0.0, device=logits.device)
            
            # è®¡ç®—åˆ†ç±»æŸå¤±
            labels = labels.long()
            classification_loss = criterion(logits, labels)
            
            # æ€»æŸå¤± = åˆ†ç±»æŸå¤± + è´Ÿè½½å‡è¡¡æŸå¤±
            total_loss = classification_loss + model.load_balance_weight * load_balance_loss
            
            # åå‘ä¼ æ’­
            total_loss.backward()
            optimizer.step()
            
            preds = logits.argmax(dim=1)
            
            # æ”¶é›†é¢„æµ‹ç»“æœå’ŒçœŸå®æ ‡ç­¾ï¼Œç”¨äºè®¡ç®—F1
            train_preds.extend(preds.cpu().numpy())
            train_labels_list.extend(labels.cpu().numpy())
            
            train_loss += classification_loss.item()
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        val_preds = []
        val_labels_list = []

        with torch.no_grad():
            for input_ids, attention_mask, labels in val_dataloader:
                # ç¡®ä¿labelsæ˜¯é•¿æ•´å‹
                labels = labels.long()
                
                # å‰å‘ä¼ æ’­ - æ¨ç†æ¨¡å¼
                logits = model(input_ids, attention_mask, return_loss=False)
                
                loss = criterion(logits, labels)
                val_loss += loss.item()
                preds = logits.argmax(dim=1)
                
                # æ”¶é›†é¢„æµ‹ç»“æœå’ŒçœŸå®æ ‡ç­¾ï¼Œç”¨äºè®¡ç®—F1å’Œæ··æ·†çŸ©é˜µ
                val_preds.extend(preds.cpu().numpy())
                val_labels_list.extend(labels.cpu().numpy())
        
        val_f1 = f1_score(val_labels_list, val_preds, average='macro')
        val_acc = accuracy_score(val_labels_list, val_preds)
        
        if val_f1 > best_val_f1:
            best_val_f1 = val_f1
            best_val_acc = val_acc
            best_epoch = epoch + 1
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= FIXED_CONFIG['patience']:
                break

        model.train()
    
    return {
        'best_f1': best_val_f1,
        'best_acc': best_val_acc,
        'best_epoch': best_epoch,
        'params': params
    }


def objective(trial):
    """Optunaç›®æ ‡å‡½æ•°"""
    
    # å®šä¹‰æœç´¢ç©ºé—´
    params = {
        'num_experts': trial.suggest_categorical('num_experts', [4, 8, 16, 32, 64]),
        'expert_mlp_layers': trial.suggest_int('expert_mlp_layers', 1, 4),
        'expert_hidden_size': trial.suggest_categorical('expert_hidden_size', [128, 256, 512, 768, 1024]),
        'load_balance_weight': trial.suggest_float('load_balance_weight', 0.001, 0.1, log=True),
        'batch_size': trial.suggest_categorical('batch_size', [4, 8, 16, 32]),
        'lr': trial.suggest_float('lr', 1e-6, 1e-3, log=True),
        'weight_decay': trial.suggest_float('weight_decay', 1e-5, 1e-1, log=True),
        'seed': trial.suggest_int('seed', 42, 1000),
        'dropout': trial.suggest_float('dropout', 0.1, 0.4, log=True)
    }
    
    # top_kéœ€è¦æ ¹æ®num_expertsåŠ¨æ€è®¾ç½®
    max_top_k = min(8, params['num_experts'])
    params['top_k'] = trial.suggest_int('top_k', 1, max_top_k)
    
    try:
        result = train_single_trial(params)
        
        # è®°å½•ä¸­é—´ç»“æœ
        trial.set_user_attr('best_acc', result['best_acc'])
        trial.set_user_attr('best_epoch', result['best_epoch'])
        trial.set_user_attr('params', result['params'])
        
        return result['best_f1']  # ä¼˜åŒ–ç›®æ ‡ä¸ºF1åˆ†æ•°
        
    except Exception as e:
        print(f"Trial failed: {e}")
        return 0.0  # å¤±è´¥æ—¶è¿”å›æœ€ä½åˆ†æ•°


def run_bayesian_optimization():
    """è¿è¡Œè´å¶æ–¯ä¼˜åŒ–"""
    
    # åˆ›å»ºç»“æœä¿å­˜ç›®å½•
    os.makedirs(FIXED_CONFIG['checkpoint_dir'], exist_ok=True)
    
    # åˆ›å»ºstudy
    study = optuna.create_study(
        direction='maximize',  # æœ€å¤§åŒ–F1åˆ†æ•°
        study_name='single_task_deberta_moe_optimization',
        storage=f'sqlite:///{FIXED_CONFIG["checkpoint_dir"]}/optuna_study.db',
        load_if_exists=True
    )
    
    print("ğŸš€ å¼€å§‹å•ä»»åŠ¡DeBERTa MoEè´å¶æ–¯è¶…å‚æ•°ä¼˜åŒ–...")
    print(f"ä»»åŠ¡: {FIXED_CONFIG['main_task']}")
    print(f"ç›®æ ‡: æœ€å¤§åŒ–éªŒè¯é›†F1åˆ†æ•°")
    print(f"æ€»è¯•éªŒæ¬¡æ•°: 1000")
    print(f"ç»“æœä¿å­˜è‡³: {FIXED_CONFIG['checkpoint_dir']}")
    
    # è¿è¡Œä¼˜åŒ–
    study.optimize(objective, n_trials=1000, timeout=None)
    
    # è·å–æœ€ä½³ç»“æœ
    best_trial = study.best_trial
    best_params = best_trial.params
    best_f1 = best_trial.value
    best_acc = best_trial.user_attrs.get('best_acc', 0.0)
    best_epoch = best_trial.user_attrs.get('best_epoch', 0)
    
    # ä¿å­˜ç»“æœ
    results = {
        'task': FIXED_CONFIG['main_task'],
        'best_params': best_params,
        'best_f1': best_f1,
        'best_acc': best_acc,
        'best_epoch': best_epoch,
        'best_trial_number': best_trial.number,
        'optimization_history': [
            {
                'trial_number': trial.number,
                'value': trial.value,
                'params': trial.params,
                'user_attrs': trial.user_attrs
            }
            for trial in study.trials if trial.state == optuna.trial.TrialState.COMPLETE
        ],
        'timestamp': datetime.now().isoformat()
    }
    
    # ä¿å­˜ä¸ºJSONå’Œpickle
    with open(f'{FIXED_CONFIG["checkpoint_dir"]}/best_results_{FIXED_CONFIG["main_task"]}.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    with open(f'{FIXED_CONFIG["checkpoint_dir"]}/best_results_{FIXED_CONFIG["main_task"]}.pkl', 'wb') as f:
        pickle.dump(results, f)
    
    # æ‰“å°ç»“æœ
    print("\nğŸ‰ ä¼˜åŒ–å®Œæˆ!")
    print("=" * 70)
    print(f"ä»»åŠ¡: {FIXED_CONFIG['main_task']}")
    print(f"æœ€ä½³F1åˆ†æ•°: {best_f1:.4f}")
    print(f"æœ€ä½³å‡†ç¡®ç‡: {best_acc:.4f}")
    print(f"æœ€ä½³epoch: {best_epoch}")
    print(f"æœ€ä½³å‚æ•°ç»„åˆ:")
    for key, value in best_params.items():
        print(f"  {key}: {value}")
    print("=" * 70)
    
    # ä¿å­˜æœ€ä½³å‚æ•°ä¸ºå•ç‹¬æ–‡ä»¶ä¾¿äºä½¿ç”¨
    with open(f'{FIXED_CONFIG["checkpoint_dir"]}/best_hyperparams_{FIXED_CONFIG["main_task"]}.json', 'w') as f:
        json.dump(best_params, f, indent=2)
    
    print(f"è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³: {FIXED_CONFIG['checkpoint_dir']}/best_results_{FIXED_CONFIG['main_task']}.json")
    print(f"æœ€ä½³å‚æ•°å·²ä¿å­˜è‡³: {FIXED_CONFIG['checkpoint_dir']}/best_hyperparams_{FIXED_CONFIG['main_task']}.json")
    
    return study, results


if __name__ == '__main__':
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    print(f"ä¼˜åŒ–ä»»åŠ¡: {FIXED_CONFIG['main_task']}")
    print("=" * 70)
    print("è´å¶æ–¯ä¼˜åŒ–æœç´¢ç©ºé—´:")
    print("  num_experts: [4, 8, 16, 32, 64]")
    print("  top_k: [1, min(8, num_experts)]")
    print("  expert_mlp_layers: [1, 2, 3, 4]")
    print("  expert_hidden_size: [128, 256, 512, 768, 1024]")
    print("  load_balance_weight: [0.001, 0.1] (log scale)")
    print("  batch_size: [4, 8, 16, 32]")
    print("  lr: [1e-6, 1e-3] (log scale)")
    print("  weight_decay: [1e-5, 1e-1] (log scale)")
    print("  seed: [42, 1000]")
    print("  dropout: [0.1, 0.4]")
    print("=" * 70)
    print()
    print("ğŸ’¡ æç¤º: å¦‚éœ€åˆ‡æ¢ä»»åŠ¡ï¼Œè¯·ä¿®æ”¹FIXED_CONFIGä¸­çš„'main_task'")
    print("   å¯é€‰ä»»åŠ¡: 'Actionability', 'Mistake_Identification', 'Mistake_Location', 'Providing_Guidance'")
    print()
    
    # è¿è¡Œä¼˜åŒ–
    study, results = run_bayesian_optimization()