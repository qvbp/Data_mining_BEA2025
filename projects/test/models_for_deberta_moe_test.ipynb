{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# 设置环境变量，只让程序看到 GPU 2\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import BertModel, AutoModel\n",
    "from transformers import AdamW\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理函数  测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class TestBAE2025Dataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            data_path,\n",
    "            label_type=\"Actionability\"  # 预测的标签类型\n",
    "    ):\n",
    "        self.data_path = data_path\n",
    "        self.label_type = label_type\n",
    "        self._get_data()\n",
    "        \n",
    "    def _get_data(self):\n",
    "        with open(self.data_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "        self.data = []\n",
    "        self.sample_info = []  # 存储样本ID和tutor信息，用于结果映射\n",
    "        \n",
    "        for item in data:\n",
    "            conversation_id = item['conversation_id']\n",
    "            conversation_history = item['conversation_history']\n",
    "            \n",
    "            # 处理每个tutor的回复\n",
    "            for tutor_key, tutor_data in item['tutor_responses'].items():\n",
    "                tutor_response = tutor_data['response']\n",
    "                # 存储句子对\n",
    "                self.data.append((conversation_history, tutor_response))\n",
    "                # 存储样本信息，用于后续预测结果的映射\n",
    "                self.sample_info.append({\n",
    "                    'conversation_id': conversation_id,\n",
    "                    'tutor_key': tutor_key\n",
    "                })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.sample_info[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据加载函数  测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestBAE2025DataLoader:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        batch_size=16,\n",
    "        max_length=512,\n",
    "        shuffle=False,  # 测试集不需要打乱顺序\n",
    "        drop_last=False,  # 测试集不应丢弃最后一个批次\n",
    "        device=None,\n",
    "        tokenizer_name='microsoft/deberta-v3-base'\n",
    "    ):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        self.tokenizer.truncation_side = 'left'  # 设置截断方向为左侧，保留句子结尾的部分\n",
    "        print(\"当前使用的 tokenizer 类型：\", type(self.tokenizer))\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "        self.shuffle = shuffle\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "        if device is None:\n",
    "            self.device = torch.device(\n",
    "                'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            )\n",
    "        else:\n",
    "            self.device = device\n",
    "\n",
    "        self.loader = DataLoader(\n",
    "            dataset=self.dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=self.collate_fn,\n",
    "            shuffle=self.shuffle,\n",
    "            drop_last=self.drop_last\n",
    "        )\n",
    "\n",
    "    def collate_fn(self, data):\n",
    "        sents = [i[0] for i in data]\n",
    "        sample_info = [i[1] for i in data]\n",
    "\n",
    "        # 处理两个句子的情况\n",
    "        encoded = self.tokenizer.batch_encode_plus(\n",
    "            batch_text_or_text_pairs=[(sent[0], sent[1]) for sent in sents],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt',\n",
    "            return_length=True\n",
    "        )\n",
    "        \n",
    "        input_ids = encoded['input_ids'].to(self.device)\n",
    "        attention_mask = encoded['attention_mask'].to(self.device)\n",
    "        \n",
    "        # 返回编码后的输入、样本信息（没有标签）\n",
    "        return input_ids, attention_mask, sample_info\n",
    "\n",
    "    def __iter__(self):\n",
    "        for data in self.loader:\n",
    "            yield data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel\n",
    "\n",
    "\n",
    "class ExpertLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, input_size]\n",
    "        output, (hidden, _) = self.lstm(x)\n",
    "        # 返回最后一个时间步的隐藏状态\n",
    "        return hidden[-1]  # [batch_size, hidden_size]\n",
    "\n",
    "\n",
    "class ExpertBiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size // 2,  # 因为是双向的，所以每个方向的隐藏层大小减半\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, input_size]\n",
    "        output, (hidden, _) = self.bilstm(x)\n",
    "        # 拼接最后一层的正向和反向隐藏状态\n",
    "        # hidden shape: [num_layers * num_directions, batch_size, hidden_size//2]\n",
    "        hidden_forward = hidden[-2]  # 正向的最后一层 [batch_size, hidden_size//2]\n",
    "        hidden_backward = hidden[-1]  # 反向的最后一层 [batch_size, hidden_size//2]\n",
    "        hidden_concat = torch.cat([hidden_forward, hidden_backward], dim=1)  # [batch_size, hidden_size]\n",
    "        return hidden_concat\n",
    "\n",
    "\n",
    "class ExpertRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, input_size]\n",
    "        _, hidden = self.rnn(x)\n",
    "        # 返回最后一个时间步的隐藏状态\n",
    "        return hidden[-1]  # [batch_size, hidden_size]\n",
    "\n",
    "\n",
    "class ExpertGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, input_size]\n",
    "        _, hidden = self.gru(x)\n",
    "        # 返回最后一个时间步的隐藏状态\n",
    "        return hidden[-1]  # [batch_size, hidden_size]\n",
    "\n",
    "\n",
    "class ExpertLinear(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size * 2),\n",
    "            nn.LayerNorm(hidden_size * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size * 2, hidden_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, input_size]\n",
    "        # 我们需要把序列信息压缩为一个向量，可以使用平均池化\n",
    "        pooled = torch.mean(x, dim=1)  # [batch_size, input_size]\n",
    "        return self.linear(pooled)  # [batch_size, hidden_size]\n",
    "\n",
    "\n",
    "class BertClassificationHead(nn.Module):\n",
    "    def __init__(self, hidden_size=1024, num_classes=3, dropout_prob=0.3):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.out_proj = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, features):\n",
    "        # 提取 [CLS] 标记的表示\n",
    "        x = features[:, 0, :]  # 使用第一个标记([CLS])\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MoERouter(nn.Module):\n",
    "    \"\"\"专家路由器，学习为每个样本分配专家权重\"\"\"\n",
    "    def __init__(self, input_size, num_experts):\n",
    "        super().__init__()\n",
    "        self.router = nn.Linear(input_size, num_experts)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, input_size]\n",
    "        # 计算每个专家的权重 (使用softmax确保权重和为1)\n",
    "        router_logits = self.router(x)\n",
    "        router_probs = F.softmax(router_logits, dim=-1)\n",
    "        return router_probs  # [batch_size, num_experts]\n",
    "\n",
    "\n",
    "class DeBERTaMoEClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        pretrained_model_name, \n",
    "        num_classes=3, \n",
    "        freeze_pooler=0,\n",
    "        expert_hidden_size=256,\n",
    "        dropout=0.3,\n",
    "        num_rnn_layers=1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 使用 AutoModel 加载 DeBERTa 模型\n",
    "        self.bert = AutoModel.from_pretrained(pretrained_model_name)\n",
    "        \n",
    "        # 获取 bert 隐藏层大小\n",
    "        self.bert_hidden_size = self.bert.config.hidden_size\n",
    "        \n",
    "        # 保留原有的分类头\n",
    "        self.original_classifier = BertClassificationHead(\n",
    "            hidden_size=self.bert_hidden_size,\n",
    "            num_classes=num_classes,\n",
    "            dropout_prob=dropout\n",
    "        )\n",
    "        \n",
    "        # 创建多个专家模型\n",
    "        self.experts = nn.ModuleDict({\n",
    "            'lstm': ExpertLSTM(\n",
    "                input_size=self.bert_hidden_size, \n",
    "                hidden_size=expert_hidden_size,\n",
    "                num_layers=num_rnn_layers,\n",
    "                dropout=dropout\n",
    "            ),\n",
    "            'bilstm': ExpertBiLSTM(\n",
    "                input_size=self.bert_hidden_size, \n",
    "                hidden_size=expert_hidden_size,\n",
    "                num_layers=num_rnn_layers,\n",
    "                dropout=dropout\n",
    "            ),\n",
    "            'rnn': ExpertRNN(\n",
    "                input_size=self.bert_hidden_size, \n",
    "                hidden_size=expert_hidden_size,\n",
    "                num_layers=num_rnn_layers,\n",
    "                dropout=dropout\n",
    "            ),\n",
    "            'gru': ExpertGRU(\n",
    "                input_size=self.bert_hidden_size, \n",
    "                hidden_size=expert_hidden_size,\n",
    "                num_layers=num_rnn_layers,\n",
    "                dropout=dropout\n",
    "            ),\n",
    "            'linear': ExpertLinear(\n",
    "                input_size=self.bert_hidden_size, \n",
    "                hidden_size=expert_hidden_size\n",
    "            ),\n",
    "        })\n",
    "        \n",
    "        # 创建路由器 (使用[CLS]标记表示作为路由的输入)\n",
    "        self.router = MoERouter(self.bert_hidden_size, len(self.experts))\n",
    "        \n",
    "        # 各专家模型的输出映射层，将各自的hidden_size映射到统一的输出空间\n",
    "        self.expert_outputs = nn.ModuleDict({\n",
    "            expert_name: nn.Linear(expert_hidden_size, num_classes)\n",
    "            for expert_name in self.experts.keys()\n",
    "        })\n",
    "        \n",
    "        # 最终的融合层，将所有结果拼接后映射到输出类别\n",
    "        # (1个原始分类头 + 5个专家) * 每个输出num_classes = 6 * num_classes\n",
    "        combined_dim = num_classes * (1 + len(self.experts))\n",
    "        self.final_classifier = nn.Sequential(\n",
    "            nn.Linear(combined_dim, combined_dim // 2),\n",
    "            nn.LayerNorm(combined_dim // 2),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(combined_dim // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # DeBERTa 编码\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # 获取序列隐藏状态\n",
    "        hidden_states = outputs.last_hidden_state  # [batch_size, seq_len, hidden_size]\n",
    "        \n",
    "        # 获取原始分类头结果\n",
    "        original_logits = self.original_classifier(hidden_states)  # [batch_size, num_classes]\n",
    "        \n",
    "        # 获取路由权重\n",
    "        cls_embedding = hidden_states[:, 0]  # [batch_size, hidden_size]\n",
    "        routing_weights = self.router(cls_embedding)  # [batch_size, num_experts]\n",
    "        \n",
    "        # 获取各专家结果\n",
    "        expert_outputs = {}\n",
    "        for expert_name, expert in self.experts.items():\n",
    "            # 获取专家输出\n",
    "            expert_hidden = expert(hidden_states)  # [batch_size, expert_hidden_size]\n",
    "            # 映射到类别空间\n",
    "            expert_logits = self.expert_outputs[expert_name](expert_hidden)  # [batch_size, num_classes]\n",
    "            # 存储结果\n",
    "            expert_outputs[expert_name] = expert_logits\n",
    "        \n",
    "        # 根据路由权重加权专家结果\n",
    "        # 首先，将所有专家的结果拼接到一起\n",
    "        expert_logits_list = [original_logits]  # 包含原始分类头\n",
    "        expert_names = list(self.experts.keys())\n",
    "        \n",
    "        for expert_name in expert_names:\n",
    "            expert_logits_list.append(expert_outputs[expert_name])\n",
    "        \n",
    "        # 拼接所有结果 [batch_size, (1+num_experts)*num_classes]\n",
    "        combined_logits = torch.cat(expert_logits_list, dim=1)\n",
    "        \n",
    "        # 通过最终分类器输出最终结果\n",
    "        final_logits = self.final_classifier(combined_logits)\n",
    "        \n",
    "        # return {\n",
    "        #     'logits': final_logits,  # 最终预测\n",
    "        #     'original_logits': original_logits,  # 原始分类头预测\n",
    "        #     'expert_logits': expert_outputs,  # 各专家预测\n",
    "        #     'routing_weights': routing_weights,  # 路由权重\n",
    "        #     'combined_logits': combined_logits  # 拼接的中间结果\n",
    "        # }\n",
    "        \n",
    "        return final_logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "import random\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import AdamW\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "# 如果在Jupyter Notebook中运行，可以使用这个自定义参数函数替代argparser\n",
    "def get_default_configs():\n",
    "    \"\"\"在Jupyter环境中使用的默认配置，避免argparse解析错误\"\"\"\n",
    "    class Args:\n",
    "        def __init__(self):\n",
    "            # self.model_name = '/mnt/cfs/huangzhiwei/pykt-moekt/SBM/bge-large-en-v1.5'\n",
    "            # self.model_name = \"/mnt/cfs/huangzhiwei/BAE2025/models/ModernBERT-large\"\n",
    "            # self.model_name = '/mnt/cfs/huangzhiwei/pykt-moekt/SBM/xlm-roberta-large'\n",
    "            # self.model_name = '/mnt/cfs/huangzhiwei/BAE2025/models/bge-base-en-v1.5'\n",
    "            # self.model_name = '/mnt/cfs/huangzhiwei/BAE2025/models/bert-base-uncased'\n",
    "            self.model_name = '/mnt/cfs/huangzhiwei/BAE2025/models/deberta-v3-base'\n",
    "            # self.model_name = '/mnt/cfs/huangzhiwei/BAE2025/models/roberta-base'\n",
    "            self.num_classes = 3\n",
    "            self.dropout = 0.25\n",
    "            self.freeze_pooler = 8\n",
    "            self.batch_size = 16\n",
    "            self.max_length = 512\n",
    "            self.lr = 1e-5\n",
    "            self.epochs = 50\n",
    "            self.device = device\n",
    "            self.name = None\n",
    "            self.seed = 42\n",
    "            self.data_path = '../data_new/track4_train.json'\n",
    "            self.val_data_path = '../data_new/track4_val.json'\n",
    "            self.checkpoint_dir = \"/mnt/cfs/huangzhiwei/BAE2025/projects/predict\"\n",
    "            self.patience = 6\n",
    "            self.expert_hidden_size = 512\n",
    "            self.num_rnn_layers = 1\n",
    "            self.warmup_ratio = 0.1\n",
    "            self.test_data_path = \"/mnt/cfs/huangzhiwei/BAE2025/data_new/mrbench_v3_testset.json\"\n",
    "            self.output_path = \"/mnt/cfs/huangzhiwei/BAE2025/projects/predict/predictions_final.json\"\n",
    "            self.exp_name = 'BAE2025_track4_bert'\n",
    "    return Args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Jupyter environment, using default configs\n",
      "使用设备: cuda\n",
      "开始对 /mnt/cfs/huangzhiwei/BAE2025/data_new/mrbench_v3_testset.json 进行预测...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前使用的 tokenizer 类型： <class 'transformers.models.deberta_v2.tokenization_deberta_v2_fast.DebertaV2TokenizerFast'>\n",
      "加载模型: /mnt/cfs/huangzhiwei/BAE2025/projects/predict/best_model_f1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1457632/102974723.py:60: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('/mnt/cfs/huangzhiwei/BAE2025/projects/predict/deberta_lr0.1/best_model_f1_21.pt', map_location=device))\n",
      "预测中: 100%|██████████| 97/97 [00:14<00:00,  6.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测完成，结果已保存到: /mnt/cfs/huangzhiwei/BAE2025/projects/predict/predictions_final.json\n",
      "\n",
      "预测标签分布:\n",
      "  Yes: 826 (53.39%)\n",
      "  To some extent: 232 (15.00%)\n",
      "  No: 489 (31.61%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def predict(configs, test_data_path, output_path):\n",
    "    \"\"\"\n",
    "    使用训练好的模型对测试集进行预测，并将预测结果保存为JSON格式\n",
    "    \n",
    "    Args:\n",
    "        configs: 配置参数\n",
    "        model_class: 模型类\n",
    "        test_data_path: 测试数据路径\n",
    "        output_path: 输出结果保存路径\n",
    "    \"\"\"\n",
    "    print(f\"开始对 {test_data_path} 进行预测...\")\n",
    "    \n",
    "    # 设置设备\n",
    "    if configs.device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    else:\n",
    "        device = configs.device\n",
    "    \n",
    "    # 加载测试数据集\n",
    "    test_dataset = TestBAE2025Dataset(\n",
    "        data_path=test_data_path,\n",
    "        label_type=configs.label_type if hasattr(configs, 'label_type') else \"Actionability\"\n",
    "    )\n",
    "    \n",
    "    # 创建测试数据加载器\n",
    "    test_dataloader = TestBAE2025DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=configs.batch_size,\n",
    "        max_length=configs.max_length,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        device=device,\n",
    "        tokenizer_name=configs.model_name\n",
    "    )\n",
    "    \n",
    "    # 创建模型\n",
    "    model = DeBERTaMoEClassifier(\n",
    "        pretrained_model_name=configs.model_name,\n",
    "        num_classes=configs.num_classes,\n",
    "        freeze_pooler=configs.freeze_pooler,\n",
    "        num_rnn_layers=configs.num_rnn_layers,\n",
    "        expert_hidden_size=configs.expert_hidden_size,\n",
    "        dropout=configs.dropout\n",
    "    ).to(device)\n",
    "    \n",
    "    # 获取检查点目录\n",
    "    # checkpoint_dir = os.path.join(configs.checkpoint_dir, configs.exp_name)\n",
    "    checkpoint_dir = configs.checkpoint_dir\n",
    "    \n",
    "    # 加载最佳模型\n",
    "    model_path = os.path.join(checkpoint_dir, 'best_model_f1.pt')\n",
    "    print(f\"加载模型: {model_path}\")\n",
    "    \n",
    "    model.load_state_dict(torch.load('/mnt/cfs/huangzhiwei/BAE2025/projects/predict/deberta_lr0.1/best_model_f1_21.pt', map_location=device))\n",
    "    model.eval()\n",
    "    \n",
    "    # 定义标签映射（从数字到文本标签）\n",
    "    label_mapping = {0: \"Yes\", 1: \"To some extent\", 2: \"No\"}\n",
    "    \n",
    "    # 保存所有预测结果\n",
    "    predictions = {}\n",
    "    \n",
    "    # 开始预测\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, sample_info_batch in tqdm(test_dataloader, desc=\"预测中\"):\n",
    "            # 前向传播\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            preds = logits.argmax(dim=1).cpu().numpy()\n",
    "            \n",
    "            # 处理预测结果\n",
    "            for i, pred in enumerate(preds):\n",
    "                sample_info = sample_info_batch[i]\n",
    "                conv_id = sample_info['conversation_id']\n",
    "                tutor_key = sample_info['tutor_key']\n",
    "                \n",
    "                # 获取文本标签\n",
    "                predicted_label = label_mapping[int(pred)]\n",
    "                \n",
    "                # 初始化字典（如果不存在）\n",
    "                if conv_id not in predictions:\n",
    "                    predictions[conv_id] = {}\n",
    "                    \n",
    "                if 'tutor_responses' not in predictions[conv_id]:\n",
    "                    predictions[conv_id]['tutor_responses'] = {}\n",
    "                \n",
    "                if tutor_key not in predictions[conv_id]['tutor_responses']:\n",
    "                    predictions[conv_id]['tutor_responses'][tutor_key] = {}\n",
    "                \n",
    "                # 保存预测结果\n",
    "                predictions[conv_id]['tutor_responses'][tutor_key]['annotation'] = {\n",
    "                    'Actionability': predicted_label\n",
    "                }\n",
    "    \n",
    "    # 读取原始测试数据，以保持完整结构\n",
    "    with open(test_data_path, 'r', encoding='utf-8') as f:\n",
    "        original_data = json.load(f)\n",
    "    \n",
    "    # 组合预测结果与原始数据\n",
    "    final_results = []\n",
    "    for item in original_data:\n",
    "        conv_id = item['conversation_id']\n",
    "        if conv_id in predictions:\n",
    "            # 深拷贝原始数据\n",
    "            new_item = item.copy()\n",
    "            \n",
    "            # 为每个tutor_response添加annotation字段\n",
    "            for tutor_key in new_item['tutor_responses'].keys():\n",
    "                if (tutor_key in predictions[conv_id]['tutor_responses'] and \n",
    "                    'annotation' in predictions[conv_id]['tutor_responses'][tutor_key]):\n",
    "                    # 添加预测的annotation\n",
    "                    new_item['tutor_responses'][tutor_key]['annotation'] = predictions[conv_id]['tutor_responses'][tutor_key]['annotation']\n",
    "            \n",
    "            final_results.append(new_item)\n",
    "        else:\n",
    "            # 如果没有预测结果，保留原始数据\n",
    "            final_results.append(item)\n",
    "    \n",
    "    # 保存最终结果\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(final_results, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"预测完成，结果已保存到: {output_path}\")\n",
    "    \n",
    "    # 打印预测标签分布\n",
    "    label_counts = {\"Yes\": 0, \"To some extent\": 0, \"No\": 0}\n",
    "    total_predictions = 0\n",
    "    \n",
    "    for item in final_results:\n",
    "        for tutor_data in item['tutor_responses'].values():\n",
    "            if 'annotation' in tutor_data and 'Actionability' in tutor_data['annotation']:\n",
    "                label = tutor_data['annotation']['Actionability']\n",
    "                label_counts[label] += 1\n",
    "                total_predictions += 1\n",
    "    \n",
    "    print(\"\\n预测标签分布:\")\n",
    "    for label, count in label_counts.items():\n",
    "        percentage = (count / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "        print(f\"  {label}: {count} ({percentage:.2f}%)\")\n",
    "\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 判断是否在Jupyter环境中运行\n",
    "    try:\n",
    "        # 检查是否在Jupyter中运行\n",
    "        get_ipython = globals().get('get_ipython', None)\n",
    "        if get_ipython and 'IPKernelApp' in get_ipython().config:\n",
    "            # 在Jupyter环境中运行，使用默认配置\n",
    "            print(\"Running in Jupyter environment, using default configs\")\n",
    "            configs = get_default_configs()\n",
    "        else:\n",
    "            # 在命令行环境中运行，使用argparse\n",
    "            configs = argparser()\n",
    "    except:\n",
    "        # 任何异常都使用argparse处理\n",
    "        configs = argparser()\n",
    "    \n",
    "    # 设置实验名称\n",
    "    if configs.name is None:\n",
    "        configs.exp_name = \\\n",
    "            f'{os.path.basename(configs.model_name)}' + \\\n",
    "            f'{\"_fp\" if configs.freeze_pooler else \"\"}' + \\\n",
    "            f'_b{configs.batch_size}_e{configs.epochs}' + \\\n",
    "            f'_len{configs.max_length}_lr{configs.lr}'\n",
    "    else:\n",
    "        configs.exp_name = configs.name\n",
    "    \n",
    "    # 设置设备\n",
    "    if configs.device is None:\n",
    "        configs.device = torch.device(\n",
    "            'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        )\n",
    "    \n",
    "    print(f\"使用设备: {configs.device}\")\n",
    "    \n",
    "    # 判断是否进行训练或预测\n",
    "    # if hasattr(configs, 'predict') and configs.predict:\n",
    "    # 导入模型类\n",
    "    # from model import DeBERTaMoEClassifier\n",
    "\n",
    "    # 进行预测\n",
    "    predict(\n",
    "        configs=configs,\n",
    "        test_data_path=configs.test_data_path,\n",
    "        output_path=configs.output_path\n",
    "    )\n",
    "    # else:\n",
    "    #     # 进行训练\n",
    "    #     train(configs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
