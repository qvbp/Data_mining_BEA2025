{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# 设置环境变量，只让程序看到 GPU 2\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import BertModel, AutoModel\n",
    "from transformers import AdamW\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "\n",
    "class BAE2025Dataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            data_path,\n",
    "            labels={\n",
    "                \"Yes\": 0,\n",
    "                \"To some extent\": 1, \n",
    "                \"No\": 2,\n",
    "            }\n",
    "    ):\n",
    "        self.data_path = data_path\n",
    "        self.labels = labels\n",
    "        self._get_data()\n",
    "    \n",
    "    def _get_data(self):\n",
    "        with open(self.data_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        self.data = []\n",
    "        for item in data:\n",
    "            tutor_responses = item['tutor_responses']\n",
    "            for response in tutor_responses.values():\n",
    "                sent1 = item['conversation_history']\n",
    "                sent2 = response['response']\n",
    "                label = response['annotation'][\"Providing_Guidance\"]\n",
    "                if label in self.labels:\n",
    "                    self.data.append(((sent1, sent2), self.labels[label]))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def get_labels(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class BAE2025DataLoader:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        batch_size=16,\n",
    "        max_length=512,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        device=None,\n",
    "        # tokenizer_name='chinese-bert-wwm-ext'\n",
    "        # tokenizer_name='chinese-roberta-wwm-ext'\n",
    "        # tokenizer_name='chinese-roberta-wwm-ext-large'\n",
    "        # tokenizer_name='/mnt/cfs/huangzhiwei/pykt-moekt/SBM/bge-large-en-v1.5'\n",
    "        # tokenizer_name='/mnt/cfs/huangzhiwei/BAE2025/models/bge-base-en-v1.5'\n",
    "        tokenizer_name='/mnt/cfs/huangzhiwei/BAE2025/models/bert-base-uncased'\n",
    "        # tokenizer_name='/mnt/cfs/huangzhiwei/BAE2025/models/deberta-v3-base'\n",
    "    ):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "        self.shuffle = shuffle\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "        if device is None:\n",
    "            self.device = torch.device(\n",
    "                'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            )\n",
    "        else:\n",
    "            self.device = device\n",
    "\n",
    "        self.loader = DataLoader(\n",
    "            dataset=self.dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=self.collate_fn,\n",
    "            shuffle=self.shuffle,\n",
    "            drop_last=self.drop_last\n",
    "        )\n",
    "\n",
    "    def collate_fn(self, data):\n",
    "        sents = [i[0] for i in data]\n",
    "        labels = [i[1] for i in data]\n",
    "\n",
    "        # 修改这里，处理两个句子的情况\n",
    "        data = self.tokenizer.batch_encode_plus(\n",
    "            batch_text_or_text_pairs=[(sent[0], sent[1]) for sent in sents],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt',\n",
    "            return_length=True\n",
    "        )\n",
    "        input_ids = data['input_ids'].to(self.device)\n",
    "        attention_mask = data['attention_mask'].to(self.device)\n",
    "        token_type_ids = data['token_type_ids'].to(self.device)\n",
    "        labels = torch.LongTensor(labels).to(self.device)\n",
    "\n",
    "        return input_ids, attention_mask, token_type_ids, labels\n",
    "        # return input_ids, attention_mask, labels\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        for data in self.loader:\n",
    "            yield data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "# 修改模型类以支持分层分类\n",
    "class HierarchicalBertClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model_name, freeze_pooler=0, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 第一阶段分类器：Yes vs 非Yes\n",
    "        self.bert_stage1 = BertModel.from_pretrained(pretrained_model_name, output_hidden_states=True)\n",
    "        \n",
    "        # 第二阶段分类器：To some extent vs No\n",
    "        self.bert_stage2 = BertModel.from_pretrained(pretrained_model_name, output_hidden_states=True)\n",
    "        \n",
    "        # 冻结BERT底层，保留顶层微调\n",
    "        if freeze_pooler > 0:\n",
    "            # 冻结第一阶段模型的底层\n",
    "            modules1 = [self.bert_stage1.embeddings, *self.bert_stage1.encoder.layer[:freeze_pooler]]\n",
    "            for module in modules1:\n",
    "                for param in module.parameters():\n",
    "                    param.requires_grad = False\n",
    "            \n",
    "            # 冻结第二阶段模型的底层\n",
    "            modules2 = [self.bert_stage2.embeddings, *self.bert_stage2.encoder.layer[:freeze_pooler]]\n",
    "            for module in modules2:\n",
    "                for param in module.parameters():\n",
    "                    param.requires_grad = False\n",
    "        \n",
    "        # 获取bert隐藏层大小\n",
    "        bert_hidden_size = self.bert_stage1.config.hidden_size\n",
    "        \n",
    "        # 第一阶段的分类头（二分类：Yes vs 非Yes）\n",
    "        self.stage1_classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(bert_hidden_size, bert_hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(bert_hidden_size, 2)  # 二分类\n",
    "        )\n",
    "        \n",
    "        # 第二阶段的分类头（二分类：To some extent vs No）\n",
    "        self.stage2_classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(bert_hidden_size, bert_hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(bert_hidden_size, 2)  # 二分类\n",
    "        )\n",
    "    \n",
    "    def forward_stage1(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        \"\"\"第一阶段：预测是Yes还是非Yes\"\"\"\n",
    "        outputs = self.bert_stage1(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        # 使用[CLS]表示的序列表示\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        logits = self.stage1_classifier(cls_output)\n",
    "        return logits\n",
    "    \n",
    "    def forward_stage2(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        \"\"\"第二阶段：预测是To some extent还是No\"\"\"\n",
    "        outputs = self.bert_stage2(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        # 使用[CLS]表示的序列表示\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        logits = self.stage2_classifier(cls_output)\n",
    "        return logits\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None, stage=None):\n",
    "        \"\"\"根据stage参数进行相应阶段的前向传播\"\"\"\n",
    "        if stage == 1:\n",
    "            return self.forward_stage1(input_ids, attention_mask, token_type_ids)\n",
    "        elif stage == 2:\n",
    "            return self.forward_stage2(input_ids, attention_mask, token_type_ids)\n",
    "        else:\n",
    "            # 默认行为：完整的两阶段预测\n",
    "            # 第一阶段：预测是Yes还是非Yes\n",
    "            stage1_logits = self.forward_stage1(input_ids, attention_mask, token_type_ids)\n",
    "            stage1_preds = torch.argmax(stage1_logits, dim=1)\n",
    "            \n",
    "            # 第二阶段：对预测为\"非Yes\"的样本进行To some extent vs No预测\n",
    "            # 创建一个全为0的三分类输出张量（Yes=0, To some extent=1, No=2）\n",
    "            batch_size = input_ids.size(0)\n",
    "            final_logits = torch.zeros(batch_size, 3, device=input_ids.device)\n",
    "            \n",
    "            # 设置Yes的logits值（从stage1获取）\n",
    "            final_logits[:, 0] = stage1_logits[:, 0]  # Yes的logits\n",
    "            \n",
    "            # 获取预测为非Yes(1)的样本索引\n",
    "            non_yes_indices = (stage1_preds == 1).nonzero(as_tuple=True)[0]\n",
    "            \n",
    "            if len(non_yes_indices) > 0:\n",
    "                # 只对预测为\"非Yes\"的样本进行第二阶段预测\n",
    "                non_yes_input_ids = input_ids[non_yes_indices]\n",
    "                non_yes_attention_mask = attention_mask[non_yes_indices]\n",
    "                non_yes_token_type_ids = None if token_type_ids is None else token_type_ids[non_yes_indices]\n",
    "                \n",
    "                stage2_logits = self.forward_stage2(non_yes_input_ids, non_yes_attention_mask, non_yes_token_type_ids)\n",
    "                \n",
    "                # 将第二阶段的预测结果（To some extent vs No）放入最终结果中\n",
    "                final_logits[non_yes_indices, 1] = stage2_logits[:, 0]  # To some extent的logits\n",
    "                final_logits[non_yes_indices, 2] = stage2_logits[:, 1]  # No的logits\n",
    "            \n",
    "            return final_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "import random\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import AdamW\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "# 如果在Jupyter Notebook中运行，可以使用这个自定义参数函数替代argparser\n",
    "def get_default_configs():\n",
    "    \"\"\"在Jupyter环境中使用的默认配置，避免argparse解析错误\"\"\"\n",
    "    class Args:\n",
    "        def __init__(self):\n",
    "            # self.model_name = '/mnt/cfs/huangzhiwei/pykt-moekt/SBM/bge-large-en-v1.5'\n",
    "            # self.model_name = \"/mnt/cfs/huangzhiwei/BAE2025/models/ModernBERT-large\"\n",
    "            # self.model_name = '/mnt/cfs/huangzhiwei/pykt-moekt/SBM/xlm-roberta-large'\n",
    "            # self.model_name = '/mnt/cfs/huangzhiwei/BAE2025/models/bge-base-en-v1.5'\n",
    "            self.model_name = '/mnt/cfs/huangzhiwei/BAE2025/models/bert-base-uncased'\n",
    "            # self.model_name = '/mnt/cfs/huangzhiwei/BAE2025/models/deberta-v3-base'\n",
    "            self.num_classes = 3\n",
    "            self.dropout = 0.3\n",
    "            self.freeze_pooler = 8\n",
    "            self.batch_size = 16\n",
    "            self.max_length = 512\n",
    "            self.lr = 1e-5\n",
    "            self.epochs = 50\n",
    "            self.device = device\n",
    "            self.name = None\n",
    "            self.seed = 42\n",
    "            self.data_path = './data/train_extend.json'\n",
    "            self.val_data_path = './data/valid_extend.json'\n",
    "            self.checkpoint_dir = 'checkpoints_2to2_extend'\n",
    "            self.patience = 8\n",
    "            self.exp_name = 'BAE2025_track4_bert'\n",
    "    return Args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Jupyter environment, using default configs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Epoch 1/50 =====\n",
      "Training Stage 1 (Yes vs Non-Yes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 1: 100%|█████████████████████████████████████| 198/198 [00:17<00:00, 11.36batch/s, loss=0.267]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 Training - Loss: 0.5325, Acc: 0.7061, F1: 0.6469\n",
      "Training Stage 2 (To some extent vs No)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 2: 100%|█████████████████████████████████████| 198/198 [00:12<00:00, 16.47batch/s, loss=0.455]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 Training - Loss: 0.4443, Acc: 0.8047, F1: 0.7973\n",
      "Overall Training - Acc: 0.7585, F1: 0.7634\n",
      "Evaluating on validation set...\n",
      "Stage 1 Validation - Acc: 0.8045, F1: 0.7993\n",
      "Stage 2 Validation - Acc: 0.8682, F1: 0.8680\n",
      "Overall Validation - Acc: 0.7755, F1: 0.7769\n",
      "New best model saved with F1: 0.7769, Acc: 0.7755\n",
      "\n",
      "===== Epoch 2/50 =====\n",
      "Training Stage 1 (Yes vs Non-Yes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 1: 100%|█████████████████████████████████████| 198/198 [00:17<00:00, 11.60batch/s, loss=0.407]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 Training - Loss: 0.4017, Acc: 0.7920, F1: 0.7809\n",
      "Training Stage 2 (To some extent vs No)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 2: 100%|█████████████████████████████████████| 198/198 [00:11<00:00, 16.55batch/s, loss=0.273]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 Training - Loss: 0.2756, Acc: 0.8745, F1: 0.8723\n",
      "Overall Training - Acc: 0.8005, F1: 0.8040\n",
      "Evaluating on validation set...\n",
      "Stage 1 Validation - Acc: 0.8184, F1: 0.8162\n",
      "Stage 2 Validation - Acc: 0.8722, F1: 0.8721\n",
      "Overall Validation - Acc: 0.8033, F1: 0.8042\n",
      "New best model saved with F1: 0.8042, Acc: 0.8033\n",
      "\n",
      "===== Epoch 3/50 =====\n",
      "Training Stage 1 (Yes vs Non-Yes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 1: 100%|█████████████████████████████████████| 198/198 [00:17<00:00, 11.55batch/s, loss=0.419]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 Training - Loss: 0.3497, Acc: 0.8153, F1: 0.8058\n",
      "Training Stage 2 (To some extent vs No)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 2: 100%|█████████████████████████████████████| 198/198 [00:12<00:00, 16.47batch/s, loss=0.567]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 Training - Loss: 0.2143, Acc: 0.8963, F1: 0.8947\n",
      "Overall Training - Acc: 0.8327, F1: 0.8361\n",
      "Evaluating on validation set...\n",
      "Stage 1 Validation - Acc: 0.8260, F1: 0.8183\n",
      "Stage 2 Validation - Acc: 0.8803, F1: 0.8803\n",
      "Overall Validation - Acc: 0.7793, F1: 0.7805\n",
      "\n",
      "===== Epoch 4/50 =====\n",
      "Training Stage 1 (Yes vs Non-Yes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 1: 100%|█████████████████████████████████████| 198/198 [00:17<00:00, 11.55batch/s, loss=0.278]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 Training - Loss: 0.3162, Acc: 0.8456, F1: 0.8369\n",
      "Training Stage 2 (To some extent vs No)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 2: 100%|█████████████████████████████████████| 198/198 [00:12<00:00, 16.44batch/s, loss=0.184]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 Training - Loss: 0.1874, Acc: 0.9220, F1: 0.9210\n",
      "Overall Training - Acc: 0.8415, F1: 0.8434\n",
      "Evaluating on validation set...\n",
      "Stage 1 Validation - Acc: 0.8310, F1: 0.8288\n",
      "Stage 2 Validation - Acc: 0.8661, F1: 0.8647\n",
      "Overall Validation - Acc: 0.8071, F1: 0.8060\n",
      "New best model saved with F1: 0.8060, Acc: 0.8071\n",
      "\n",
      "===== Epoch 5/50 =====\n",
      "Training Stage 1 (Yes vs Non-Yes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 1: 100%|█████████████████████████████████████| 198/198 [00:17<00:00, 11.54batch/s, loss=0.087]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 Training - Loss: 0.2832, Acc: 0.8671, F1: 0.8581\n",
      "Training Stage 2 (To some extent vs No)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 2: 100%|█████████████████████████████████████| 198/198 [00:11<00:00, 16.50batch/s, loss=0.211]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 Training - Loss: 0.1524, Acc: 0.9360, F1: 0.9353\n",
      "Overall Training - Acc: 0.8829, F1: 0.8843\n",
      "Evaluating on validation set...\n",
      "Stage 1 Validation - Acc: 0.8298, F1: 0.8258\n",
      "Stage 2 Validation - Acc: 0.8600, F1: 0.8583\n",
      "Overall Validation - Acc: 0.8045, F1: 0.8030\n",
      "\n",
      "===== Epoch 6/50 =====\n",
      "Training Stage 1 (Yes vs Non-Yes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 1: 100%|█████████████████████████████████████| 198/198 [00:17<00:00, 11.51batch/s, loss=0.353]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 Training - Loss: 0.2467, Acc: 0.8870, F1: 0.8782\n",
      "Training Stage 2 (To some extent vs No)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 2: 100%|█████████████████████████████████████| 198/198 [00:12<00:00, 16.42batch/s, loss=0.160]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 Training - Loss: 0.1177, Acc: 0.9554, F1: 0.9549\n",
      "Overall Training - Acc: 0.9167, F1: 0.9190\n",
      "Evaluating on validation set...\n",
      "Stage 1 Validation - Acc: 0.8272, F1: 0.8225\n",
      "Stage 2 Validation - Acc: 0.8742, F1: 0.8742\n",
      "Overall Validation - Acc: 0.7957, F1: 0.7953\n",
      "\n",
      "===== Epoch 7/50 =====\n",
      "Training Stage 1 (Yes vs Non-Yes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 1: 100%|█████████████████████████████████████| 198/198 [00:32<00:00,  6.19batch/s, loss=0.095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 Training - Loss: 0.2091, Acc: 0.9085, F1: 0.9010\n",
      "Training Stage 2 (To some extent vs No)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 2: 100%|█████████████████████████████████████| 198/198 [00:24<00:00,  8.23batch/s, loss=0.035]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 Training - Loss: 0.1015, Acc: 0.9593, F1: 0.9589\n",
      "Overall Training - Acc: 0.9485, F1: 0.9495\n",
      "Evaluating on validation set...\n",
      "Stage 1 Validation - Acc: 0.8235, F1: 0.8163\n",
      "Stage 2 Validation - Acc: 0.8763, F1: 0.8761\n",
      "Overall Validation - Acc: 0.7957, F1: 0.7969\n",
      "\n",
      "===== Epoch 8/50 =====\n",
      "Training Stage 1 (Yes vs Non-Yes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 1: 100%|█████████████████████████████████████| 198/198 [00:33<00:00,  5.84batch/s, loss=0.288]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 Training - Loss: 0.1754, Acc: 0.9236, F1: 0.9171\n",
      "Training Stage 2 (To some extent vs No)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 2:  70%|█████████████████████████▊           | 138/198 [00:13<00:05, 10.20batch/s, loss=0.003]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 426\u001b[0m\n\u001b[1;32m    421\u001b[0m     configs\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    423\u001b[0m     )\n\u001b[1;32m    425\u001b[0m \u001b[38;5;66;03m# 调用分层训练函数\u001b[39;00m\n\u001b[0;32m--> 426\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_hierarchical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 141\u001b[0m, in \u001b[0;36mtrain_hierarchical\u001b[0;34m(configs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Stage 2 (To some extent vs No)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(train_dataloader, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataloader), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStage 2\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m, ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m input_ids, attention_mask, token_type_ids, labels \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# 筛选非Yes样本的索引\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         non_yes_indices \u001b[38;5;241m=\u001b[39m (labels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mnonzero(as_tuple\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(non_yes_indices) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 67\u001b[0m, in \u001b[0;36mBAE2025DataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader:\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 49\u001b[0m, in \u001b[0;36mBAE2025DataLoader.collate_fn\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     46\u001b[0m labels \u001b[38;5;241m=\u001b[39m [i[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# 修改这里，处理两个句子的情况\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msent\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msents\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     56\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     58\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3311\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3301\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3302\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3303\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3304\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3308\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3309\u001b[0m )\n\u001b[0;32m-> 3311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3313\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3329\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3330\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3331\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py:577\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_ids \u001b[38;5;129;01min\u001b[39;00m sanitized_tokens[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eventual_warn_about_too_long_sequence(input_ids, max_length, verbose)\n\u001b[0;32m--> 577\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43msanitized_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msanitized_encodings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:240\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    236\u001b[0m     n_sequences \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_sequences\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_sequences \u001b[38;5;241m=\u001b[39m n_sequences\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:776\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    773\u001b[0m     value \u001b[38;5;241m=\u001b[39m [value]\n\u001b[1;32m    775\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> 776\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;66;03m# # at-least2d\u001b[39;00m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;66;03m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;66;03m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor[None, :]\u001b[39;00m\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;28mself\u001b[39m[key] \u001b[38;5;241m=\u001b[39m tensor\n",
      "File \u001b[0;32m/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:738\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors.<locals>.as_tensor\u001b[0;34m(value, dtype)\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value[\u001b[38;5;241m0\u001b[39m], np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray(value))\n\u001b[0;32m--> 738\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_hierarchical(configs):\n",
    "    # 设置随机种子\n",
    "    random.seed(configs.seed)\n",
    "    np.random.seed(configs.seed)\n",
    "    torch.manual_seed(configs.seed)\n",
    "    \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # 创建检查点目录\n",
    "    checkpoint_dir = os.path.join(configs.checkpoint_dir, configs.exp_name)\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # 为保存混淆矩阵创建目录 - 分别为训练集和验证集创建\n",
    "    train_plot_dir = os.path.join(checkpoint_dir, 'plots', 'train')\n",
    "    val_plot_dir = os.path.join(checkpoint_dir, 'plots', 'val')\n",
    "    os.makedirs(train_plot_dir, exist_ok=True)\n",
    "    os.makedirs(val_plot_dir, exist_ok=True)\n",
    "    \n",
    "    # 加载数据集\n",
    "    train_dataset = BAE2025Dataset(configs.data_path)\n",
    "    val_dataset = BAE2025Dataset(configs.val_data_path)    \n",
    "\n",
    "    # 创建数据加载器\n",
    "    train_dataloader = BAE2025DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=configs.batch_size,\n",
    "        max_length=configs.max_length,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        device=configs.device,\n",
    "        tokenizer_name=configs.model_name\n",
    "    )\n",
    "\n",
    "    val_dataloader = BAE2025DataLoader(\n",
    "        dataset=val_dataset,\n",
    "        batch_size=configs.batch_size,\n",
    "        max_length=configs.max_length,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        device=configs.device,\n",
    "        tokenizer_name=configs.model_name\n",
    "    )\n",
    "    \n",
    "    # 创建分层分类模型\n",
    "    model = HierarchicalBertClassifier(\n",
    "        pretrained_model_name=configs.model_name,\n",
    "        freeze_pooler=configs.freeze_pooler,\n",
    "        dropout=configs.dropout\n",
    "    ).to(configs.device)\n",
    "\n",
    "    # 定义两个阶段的损失函数\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # 定义优化器\n",
    "    optimizer = AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=configs.lr\n",
    "    )\n",
    "\n",
    "    # 初始化最佳验证指标\n",
    "    best_val_acc = 0.0\n",
    "    best_val_f1 = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # 定义原始类别名称和阶段类别名称\n",
    "    class_names = ['Yes', 'To some extent', 'No']\n",
    "    stage1_names = ['Yes', 'Non-Yes']\n",
    "    stage2_names = ['To some extent', 'No']\n",
    "    \n",
    "    # 添加计算所需的库\n",
    "    from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # 训练循环\n",
    "    for epoch in range(configs.epochs):\n",
    "        print(f\"\\n===== Epoch {epoch + 1}/{configs.epochs} =====\")\n",
    "        \n",
    "        # ======== 训练第一阶段模型：Yes vs 非Yes ========\n",
    "        model.train()\n",
    "        stage1_train_loss = 0.0\n",
    "        stage1_train_preds = []\n",
    "        stage1_train_labels = []\n",
    "        \n",
    "        print(\"Training Stage 1 (Yes vs Non-Yes)...\")\n",
    "        with tqdm(train_dataloader, total=len(train_dataloader), desc=\"Stage 1\", unit=\"batch\", ncols=100) as pbar:\n",
    "            for input_ids, attention_mask, token_type_ids, labels in pbar:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # 将原始标签转换为二分类标签：0(Yes) 或 1(非Yes)\n",
    "                stage1_labels = (labels > 0).long()  # Yes=0, 其他=1\n",
    "                \n",
    "                # 前向传播第一阶段\n",
    "                stage1_logits = model(input_ids, attention_mask, token_type_ids, stage=1)\n",
    "                \n",
    "                # 计算损失\n",
    "                loss = criterion(stage1_logits, stage1_labels)\n",
    "                \n",
    "                # 反向传播\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # 收集预测和标签\n",
    "                preds = torch.argmax(stage1_logits, dim=1)\n",
    "                stage1_train_preds.extend(preds.cpu().numpy())\n",
    "                stage1_train_labels.extend(stage1_labels.cpu().numpy())\n",
    "                \n",
    "                stage1_train_loss += loss.item()\n",
    "                \n",
    "                # 更新进度条\n",
    "                pbar.set_postfix(loss=f'{loss.item():.3f}')\n",
    "        \n",
    "        # 计算第一阶段训练指标\n",
    "        stage1_train_loss /= len(train_dataloader)\n",
    "        stage1_train_acc = accuracy_score(stage1_train_labels, stage1_train_preds)\n",
    "        stage1_train_f1 = f1_score(stage1_train_labels, stage1_train_preds, average='macro')\n",
    "        \n",
    "        print(f\"Stage 1 Training - Loss: {stage1_train_loss:.4f}, Acc: {stage1_train_acc:.4f}, F1: {stage1_train_f1:.4f}\")\n",
    "        \n",
    "        # 创建并保存第一阶段训练混淆矩阵\n",
    "        cm = confusion_matrix(stage1_train_labels, stage1_train_preds)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=stage1_names, yticklabels=stage1_names)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title(f'Train: Stage 1 (Yes vs Non-Yes)\\nAcc: {stage1_train_acc:.4f}, F1: {stage1_train_f1:.4f}')\n",
    "        matrix_path = os.path.join(train_plot_dir, f'stage1_cm_epoch_{epoch+1}.png')\n",
    "        plt.savefig(matrix_path)\n",
    "        plt.close()\n",
    "        \n",
    "        # ======== 训练第二阶段模型：To some extent vs No ========\n",
    "        # 筛选出标签为To some extent或No的样本索引\n",
    "        stage2_train_loss = 0.0\n",
    "        stage2_train_preds = []\n",
    "        stage2_train_labels = []\n",
    "        stage2_sample_count = 0\n",
    "        \n",
    "        print(\"Training Stage 2 (To some extent vs No)...\")\n",
    "        with tqdm(train_dataloader, total=len(train_dataloader), desc=\"Stage 2\", unit=\"batch\", ncols=100) as pbar:\n",
    "            for input_ids, attention_mask, token_type_ids, labels in pbar:\n",
    "                # 筛选非Yes样本的索引\n",
    "                non_yes_indices = (labels > 0).nonzero(as_tuple=True)[0]\n",
    "                \n",
    "                if len(non_yes_indices) == 0:\n",
    "                    continue  # 如果批次中没有非Yes样本，跳过\n",
    "                \n",
    "                # 提取非Yes样本的数据\n",
    "                non_yes_input_ids = input_ids[non_yes_indices]\n",
    "                non_yes_attention_mask = attention_mask[non_yes_indices]\n",
    "                non_yes_token_type_ids = token_type_ids[non_yes_indices]\n",
    "                non_yes_labels = labels[non_yes_indices]\n",
    "                \n",
    "                # 将原始标签转换为二分类标签：0(To some extent) 或 1(No)\n",
    "                # 原始：0=Yes, 1=To some extent, 2=No\n",
    "                # 现在：0=To some extent, 1=No\n",
    "                stage2_labels = (non_yes_labels == 2).long()  # To some extent=0, No=1\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # 前向传播第二阶段\n",
    "                stage2_logits = model(non_yes_input_ids, non_yes_attention_mask, non_yes_token_type_ids, stage=2)\n",
    "                \n",
    "                # 计算损失\n",
    "                loss = criterion(stage2_logits, stage2_labels)\n",
    "                \n",
    "                # 反向传播\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # 收集预测和标签\n",
    "                preds = torch.argmax(stage2_logits, dim=1)\n",
    "                stage2_train_preds.extend(preds.cpu().numpy())\n",
    "                stage2_train_labels.extend(stage2_labels.cpu().numpy())\n",
    "                \n",
    "                stage2_train_loss += loss.item()\n",
    "                stage2_sample_count += 1\n",
    "                \n",
    "                # 更新进度条\n",
    "                pbar.set_postfix(loss=f'{loss.item():.3f}')\n",
    "        \n",
    "        # 计算第二阶段训练指标\n",
    "        if stage2_sample_count > 0:\n",
    "            stage2_train_loss /= stage2_sample_count\n",
    "            stage2_train_acc = accuracy_score(stage2_train_labels, stage2_train_preds)\n",
    "            stage2_train_f1 = f1_score(stage2_train_labels, stage2_train_preds, average='macro')\n",
    "            \n",
    "            print(f\"Stage 2 Training - Loss: {stage2_train_loss:.4f}, Acc: {stage2_train_acc:.4f}, F1: {stage2_train_f1:.4f}\")\n",
    "            \n",
    "            # 创建并保存第二阶段训练混淆矩阵\n",
    "            cm = confusion_matrix(stage2_train_labels, stage2_train_preds)\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=stage2_names, yticklabels=stage2_names)\n",
    "            plt.xlabel('Predicted')\n",
    "            plt.ylabel('True')\n",
    "            plt.title(f'Train: Stage 2 (To some extent vs No)\\nAcc: {stage2_train_acc:.4f}, F1: {stage2_train_f1:.4f}')\n",
    "            matrix_path = os.path.join(train_plot_dir, f'stage2_cm_epoch_{epoch+1}.png')\n",
    "            plt.savefig(matrix_path)\n",
    "            plt.close()\n",
    "        \n",
    "        # ======== 训练集整体评估 ========\n",
    "        model.eval()\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for input_ids, attention_mask, token_type_ids, labels in train_dataloader:\n",
    "                # 完整两阶段预测\n",
    "                logits = model(input_ids, attention_mask, token_type_ids)\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                \n",
    "                train_preds.extend(preds.cpu().numpy())\n",
    "                train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # 计算整体训练集指标\n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        train_f1 = f1_score(train_labels, train_preds, average='macro')\n",
    "        \n",
    "        print(f\"Overall Training - Acc: {train_acc:.4f}, F1: {train_f1:.4f}\")\n",
    "        \n",
    "        # 创建完整的训练集混淆矩阵\n",
    "        cm_full = confusion_matrix(train_labels, train_preds, labels=[0, 1, 2])\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm_full, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title(f'Train: Full Hierarchical Confusion Matrix\\nAcc: {train_acc:.4f}, F1: {train_f1:.4f}')\n",
    "        matrix_path = os.path.join(train_plot_dir, f'full_cm_epoch_{epoch+1}.png')\n",
    "        plt.savefig(matrix_path)\n",
    "        plt.close()\n",
    "        \n",
    "        # ======== 验证集评估 ========\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        stage1_val_preds = []\n",
    "        stage1_val_labels = []\n",
    "        stage2_val_preds = []\n",
    "        stage2_val_labels = []\n",
    "        \n",
    "        print(\"Evaluating on validation set...\")\n",
    "        with torch.no_grad():\n",
    "            for input_ids, attention_mask, token_type_ids, labels in val_dataloader:\n",
    "                # 第一阶段评估\n",
    "                stage1_logits = model(input_ids, attention_mask, token_type_ids, stage=1)\n",
    "                stage1_preds = torch.argmax(stage1_logits, dim=1)\n",
    "                stage1_labels_binary = (labels > 0).long()\n",
    "                \n",
    "                stage1_val_preds.extend(stage1_preds.cpu().numpy())\n",
    "                stage1_val_labels.extend(stage1_labels_binary.cpu().numpy())\n",
    "                \n",
    "                # 找出非Yes样本\n",
    "                non_yes_indices = (labels > 0).nonzero(as_tuple=True)[0]\n",
    "                \n",
    "                if len(non_yes_indices) > 0:\n",
    "                    # 第二阶段评估\n",
    "                    non_yes_input_ids = input_ids[non_yes_indices]\n",
    "                    non_yes_attention_mask = attention_mask[non_yes_indices]\n",
    "                    non_yes_token_type_ids = token_type_ids[non_yes_indices]\n",
    "                    non_yes_labels = labels[non_yes_indices]\n",
    "                    \n",
    "                    stage2_logits = model(non_yes_input_ids, non_yes_attention_mask, non_yes_token_type_ids, stage=2)\n",
    "                    stage2_preds = torch.argmax(stage2_logits, dim=1)\n",
    "                    \n",
    "                    # 转换为二分类标签：0=To some extent, 1=No\n",
    "                    stage2_labels_binary = (non_yes_labels == 2).long()\n",
    "                    \n",
    "                    stage2_val_preds.extend(stage2_preds.cpu().numpy())\n",
    "                    stage2_val_labels.extend(stage2_labels_binary.cpu().numpy())\n",
    "                \n",
    "                # 完整两阶段预测\n",
    "                logits = model(input_ids, attention_mask, token_type_ids)\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                \n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # 计算验证集指标\n",
    "        # 阶段1\n",
    "        stage1_val_acc = accuracy_score(stage1_val_labels, stage1_val_preds)\n",
    "        stage1_val_f1 = f1_score(stage1_val_labels, stage1_val_preds, average='macro')\n",
    "        \n",
    "        print(f\"Stage 1 Validation - Acc: {stage1_val_acc:.4f}, F1: {stage1_val_f1:.4f}\")\n",
    "        \n",
    "        # 创建阶段1验证混淆矩阵\n",
    "        cm = confusion_matrix(stage1_val_labels, stage1_val_preds)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=stage1_names, yticklabels=stage1_names)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title(f'Val: Stage 1 (Yes vs Non-Yes)\\nAcc: {stage1_val_acc:.4f}, F1: {stage1_val_f1:.4f}')\n",
    "        matrix_path = os.path.join(val_plot_dir, f'stage1_cm_epoch_{epoch+1}.png')\n",
    "        plt.savefig(matrix_path)\n",
    "        plt.close()\n",
    "        \n",
    "        # 阶段2\n",
    "        if len(stage2_val_labels) > 0:\n",
    "            stage2_val_acc = accuracy_score(stage2_val_labels, stage2_val_preds)\n",
    "            stage2_val_f1 = f1_score(stage2_val_labels, stage2_val_preds, average='macro')\n",
    "            \n",
    "            print(f\"Stage 2 Validation - Acc: {stage2_val_acc:.4f}, F1: {stage2_val_f1:.4f}\")\n",
    "            \n",
    "            # 创建阶段2验证混淆矩阵\n",
    "            cm = confusion_matrix(stage2_val_labels, stage2_val_preds)\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=stage2_names, yticklabels=stage2_names)\n",
    "            plt.xlabel('Predicted')\n",
    "            plt.ylabel('True')\n",
    "            plt.title(f'Val: Stage 2 (To some extent vs No)\\nAcc: {stage2_val_acc:.4f}, F1: {stage2_val_f1:.4f}')\n",
    "            matrix_path = os.path.join(val_plot_dir, f'stage2_cm_epoch_{epoch+1}.png')\n",
    "            plt.savefig(matrix_path)\n",
    "            plt.close()\n",
    "        \n",
    "        # 整体验证集评估\n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        val_f1 = f1_score(val_labels, val_preds, average='macro')\n",
    "        \n",
    "        print(f\"Overall Validation - Acc: {val_acc:.4f}, F1: {val_f1:.4f}\")\n",
    "        \n",
    "        # 创建完整验证集混淆矩阵\n",
    "        cm_full = confusion_matrix(val_labels, val_preds, labels=[0, 1, 2])\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm_full, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title(f'Val: Full Hierarchical Confusion Matrix\\nAcc: {val_acc:.4f}, F1: {val_f1:.4f}')\n",
    "        matrix_path = os.path.join(val_plot_dir, f'full_cm_epoch_{epoch+1}.png')\n",
    "        plt.savefig(matrix_path)\n",
    "        plt.close()\n",
    "        \n",
    "        # 绘制两两类别的验证集混淆矩阵\n",
    "        class_pairs = [\n",
    "            ([0, 1], ['Yes', 'To some extent']),  # Yes vs To some extent\n",
    "            ([0, 2], ['Yes', 'No']),              # Yes vs No\n",
    "            ([1, 2], ['To some extent', 'No'])    # To some extent vs No\n",
    "        ]\n",
    "        \n",
    "        for classes_idx, classes_names in class_pairs:\n",
    "            # 筛选出对应两个类别的预测和标签\n",
    "            mask = np.isin(np.array(val_labels), classes_idx)\n",
    "            filtered_preds = np.array(val_preds)[mask]\n",
    "            filtered_labels = np.array(val_labels)[mask]\n",
    "            \n",
    "            # 计算此对类别的准确率和F1分数\n",
    "            if len(filtered_labels) > 0:\n",
    "                pair_acc = accuracy_score(filtered_labels, filtered_preds)\n",
    "                # 计算二分类F1分数\n",
    "                pair_f1 = f1_score(filtered_labels, filtered_preds, average='macro')\n",
    "                \n",
    "                # 创建混淆矩阵\n",
    "                cm = confusion_matrix(filtered_labels, filtered_preds, labels=classes_idx)\n",
    "                \n",
    "                # 绘制混淆矩阵\n",
    "                plt.figure(figsize=(8, 6))\n",
    "                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                            xticklabels=[classes_names[i == classes_idx[1]] for i in classes_idx],\n",
    "                            yticklabels=[classes_names[i == classes_idx[1]] for i in classes_idx])\n",
    "                plt.xlabel('Predicted')\n",
    "                plt.ylabel('True')\n",
    "                plt.title(f'Val: {classes_names[0]} vs {classes_names[1]}\\nAcc: {pair_acc:.4f}, F1: {pair_f1:.4f}')\n",
    "                \n",
    "                # 保存图表\n",
    "                matrix_path = os.path.join(val_plot_dir, f'cm_{classes_names[0].replace(\" \", \"_\")}_{classes_names[1].replace(\" \", \"_\")}_epoch_{epoch+1}.png')\n",
    "                plt.savefig(matrix_path)\n",
    "                plt.close()\n",
    "        \n",
    "        # 检查是否保存模型并判断是否需要早停\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            best_val_acc = val_acc\n",
    "            \n",
    "            # 保存模型\n",
    "            # torch.save(model.state_dict(), os.path.join(checkpoint_dir, 'best_hierarchical_model.pt'))\n",
    "            print(f'New best model saved with F1: {best_val_f1:.4f}, Acc: {best_val_acc:.4f}')\n",
    "            \n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= configs.patience:\n",
    "                print(f'Early stopping triggered after {epoch+1} epochs.')\n",
    "                break\n",
    "        \n",
    "        # 返回训练状态\n",
    "        model.train()\n",
    "    \n",
    "    print(\"\\nTraining complete!\")\n",
    "    print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
    "    print(f\"Best validation F1 score: {best_val_f1:.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 修改主函数\n",
    "if __name__ == '__main__':\n",
    "    # 判断是否在Jupyter环境中运行\n",
    "    try:\n",
    "        # 检查是否在Jupyter中运行\n",
    "        get_ipython = globals().get('get_ipython', None)\n",
    "        if get_ipython and 'IPKernelApp' in get_ipython().config:\n",
    "            # 在Jupyter环境中运行，使用默认配置\n",
    "            print(\"Running in Jupyter environment, using default configs\")\n",
    "            configs = get_default_configs()\n",
    "        else:\n",
    "            # 在命令行环境中运行，使用argparse\n",
    "            configs = argparser()\n",
    "    except:\n",
    "        # 任何异常都使用argparse处理\n",
    "        configs = argparser()\n",
    "    \n",
    "    # 设置实验名称\n",
    "    if configs.name is None:\n",
    "        configs.exp_name = \\\n",
    "            f'hierarchical_{os.path.basename(configs.model_name)}' + \\\n",
    "            f'{\"_fp\" if configs.freeze_pooler else \"\"}' + \\\n",
    "            f'_b{configs.batch_size}_e{configs.epochs}' + \\\n",
    "            f'_len{configs.max_length}_lr{configs.lr}'\n",
    "    else:\n",
    "        configs.exp_name = configs.name\n",
    "    \n",
    "    # 设置设备\n",
    "    if configs.device is None:\n",
    "        configs.device = torch.device(\n",
    "            'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        )\n",
    "    \n",
    "    # 调用分层训练函数\n",
    "    trained_model = train_hierarchical(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
