{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# 设置环境变量，只让程序看到 GPU 2\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import BertModel, AutoModel\n",
    "from transformers import AdamW\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "\n",
    "class BAE2025Dataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            data_path,\n",
    "            labels={\n",
    "                \"Yes\": 0,\n",
    "                \"To some extent\": 1, \n",
    "                \"No\": 2,\n",
    "            }\n",
    "    ):\n",
    "        self.data_path = data_path\n",
    "        self.labels = labels\n",
    "        self._get_data()\n",
    "        \n",
    "    def _get_data(self):\n",
    "        with open(self.data_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "        self.data = []\n",
    "        self.original_data = data  # 保存原始数据\n",
    "        \n",
    "        # 增加数据编号跟踪\n",
    "        data_idx = 0\n",
    "        for item_idx, item in enumerate(data):\n",
    "            tutor_responses = item['tutor_responses']\n",
    "            for response_id, response in tutor_responses.items():\n",
    "                sent1 = item['conversation_history']\n",
    "                sent2 = response['response']\n",
    "                label = response['annotation'][\"Providing_Guidance\"]\n",
    "                if label in self.labels:\n",
    "                    # 保存额外的索引信息，以便跟踪\n",
    "                    self.data.append((\n",
    "                        (sent1, sent2), \n",
    "                        self.labels[label],\n",
    "                        {\n",
    "                            'data_idx': data_idx,\n",
    "                            'item_idx': item_idx,\n",
    "                            'response_id': response_id\n",
    "                        }\n",
    "                    ))\n",
    "                    data_idx += 1\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def get_labels(self):\n",
    "        return self.labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据加载函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class BAE2025DataLoader:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        batch_size=16,\n",
    "        max_length=512,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        device=None,\n",
    "        # tokenizer_name='chinese-bert-wwm-ext'\n",
    "        # tokenizer_name='chinese-roberta-wwm-ext'\n",
    "        # tokenizer_name='chinese-roberta-wwm-ext-large'\n",
    "        # tokenizer_name='/mnt/cfs/huangzhiwei/pykt-moekt/SBM/bge-large-en-v1.5'\n",
    "        # tokenizer_name='/mnt/cfs/huangzhiwei/BAE2025/models/bge-base-en-v1.5'\n",
    "        tokenizer_name='/mnt/cfs/huangzhiwei/BAE2025/models/bert-base-uncased'\n",
    "        # tokenizer_name='/mnt/cfs/huangzhiwei/BAE2025/models/deberta-v3-base'\n",
    "    ):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "        self.shuffle = shuffle\n",
    "        self.drop_last = drop_last\n",
    "        \n",
    "        if device is None:\n",
    "            self.device = torch.device(\n",
    "                'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            )\n",
    "        else:\n",
    "            self.device = device\n",
    "        \n",
    "        self.loader = DataLoader(\n",
    "            dataset=self.dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=self.collate_fn,\n",
    "            shuffle=self.shuffle,\n",
    "            drop_last=self.drop_last\n",
    "        )\n",
    "    \n",
    "    def collate_fn(self, data):\n",
    "        sents = [i[0] for i in data]\n",
    "        labels = [i[1] for i in data]\n",
    "        indices_info = [i[2] for i in data]  # 获取索引信息\n",
    "        \n",
    "        # 处理两个句子的情况\n",
    "        encoded_data = self.tokenizer.batch_encode_plus(\n",
    "            batch_text_or_text_pairs=[(sent[0], sent[1]) for sent in sents],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt',\n",
    "            return_length=True\n",
    "        )\n",
    "        \n",
    "        input_ids = encoded_data['input_ids'].to(self.device)\n",
    "        attention_mask = encoded_data['attention_mask'].to(self.device)\n",
    "        token_type_ids = encoded_data['token_type_ids'].to(self.device)\n",
    "        labels = torch.LongTensor(labels).to(self.device)\n",
    "        \n",
    "        # 返回原始索引信息\n",
    "        return input_ids, attention_mask, token_type_ids, labels, indices_info\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for data in self.loader:\n",
    "            yield data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check 一下数据是不是对的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 校验上述数据加载器出来的数据是否正确，输出数据出来看一看\n",
    "# train_data_path = '../data/train.json'\n",
    "# val_data_path = '../data/valid.json'\n",
    "# train_dataset = BAE2025Dataset(train_data_path)\n",
    "# train_dataloader = BAE2025DataLoader(train_dataset, batch_size=1)\n",
    "# val_dataset = BAE2025Dataset(val_data_path)\n",
    "# val_dataloader = BAE2025DataLoader(val_dataset, batch_size=1)\n",
    "\n",
    "# cnt_train=0\n",
    "# for batch in train_dataloader:\n",
    "#     cnt_train += 1\n",
    "#     input_ids, attention_mask, token_type_ids, labels = batch\n",
    "#     # print(input_ids.shape, attention_mask.shape, token_type_ids.shape, coarse_labels.shape, fine_labels.shape)\n",
    "#     # print(input_ids.shape, attention_mask.shape, token_type_ids.shape, labels.shape)\n",
    "#     # break\n",
    "# print(\"train data size:\", cnt_train)\n",
    "\n",
    "# cnt_val=0\n",
    "# for batch in val_dataloader:\n",
    "#     cnt_val += 1\n",
    "#     input_ids, attention_mask, token_type_ids, labels = batch\n",
    "#     # print(input_ids.shape, attention_mask.shape, token_type_ids.shape, coarse_labels.shape, fine_labels.shape)\n",
    "#     # print(input_ids.shape, attention_mask.shape, token_type_ids.shape, labels.shape)\n",
    "#     # break\n",
    "# print(\"val data size:\", cnt_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 检查一下dataset中处理出来的数据是否正确\n",
    "# for data in train_dataset:\n",
    "#     sent, label = data\n",
    "#     # print(sent, coarse_label, fine_label, sent_id)\n",
    "#     print(sent, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from transformers import BertModel\n",
    "\n",
    "# class BertClassifier(nn.Module):\n",
    "#     def __init__(self, pretrained_model_name, num_classes=3, freeze_pooler=False, dropout=0.2):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.freeze_pooler = freeze_pooler\n",
    "#         self.bert = BertModel.from_pretrained(pretrained_model_name)\n",
    "#         self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "#         if freeze_pooler:\n",
    "#             for param in self.bert.pooler.parameters():\n",
    "#                 param.requires_grad = False\n",
    "\n",
    "#         # self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(self.bert.config.hidden_size, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(256, num_classes)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "#     # def forward(self, input_ids, attention_mask):\n",
    "#         outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "#         # outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "#         pooled_output = outputs.pooler_output\n",
    "\n",
    "#         pooled_output = self.dropout(pooled_output)\n",
    "\n",
    "#         logits = self.classifier(pooled_output)\n",
    "        \n",
    "#         # logits = torch.sigmoid(logits)\n",
    "\n",
    "#         return logits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "\n",
    "class BertClassificationHead(nn.Module):\n",
    "    def __init__(self, hidden_size=1024, num_classes=3, dropout_prob=0.3):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.out_proj = nn.Linear(hidden_size, num_classes)  # (输入维度，输出维度)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = features[-1][:, 0, :]  # features[-1]是一个三维张量，其维度为[批次大小, 序列长度, 隐藏大小]。\n",
    "        x = self.dropout(x)  # 这是一种正则化技术，用于防止模型过拟合。在训练过程中，它通过随机将输入张量中的一部分元素设置为0，来增加模型的泛化能力。\n",
    "        x = self.dense(x)  # 这是一个全连接层，它将输入特征映射到一个新的特征空间。这是通过学习一个权重矩阵和一个偏置向量，并使用它们对输入特征进行线性变换来实现的，方便后续可以引入非线性变换。\n",
    "        x = torch.tanh(x)  # 这是一个激活函数，它将线性层的输出转换为非线性，使得模型可以学习并表示更复杂的模式。\n",
    "        x = self.dropout(x)  # 增加模型的泛化能力。\n",
    "        x = self.out_proj(x)  # 这是最后的全连接层，它将特征映射到最终的输出空间。在这个例子中，输出空间的维度等于分类任务的类别数量。\n",
    "        return x\n",
    "    \n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model_name, num_classes=3, freeze_pooler=0, dropout=0.3, hidden_size=768):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert = BertModel.from_pretrained(pretrained_model_name, output_hidden_states=True)\n",
    "        \n",
    "        # 冻结BERT底层，保留顶层微调\n",
    "        if freeze_pooler > 0:\n",
    "            modules = [self.bert.embeddings, *self.bert.encoder.layer[:freeze_pooler]]\n",
    "            for module in modules:\n",
    "                for param in module.parameters():\n",
    "                    param.requires_grad = False\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # 获取bert隐藏层大小\n",
    "        bert_hidden_size = self.bert.config.hidden_size\n",
    "        \n",
    "        self.classifier = BertClassificationHead(\n",
    "            hidden_size=self.bert.config.hidden_size,\n",
    "            num_classes=3,  # 三分类任务\n",
    "            dropout_prob=dropout\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        # 获取BERT输出\n",
    "        outputs = self.bert(\n",
    "            input_ids, \n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "            # output_hidden_states=True  # 获取所有隐藏层\n",
    "        )\n",
    "        \n",
    "        # 使用[CLS]表示的序列表示\n",
    "        # pooled_output = outputs.pooler_output\n",
    "        \n",
    "        # 可选：结合最后四层的[CLS]表示以获取更丰富的信息\n",
    "        # last_4_layers = outputs.hidden_states[-4:]\n",
    "        # cls_embeddings = torch.stack([layer[:, 0, :] for layer in last_4_layers], dim=0)\n",
    "        # pooled_output = torch.mean(cls_embeddings, dim=0)  # 平均最后四层\n",
    "        \n",
    "        # 应用dropout\n",
    "        # pooled_output = self.dropout1(pooled_output)\n",
    "        \n",
    "        # 分类\n",
    "        logits = self.classifier(outputs.hidden_states)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果在Jupyter Notebook中运行，可以使用这个自定义参数函数替代argparser\n",
    "def get_default_configs():\n",
    "    \"\"\"在Jupyter环境中使用的默认配置，避免argparse解析错误\"\"\"\n",
    "    class Args:\n",
    "        def __init__(self):\n",
    "            # self.model_name = '/mnt/cfs/huangzhiwei/pykt-moekt/SBM/bge-large-en-v1.5'\n",
    "            # self.model_name = \"/mnt/cfs/huangzhiwei/BAE2025/models/ModernBERT-large\"\n",
    "            # self.model_name = '/mnt/cfs/huangzhiwei/pykt-moekt/SBM/xlm-roberta-large'\n",
    "            # self.model_name = '/mnt/cfs/huangzhiwei/BAE2025/models/bge-base-en-v1.5'\n",
    "            self.model_name = '/mnt/cfs/huangzhiwei/BAE2025/models/bert-base-uncased'\n",
    "            # self.model_name = '/mnt/cfs/huangzhiwei/BAE2025/models/deberta-v3-base'\n",
    "            self.num_classes = 3\n",
    "            self.dropout = 0.3\n",
    "            self.freeze_pooler = 8\n",
    "            self.batch_size = 16\n",
    "            self.max_length = 512\n",
    "            self.lr = 1e-5\n",
    "            self.epochs = 50\n",
    "            self.device = device\n",
    "            self.name = None\n",
    "            self.seed = 42\n",
    "            self.data_path = '../data/train.json'\n",
    "            self.val_data_path = '../data/valid.json'\n",
    "            self.checkpoint_dir = 'checkpoints_error'\n",
    "            self.patience = 8\n",
    "            self.exp_name = 'BAE2025_track4_bert'\n",
    "            \n",
    "            # 新增参数 - 用于错误提取功能\n",
    "            self.mode = 'train'  # 默认为训练模式，可选值: 'train', 'extract_errors'\n",
    "            self.model_path = None  # 默认为None，当mode为'extract_errors'时使用\n",
    "    \n",
    "    return Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Jupyter environment, using default configs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/50: 100%|██████████████████| 123/123 [00:10<00:00, 11.31batch/s, accuracy=0.688, loss=0.930]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.9581\n",
      "Training Accuracy: 0.5878\n",
      "Training F1 Score: 0.3560\n",
      "Validation Loss: 0.8338 Acc: 0.6687 F1: 0.4265\n",
      "Number of error examples: 166\n",
      "Error examples saved to checkpoints_error/bert-base-uncased_fp_b16_e50_len512_lr1e-05/error_analysis/error_examples_epoch_1.json\n",
      "New best model saved with F1: 0.4265, Acc: 0.6687\n",
      "Best model error examples saved to checkpoints_error/bert-base-uncased_fp_b16_e50_len512_lr1e-05/error_analysis/best_model_error_examples.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████████████| 123/123 [00:10<00:00, 11.71batch/s, accuracy=0.500, loss=1.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.8540\n",
      "Training Accuracy: 0.6466\n",
      "Training F1 Score: 0.4491\n",
      "Validation Loss: 0.8012 Acc: 0.6946 F1: 0.4789\n",
      "Number of error examples: 153\n",
      "Error examples saved to checkpoints_error/bert-base-uncased_fp_b16_e50_len512_lr1e-05/error_analysis/error_examples_epoch_2.json\n",
      "New best model saved with F1: 0.4789, Acc: 0.6946\n",
      "Best model error examples saved to checkpoints_error/bert-base-uncased_fp_b16_e50_len512_lr1e-05/error_analysis/best_model_error_examples.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████████████| 123/123 [00:10<00:00, 11.67batch/s, accuracy=0.625, loss=0.739]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.8079\n",
      "Training Accuracy: 0.6592\n",
      "Training F1 Score: 0.4715\n",
      "Validation Loss: 0.7987 Acc: 0.7126 F1: 0.5541\n",
      "Number of error examples: 144\n",
      "Error examples saved to checkpoints_error/bert-base-uncased_fp_b16_e50_len512_lr1e-05/error_analysis/error_examples_epoch_3.json\n",
      "New best model saved with F1: 0.5541, Acc: 0.7126\n",
      "Best model error examples saved to checkpoints_error/bert-base-uncased_fp_b16_e50_len512_lr1e-05/error_analysis/best_model_error_examples.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50:  41%|███████▋           | 50/123 [00:04<00:06, 11.42batch/s, accuracy=0.750, loss=0.679]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 516\u001b[0m\n\u001b[1;32m    513\u001b[0m     extract_error_examples(configs, configs\u001b[38;5;241m.\u001b[39mmodel_path)\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;66;03m# 正常训练模式\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 118\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(configs)\u001b[0m\n\u001b[1;32m    115\u001b[0m accuracy_all \u001b[38;5;241m=\u001b[39m (preds \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# 收集预测结果和真实标签，用于计算F1\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m train_preds\u001b[38;5;241m.\u001b[39mextend(\u001b[43mpreds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m    119\u001b[0m train_labels_list\u001b[38;5;241m.\u001b[39mextend(labels\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m    121\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(configs):\n",
    "    \n",
    "    # 设置随机种子\n",
    "    random.seed(configs.seed)\n",
    "    np.random.seed(configs.seed)\n",
    "    torch.manual_seed(configs.seed)\n",
    "    \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # 创建检查点目录\n",
    "    checkpoint_dir = os.path.join(configs.checkpoint_dir, configs.exp_name)\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # 为保存混淆矩阵创建目录 - 分别为训练集和验证集创建\n",
    "    train_plot_dir = os.path.join(checkpoint_dir, 'plots', 'train')\n",
    "    val_plot_dir = os.path.join(checkpoint_dir, 'plots', 'val')\n",
    "    os.makedirs(train_plot_dir, exist_ok=True)\n",
    "    os.makedirs(val_plot_dir, exist_ok=True)\n",
    "    \n",
    "    # 创建错误预测保存目录\n",
    "    error_analysis_dir = os.path.join(checkpoint_dir, 'error_analysis')\n",
    "    os.makedirs(error_analysis_dir, exist_ok=True)\n",
    "    \n",
    "    # 加载数据集\n",
    "    train_dataset = BAE2025Dataset(configs.data_path)\n",
    "    val_dataset = BAE2025Dataset(configs.val_data_path)    \n",
    "\n",
    "    # 创建数据加载器\n",
    "    train_dataloader = BAE2025DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=configs.batch_size,\n",
    "        max_length=configs.max_length,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        device=configs.device,\n",
    "        tokenizer_name=configs.model_name\n",
    "    )\n",
    "\n",
    "    val_dataloader = BAE2025DataLoader(\n",
    "        dataset=val_dataset,\n",
    "        batch_size=configs.batch_size,\n",
    "        max_length=configs.max_length,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        device=configs.device,\n",
    "        tokenizer_name=configs.model_name\n",
    "    )\n",
    "    \n",
    "    # 创建模型\n",
    "    model = BertClassifier(\n",
    "        pretrained_model_name=configs.model_name,\n",
    "        num_classes=configs.num_classes,\n",
    "        freeze_pooler=configs.freeze_pooler,\n",
    "        dropout=configs.dropout\n",
    "    ).to(configs.device)\n",
    "\n",
    "    # 获取对应的tokenizer用于后面解码预测错误的样本\n",
    "    tokenizer = AutoTokenizer.from_pretrained(configs.model_name)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # 定义优化器\n",
    "    optimizer = AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=configs.lr\n",
    "    )\n",
    "\n",
    "    # 初始化最佳验证损失和早停计数器\n",
    "    best_val_acc = 0.0\n",
    "    best_val_f1 = 0.0  # 添加F1分数作为评估指标\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # 定义类别名称\n",
    "    class_names = ['Yes', 'To some extent', 'No']\n",
    "    \n",
    "    # 添加F1计算所需的库\n",
    "    from sklearn.metrics import f1_score, confusion_matrix\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # 训练循环\n",
    "    for epoch in range(configs.epochs):\n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        train_preds = []\n",
    "        train_labels_list = []\n",
    "        \n",
    "        with tqdm(\n",
    "            train_dataloader,\n",
    "            total=len(train_dataloader),\n",
    "            desc=f'Epoch {epoch + 1}/{configs.epochs}',\n",
    "            unit='batch',\n",
    "            ncols=100\n",
    "        ) as pbar:\n",
    "            for input_ids, attention_mask, token_type_ids, labels, indices_info in pbar:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # 前向传播\n",
    "                logits = model(input_ids, attention_mask, token_type_ids)\n",
    "                \n",
    "                # 计算损失 - 确保labels是长整型\n",
    "                labels = labels.long()\n",
    "                loss = criterion(logits, labels)\n",
    "                \n",
    "                # 反向传播\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                preds = logits.argmax(dim=1)\n",
    "                accuracy = (preds == labels).float().mean()\n",
    "                accuracy_all = (preds == labels).float().sum()\n",
    "                \n",
    "                # 收集预测结果和真实标签，用于计算F1\n",
    "                train_preds.extend(preds.cpu().numpy())\n",
    "                train_labels_list.extend(labels.cpu().numpy())\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                train_acc += accuracy_all.item()\n",
    "                \n",
    "                pbar.set_postfix(\n",
    "                    loss=f'{loss.item():.3f}',\n",
    "                    accuracy=f'{accuracy.item():.3f}'\n",
    "                )\n",
    "        \n",
    "        train_loss = train_loss / len(train_dataloader)\n",
    "        train_acc = train_acc / len(train_dataset)\n",
    "        \n",
    "        # 计算训练集的F1分数 - 使用macro平均以处理多分类\n",
    "        train_f1 = f1_score(train_labels_list, train_preds, average='macro')\n",
    "        \n",
    "        print(f'Training Loss: {train_loss:.4f}')\n",
    "        print(f'Training Accuracy: {train_acc:.4f}')\n",
    "        print(f'Training F1 Score: {train_f1:.4f}')\n",
    "        \n",
    "        # 创建训练集的混淆矩阵\n",
    "        # 创建三个二分类混淆矩阵（两两类别之间）\n",
    "        class_pairs = [\n",
    "            ([0, 1], ['Yes', 'To some extent']),  # Yes vs To some extent\n",
    "            ([0, 2], ['Yes', 'No']),              # Yes vs No\n",
    "            ([1, 2], ['To some extent', 'No'])    # To some extent vs No\n",
    "        ]\n",
    "        \n",
    "        for classes_idx, classes_names in class_pairs:\n",
    "            # 筛选出对应两个类别的预测和标签\n",
    "            mask = np.isin(np.array(train_labels_list), classes_idx)\n",
    "            filtered_preds = np.array(train_preds)[mask]\n",
    "            filtered_labels = np.array(train_labels_list)[mask]\n",
    "            \n",
    "            # 创建混淆矩阵\n",
    "            cm = confusion_matrix(filtered_labels, filtered_preds, labels=classes_idx)\n",
    "            \n",
    "            # 计算此对类别的准确率和F1分数\n",
    "            pair_mask = np.isin(np.array(train_labels_list), classes_idx)\n",
    "            pair_acc = np.mean(np.array(train_preds)[pair_mask] == np.array(train_labels_list)[pair_mask])\n",
    "            # 计算二分类F1分数\n",
    "            pair_f1 = f1_score(\n",
    "                np.array(train_labels_list)[pair_mask], \n",
    "                np.array(train_preds)[pair_mask], \n",
    "                labels=classes_idx, \n",
    "                average='macro'\n",
    "            )\n",
    "            \n",
    "            # 绘制混淆矩阵\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                        xticklabels=[classes_names[i == classes_idx[1]] for i in classes_idx],\n",
    "                        yticklabels=[classes_names[i == classes_idx[1]] for i in classes_idx])\n",
    "            plt.xlabel('Predicted')\n",
    "            plt.ylabel('True')\n",
    "            plt.title(f'Train: {classes_names[0]} vs {classes_names[1]}\\nAcc: {pair_acc:.4f}, F1: {pair_f1:.4f}')\n",
    "            \n",
    "            # 保存图表\n",
    "            matrix_path = os.path.join(train_plot_dir, f'cm_{classes_names[0].replace(\" \", \"_\")}_{classes_names[1].replace(\" \", \"_\")}_epoch_{epoch+1}.png')\n",
    "            plt.savefig(matrix_path)\n",
    "            plt.close()\n",
    "            \n",
    "        # 创建完整的三分类混淆矩阵\n",
    "        cm_full = confusion_matrix(train_labels_list, train_preds, labels=[0, 1, 2])\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm_full, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=class_names,\n",
    "                    yticklabels=class_names)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title(f'Train: Full Confusion Matrix\\nAcc: {train_acc:.4f}, F1: {train_f1:.4f}')\n",
    "        \n",
    "        # 保存完整混淆矩阵\n",
    "        matrix_path = os.path.join(train_plot_dir, f'cm_full_epoch_{epoch+1}.png')\n",
    "        plt.savefig(matrix_path)\n",
    "        plt.close()\n",
    "        \n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_corrects = 0.0\n",
    "        val_preds = []\n",
    "        val_labels_list = []\n",
    "        \n",
    "        # 用于存储错误预测的信息\n",
    "        error_examples = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (input_ids, attention_mask, token_type_ids, labels, indices_info) in enumerate(val_dataloader):\n",
    "                # 确保labels是长整型\n",
    "                labels = labels.long()\n",
    "                \n",
    "                # 前向传播\n",
    "                logits = model(input_ids, attention_mask, token_type_ids)\n",
    "                \n",
    "                loss = criterion(logits, labels)\n",
    "                val_loss += loss.item()\n",
    "                preds = logits.argmax(dim=1)\n",
    "                \n",
    "                # 找出本批次中预测错误的样本\n",
    "                error_indices = (preds != labels).nonzero(as_tuple=True)[0]\n",
    "                \n",
    "                # 如果有错误预测，使用原始索引获取文本内容并保存\n",
    "                if len(error_indices) > 0:\n",
    "                    for idx in error_indices:\n",
    "                        # 获取样本的原始索引信息\n",
    "                        sample_indices = indices_info[idx]\n",
    "                        data_idx = sample_indices['data_idx']\n",
    "                        item_idx = sample_indices['item_idx']\n",
    "                        response_id = sample_indices['response_id']\n",
    "                        \n",
    "                        # 从原始数据中获取文本，而不是解码\n",
    "                        original_item = val_dataset.original_data[item_idx]\n",
    "                        conversation_history = original_item['conversation_history']\n",
    "                        response_text = original_item['tutor_responses'][response_id]['response']\n",
    "                        \n",
    "                        # 获取真实标签和预测标签\n",
    "                        true_label = labels[idx].item()\n",
    "                        pred_label = preds[idx].item()\n",
    "                        \n",
    "                        # 记录错误预测信息 - 使用原始文本\n",
    "                        error_examples.append({\n",
    "                            'first_sentence': conversation_history,\n",
    "                            'second_sentence': response_text,\n",
    "                            'true_label': class_names[true_label],\n",
    "                            'pred_label': class_names[pred_label],\n",
    "                            'data_idx': data_idx,\n",
    "                            'item_idx': item_idx,\n",
    "                            'response_id': response_id\n",
    "                        })\n",
    "                \n",
    "                accuracy = (preds == labels).float().sum()\n",
    "                val_corrects += accuracy\n",
    "                \n",
    "                # 收集预测结果和真实标签，用于计算F1和混淆矩阵\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels_list.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_loss = val_loss / len(val_dataloader)\n",
    "        val_acc = val_corrects.double() / len(val_dataset)\n",
    "        \n",
    "        # 计算验证集的F1分数\n",
    "        val_f1 = f1_score(val_labels_list, val_preds, average='macro')\n",
    "        \n",
    "        print('Validation Loss: {:.4f} Acc: {:.4f} F1: {:.4f}'.format(val_loss, val_acc, val_f1))\n",
    "        print(f'Number of error examples: {len(error_examples)}')\n",
    "        \n",
    "        # 保存错误预测的样本到JSON文件\n",
    "        error_file_path = os.path.join(error_analysis_dir, f'error_examples_epoch_{epoch+1}.json')\n",
    "        with open(error_file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(error_examples, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        print(f'Error examples saved to {error_file_path}')\n",
    "        \n",
    "        # 创建验证集三个二分类混淆矩阵（两两类别之间）\n",
    "        class_pairs = [\n",
    "            ([0, 1], ['Yes', 'To some extent']),  # Yes vs To some extent\n",
    "            ([0, 2], ['Yes', 'No']),              # Yes vs No\n",
    "            ([1, 2], ['To some extent', 'No'])    # To some extent vs No\n",
    "        ]\n",
    "        \n",
    "        for classes_idx, classes_names in class_pairs:\n",
    "            # 筛选出对应两个类别的预测和标签\n",
    "            mask = np.isin(np.array(val_labels_list), classes_idx)\n",
    "            filtered_preds = np.array(val_preds)[mask]\n",
    "            filtered_labels = np.array(val_labels_list)[mask]\n",
    "            \n",
    "            # 创建混淆矩阵\n",
    "            cm = confusion_matrix(filtered_labels, filtered_preds, labels=classes_idx)\n",
    "            \n",
    "            # 计算此对类别的准确率和F1分数\n",
    "            pair_mask = np.isin(np.array(val_labels_list), classes_idx)\n",
    "            pair_acc = np.mean(np.array(val_preds)[pair_mask] == np.array(val_labels_list)[pair_mask])\n",
    "            # 计算二分类F1分数\n",
    "            pair_f1 = f1_score(\n",
    "                np.array(val_labels_list)[pair_mask], \n",
    "                np.array(val_preds)[pair_mask], \n",
    "                labels=classes_idx, \n",
    "                average='macro'\n",
    "            )\n",
    "            \n",
    "            # 绘制混淆矩阵\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                        xticklabels=[classes_names[i == classes_idx[1]] for i in classes_idx],\n",
    "                        yticklabels=[classes_names[i == classes_idx[1]] for i in classes_idx])\n",
    "            plt.xlabel('Predicted')\n",
    "            plt.ylabel('True')\n",
    "            plt.title(f'Val: {classes_names[0]} vs {classes_names[1]}\\nAcc: {pair_acc:.4f}, F1: {pair_f1:.4f}')\n",
    "            \n",
    "            # 保存图表\n",
    "            matrix_path = os.path.join(val_plot_dir, f'cm_{classes_names[0].replace(\" \", \"_\")}_{classes_names[1].replace(\" \", \"_\")}_epoch_{epoch+1}.png')\n",
    "            plt.savefig(matrix_path)\n",
    "            plt.close()\n",
    "            \n",
    "        # 创建完整的三分类混淆矩阵\n",
    "        cm_full = confusion_matrix(val_labels_list, val_preds, labels=[0, 1, 2])\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm_full, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=class_names,\n",
    "                    yticklabels=class_names)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title(f'Val: Full Confusion Matrix\\nAcc: {val_acc:.4f}, F1: {val_f1:.4f}')\n",
    "        \n",
    "        # 保存完整混淆矩阵\n",
    "        matrix_path = os.path.join(val_plot_dir, f'cm_full_epoch_{epoch+1}.png')\n",
    "        plt.savefig(matrix_path)\n",
    "        plt.close()\n",
    "        \n",
    "        # 检查是否保存模型并判断是否需要早停\n",
    "        # 使用F1分数作为主要指标\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            best_val_acc = val_acc\n",
    "            \n",
    "            # 保存模型\n",
    "            # state_dict = model.state_dict()\n",
    "            # torch.save(state_dict, os.path.join(checkpoint_dir, 'best_model_f1.pt'))\n",
    "            print(f'New best model saved with F1: {best_val_f1:.4f}, Acc: {best_val_acc:.4f}')\n",
    "            \n",
    "            # 同时保存这个最佳模型的错误预测样本\n",
    "            best_error_file_path = os.path.join(error_analysis_dir, 'best_model_error_examples.json')\n",
    "            with open(best_error_file_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(error_examples, f, ensure_ascii=False, indent=4)\n",
    "            print(f'Best model error examples saved to {best_error_file_path}')\n",
    "            \n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= configs.patience:\n",
    "                print(f'Early stopping triggered after {epoch+1} epochs.')\n",
    "                break\n",
    "\n",
    "        model.train()\n",
    "\n",
    "\n",
    "# 同样修改extract_error_examples函数\n",
    "def extract_error_examples(configs, model_path):\n",
    "    \"\"\"\n",
    "    从已训练的模型中提取验证集上的错误预测\n",
    "    \n",
    "    Args:\n",
    "        configs: 配置对象\n",
    "        model_path: 已训练模型的路径\n",
    "    \"\"\"\n",
    "    import json\n",
    "    from transformers import AutoTokenizer\n",
    "    \n",
    "    # 确保输出目录存在\n",
    "    error_analysis_dir = os.path.join(configs.checkpoint_dir, configs.exp_name, 'error_analysis')\n",
    "    os.makedirs(error_analysis_dir, exist_ok=True)\n",
    "    \n",
    "    # 加载验证数据集\n",
    "    val_dataset = BAE2025Dataset(configs.val_data_path)\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    val_dataloader = BAE2025DataLoader(\n",
    "        dataset=val_dataset,\n",
    "        batch_size=configs.batch_size,\n",
    "        max_length=configs.max_length,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        device=configs.device,\n",
    "        tokenizer_name=configs.model_name\n",
    "    )\n",
    "    \n",
    "    # 加载tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(configs.model_name)\n",
    "    \n",
    "    # 创建模型\n",
    "    model = BertClassifier(\n",
    "        pretrained_model_name=configs.model_name,\n",
    "        num_classes=configs.num_classes,\n",
    "        freeze_pooler=configs.freeze_pooler,\n",
    "        dropout=configs.dropout\n",
    "    ).to(configs.device)\n",
    "    \n",
    "    # 加载模型权重\n",
    "    model.load_state_dict(torch.load(model_path, map_location=configs.device))\n",
    "    model.eval()\n",
    "    \n",
    "    # 定义类别名称\n",
    "    class_names = ['Yes', 'To some extent', 'No']\n",
    "    \n",
    "    # 用于存储错误预测的信息\n",
    "    error_examples = []\n",
    "    all_examples = []  # 存储所有样本信息（可选）\n",
    "    \n",
    "    # 预测和收集错误\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (input_ids, attention_mask, token_type_ids, labels, indices_info) in enumerate(tqdm(val_dataloader, desc=\"Evaluating\")):\n",
    "            # 确保labels是长整型\n",
    "            labels = labels.long()\n",
    "            \n",
    "            # 前向传播\n",
    "            logits = model(input_ids, attention_mask, token_type_ids)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            \n",
    "            # 对批次中的每个样本进行处理\n",
    "            for idx in range(len(labels)):\n",
    "                # 获取样本的原始索引信息\n",
    "                sample_indices = indices_info[idx]\n",
    "                data_idx = sample_indices['data_idx']\n",
    "                item_idx = sample_indices['item_idx']\n",
    "                response_id = sample_indices['response_id']\n",
    "                \n",
    "                # 从原始数据中获取文本\n",
    "                original_item = val_dataset.original_data[item_idx]\n",
    "                conversation_history = original_item['conversation_history']\n",
    "                response_text = original_item['tutor_responses'][response_id]['response']\n",
    "                \n",
    "                # 获取真实标签和预测标签\n",
    "                true_label = labels[idx].item()\n",
    "                pred_label = preds[idx].item()\n",
    "                \n",
    "                # 创建样本信息\n",
    "                example_info = {\n",
    "                    'first_sentence': conversation_history,\n",
    "                    'second_sentence': response_text,\n",
    "                    'true_label': class_names[true_label],\n",
    "                    'pred_label': class_names[pred_label],\n",
    "                    'is_error': true_label != pred_label,\n",
    "                    'data_idx': data_idx,\n",
    "                    'item_idx': item_idx,\n",
    "                    'response_id': response_id\n",
    "                }\n",
    "                \n",
    "                # 如果是错误预测，添加到错误样本列表\n",
    "                if true_label != pred_label:\n",
    "                    error_examples.append(example_info)\n",
    "                \n",
    "                # 可选：存储所有样本信息\n",
    "                all_examples.append(example_info)\n",
    "    \n",
    "    # 保存错误预测的样本到JSON文件\n",
    "    error_file_path = os.path.join(error_analysis_dir, 'error_examples.json')\n",
    "    with open(error_file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(error_examples, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(f'Found {len(error_examples)} error examples out of {len(val_dataset)} samples ({len(error_examples)/len(val_dataset)*100:.2f}%)')\n",
    "    print(f'Error examples saved to {error_file_path}')\n",
    "    \n",
    "    # 可选：保存所有样本的预测结果\n",
    "    all_examples_path = os.path.join(error_analysis_dir, 'all_examples.json')\n",
    "    with open(all_examples_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_examples, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(f'All examples with predictions saved to {all_examples_path}')\n",
    "    \n",
    "    return error_examples\n",
    "\n",
    "\n",
    "# 在以下主函数中添加判断Jupyter环境的逻辑\n",
    "if __name__ == '__main__':\n",
    "    # 判断是否在Jupyter环境中运行\n",
    "    try:\n",
    "        # 检查是否在Jupyter中运行\n",
    "        get_ipython = globals().get('get_ipython', None)\n",
    "        if get_ipython and 'IPKernelApp' in get_ipython().config:\n",
    "            # 在Jupyter环境中运行，使用默认配置\n",
    "            print(\"Running in Jupyter environment, using default configs\")\n",
    "            configs = get_default_configs()\n",
    "        else:\n",
    "            # 在命令行环境中运行，使用argparse\n",
    "            configs = argparser()\n",
    "    except:\n",
    "        # 任何异常都使用argparse处理\n",
    "        configs = argparser()\n",
    "    \n",
    "    # 设置实验名称\n",
    "    if configs.name is None:\n",
    "        configs.exp_name = \\\n",
    "            f'{os.path.basename(configs.model_name)}' + \\\n",
    "            f'{\"_fp\" if configs.freeze_pooler else \"\"}' + \\\n",
    "            f'_b{configs.batch_size}_e{configs.epochs}' + \\\n",
    "            f'_len{configs.max_length}_lr{configs.lr}'\n",
    "    else:\n",
    "        configs.exp_name = configs.name\n",
    "    \n",
    "    # 设置设备\n",
    "    if configs.device is None:\n",
    "        configs.device = torch.device(\n",
    "            'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        )\n",
    "    \n",
    "    # 根据命令行参数选择操作模式\n",
    "    if hasattr(configs, 'mode') and configs.mode == 'extract_errors':\n",
    "        # 如果是提取错误模式，则运行错误提取函数\n",
    "        if not hasattr(configs, 'model_path') or configs.model_path is None:\n",
    "            # 如果没有指定模型路径，使用默认的最佳模型路径\n",
    "            configs.model_path = os.path.join(configs.checkpoint_dir, configs.exp_name, 'best_model_f1.pt')\n",
    "        \n",
    "        print(f\"Extracting error examples using model: {configs.model_path}\")\n",
    "        extract_error_examples(configs, configs.model_path)\n",
    "    else:\n",
    "        # 正常训练模式\n",
    "        train(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
