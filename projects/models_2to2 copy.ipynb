{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# 设置环境变量，只让程序看到 GPU 2\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import BertModel, AutoModel\n",
    "from transformers import AdamW\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "\n",
    "class BAE2025Dataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            data_path,\n",
    "            labels={\n",
    "                \"Yes\": 0,\n",
    "                \"To some extent\": 1, \n",
    "                \"No\": 2,\n",
    "            }\n",
    "    ):\n",
    "        self.data_path = data_path\n",
    "        self.labels = labels\n",
    "        self._get_data()\n",
    "    \n",
    "    def _get_data(self):\n",
    "        with open(self.data_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        self.data = []\n",
    "        for item in data:\n",
    "            tutor_responses = item['tutor_responses']\n",
    "            for response in tutor_responses.values():\n",
    "                sent1 = item['conversation_history']\n",
    "                sent2 = response['response']\n",
    "                label = response['annotation'][\"Providing_Guidance\"]\n",
    "                if label in self.labels:\n",
    "                    self.data.append(((sent1, sent2), self.labels[label]))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def get_labels(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class BAE2025DataLoader:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        batch_size=16,\n",
    "        max_length=512,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        device=None,\n",
    "        # tokenizer_name='chinese-bert-wwm-ext'\n",
    "        # tokenizer_name='chinese-roberta-wwm-ext'\n",
    "        # tokenizer_name='chinese-roberta-wwm-ext-large'\n",
    "        # tokenizer_name='/mnt/cfs/huangzhiwei/pykt-moekt/SBM/bge-large-en-v1.5'\n",
    "        # tokenizer_name='/mnt/cfs/huangzhiwei/BAE2025/models/bge-base-en-v1.5'\n",
    "        tokenizer_name='/mnt/cfs/huangzhiwei/BAE2025/models/bert-base-uncased'\n",
    "        # tokenizer_name='/mnt/cfs/huangzhiwei/BAE2025/models/deberta-v3-base'\n",
    "    ):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "        self.shuffle = shuffle\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "        if device is None:\n",
    "            self.device = torch.device(\n",
    "                'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            )\n",
    "        else:\n",
    "            self.device = device\n",
    "\n",
    "        self.loader = DataLoader(\n",
    "            dataset=self.dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=self.collate_fn,\n",
    "            shuffle=self.shuffle,\n",
    "            drop_last=self.drop_last\n",
    "        )\n",
    "\n",
    "    def collate_fn(self, data):\n",
    "        sents = [i[0] for i in data]\n",
    "        labels = [i[1] for i in data]\n",
    "\n",
    "        # 修改这里，处理两个句子的情况\n",
    "        data = self.tokenizer.batch_encode_plus(\n",
    "            batch_text_or_text_pairs=[(sent[0], sent[1]) for sent in sents],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt',\n",
    "            return_length=True\n",
    "        )\n",
    "        input_ids = data['input_ids'].to(self.device)\n",
    "        attention_mask = data['attention_mask'].to(self.device)\n",
    "        token_type_ids = data['token_type_ids'].to(self.device)\n",
    "        labels = torch.LongTensor(labels).to(self.device)\n",
    "\n",
    "        return input_ids, attention_mask, token_type_ids, labels\n",
    "        # return input_ids, attention_mask, labels\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        for data in self.loader:\n",
    "            yield data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "# 修改模型类以支持分层分类\n",
    "class HierarchicalBertClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model_name, freeze_pooler=0, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 第一阶段分类器：Yes vs 非Yes\n",
    "        self.bert_stage1 = BertModel.from_pretrained(pretrained_model_name, output_hidden_states=True)\n",
    "        \n",
    "        # 第二阶段分类器：To some extent vs No\n",
    "        self.bert_stage2 = BertModel.from_pretrained(pretrained_model_name, output_hidden_states=True)\n",
    "        \n",
    "        # 冻结BERT底层，保留顶层微调\n",
    "        if freeze_pooler > 0:\n",
    "            # 冻结第一阶段模型的底层\n",
    "            modules1 = [self.bert_stage1.embeddings, *self.bert_stage1.encoder.layer[:freeze_pooler]]\n",
    "            for module in modules1:\n",
    "                for param in module.parameters():\n",
    "                    param.requires_grad = False\n",
    "            \n",
    "            # 冻结第二阶段模型的底层\n",
    "            modules2 = [self.bert_stage2.embeddings, *self.bert_stage2.encoder.layer[:freeze_pooler]]\n",
    "            for module in modules2:\n",
    "                for param in module.parameters():\n",
    "                    param.requires_grad = False\n",
    "        \n",
    "        # 获取bert隐藏层大小\n",
    "        bert_hidden_size = self.bert_stage1.config.hidden_size\n",
    "        \n",
    "        # 第一阶段的分类头（二分类：Yes vs 非Yes）\n",
    "        self.stage1_classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(bert_hidden_size, bert_hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(bert_hidden_size, 2)  # 二分类\n",
    "        )\n",
    "        \n",
    "        # 第二阶段的分类头（二分类：To some extent vs No）\n",
    "        self.stage2_classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(bert_hidden_size, bert_hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(bert_hidden_size, 2)  # 二分类\n",
    "        )\n",
    "    \n",
    "    def forward_stage1(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        \"\"\"第一阶段：预测是Yes还是非Yes\"\"\"\n",
    "        outputs = self.bert_stage1(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        # 使用[CLS]表示的序列表示\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        logits = self.stage1_classifier(cls_output)\n",
    "        return logits\n",
    "    \n",
    "    def forward_stage2(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        \"\"\"第二阶段：预测是To some extent还是No\"\"\"\n",
    "        outputs = self.bert_stage2(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        # 使用[CLS]表示的序列表示\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        logits = self.stage2_classifier(cls_output)\n",
    "        return logits\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None, stage=None):\n",
    "        \"\"\"根据stage参数进行相应阶段的前向传播\"\"\"\n",
    "        if stage == 1:\n",
    "            return self.forward_stage1(input_ids, attention_mask, token_type_ids)\n",
    "        elif stage == 2:\n",
    "            return self.forward_stage2(input_ids, attention_mask, token_type_ids)\n",
    "        else:\n",
    "            # 默认行为：完整的两阶段预测\n",
    "            # 第一阶段：预测是Yes还是非Yes\n",
    "            stage1_logits = self.forward_stage1(input_ids, attention_mask, token_type_ids)\n",
    "            stage1_preds = torch.argmax(stage1_logits, dim=1)\n",
    "            \n",
    "            # 第二阶段：对预测为\"非Yes\"的样本进行To some extent vs No预测\n",
    "            # 创建一个全为0的三分类输出张量（Yes=0, To some extent=1, No=2）\n",
    "            batch_size = input_ids.size(0)\n",
    "            final_logits = torch.zeros(batch_size, 3, device=input_ids.device)\n",
    "            \n",
    "            # 设置Yes的logits值（从stage1获取）\n",
    "            final_logits[:, 0] = stage1_logits[:, 0]  # Yes的logits\n",
    "            \n",
    "            # 获取预测为非Yes(1)的样本索引\n",
    "            non_yes_indices = (stage1_preds == 1).nonzero(as_tuple=True)[0]\n",
    "            \n",
    "            if len(non_yes_indices) > 0:\n",
    "                # 只对预测为\"非Yes\"的样本进行第二阶段预测\n",
    "                non_yes_input_ids = input_ids[non_yes_indices]\n",
    "                non_yes_attention_mask = attention_mask[non_yes_indices]\n",
    "                non_yes_token_type_ids = None if token_type_ids is None else token_type_ids[non_yes_indices]\n",
    "                \n",
    "                stage2_logits = self.forward_stage2(non_yes_input_ids, non_yes_attention_mask, non_yes_token_type_ids)\n",
    "                \n",
    "                # 将第二阶段的预测结果（To some extent vs No）放入最终结果中\n",
    "                final_logits[non_yes_indices, 1] = stage2_logits[:, 0]  # To some extent的logits\n",
    "                final_logits[non_yes_indices, 2] = stage2_logits[:, 1]  # No的logits\n",
    "            \n",
    "            return final_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "import random\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import AdamW\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "# 如果在Jupyter Notebook中运行，可以使用这个自定义参数函数替代argparser\n",
    "def get_default_configs():\n",
    "    \"\"\"在Jupyter环境中使用的默认配置，避免argparse解析错误\"\"\"\n",
    "    class Args:\n",
    "        def __init__(self):\n",
    "            # self.model_name = '/mnt/cfs/huangzhiwei/pykt-moekt/SBM/bge-large-en-v1.5'\n",
    "            # self.model_name = \"/mnt/cfs/huangzhiwei/BAE2025/models/ModernBERT-large\"\n",
    "            # self.model_name = '/mnt/cfs/huangzhiwei/pykt-moekt/SBM/xlm-roberta-large'\n",
    "            # self.model_name = '/mnt/cfs/huangzhiwei/BAE2025/models/bge-base-en-v1.5'\n",
    "            self.model_name = '/mnt/cfs/huangzhiwei/BAE2025/models/bert-base-uncased'\n",
    "            # self.model_name = '/mnt/cfs/huangzhiwei/BAE2025/models/deberta-v3-base'\n",
    "            self.num_classes = 3\n",
    "            self.dropout = 0.3\n",
    "            self.freeze_pooler = 8\n",
    "            self.batch_size = 16\n",
    "            self.max_length = 512\n",
    "            self.lr = 1e-5\n",
    "            self.epochs = 50\n",
    "            self.device = device\n",
    "            self.name = None\n",
    "            self.seed = 42\n",
    "            self.data_path = './data/train.json'\n",
    "            self.val_data_path = './data/valid.json'\n",
    "            self.checkpoint_dir = 'checkpoints_2to2_adjust'\n",
    "            self.patience = 8\n",
    "            self.exp_name = 'BAE2025_track4_bert'\n",
    "    return Args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Jupyter environment, using default configs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Epoch 1/50 =====\n",
      "Training Stage 1 (Yes vs Non-Yes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 1: 100%|█████████████████████████████████████| 123/123 [00:10<00:00, 11.25batch/s, loss=0.766]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 Training - Loss: 0.6432, Acc: 0.6336, F1: 0.5957\n",
      "Training Stage 2 (To some extent vs No)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 2: 100%|█████████████████████████████████████| 123/123 [00:05<00:00, 22.62batch/s, loss=0.658]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 Training - Loss: 0.6357, Acc: 0.6347, F1: 0.6345\n",
      "Overall Training - Acc: 0.6494, F1: 0.4962\n",
      "Evaluating on validation set...\n",
      "Stage 1 Validation - Acc: 0.7046, F1: 0.6266\n",
      "Stage 2 Validation - Acc: 0.6517, F1: 0.6513\n",
      "Overall Validation - Acc: 0.6806, F1: 0.4879\n",
      "Saved prediction results to checkpoints_2to2_adjust/hierarchical_bert-base-uncased_fp_b16_e50_len512_lr1e-05/predictions/train_predictions_epoch_1.json and checkpoints_2to2_adjust/hierarchical_bert-base-uncased_fp_b16_e50_len512_lr1e-05/predictions/val_predictions_epoch_1.json\n",
      "New best model saved with F1: 0.4879, Acc: 0.6806\n",
      "\n",
      "===== Epoch 2/50 =====\n",
      "Training Stage 1 (Yes vs Non-Yes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 1: 100%|█████████████████████████████████████| 123/123 [00:10<00:00, 11.47batch/s, loss=0.715]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 Training - Loss: 0.5916, Acc: 0.6814, F1: 0.6425\n",
      "Training Stage 2 (To some extent vs No)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 2: 100%|█████████████████████████████████████| 123/123 [00:05<00:00, 22.58batch/s, loss=0.588]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 Training - Loss: 0.5463, Acc: 0.7159, F1: 0.7156\n",
      "Overall Training - Acc: 0.5097, F1: 0.5246\n",
      "Evaluating on validation set...\n",
      "Stage 1 Validation - Acc: 0.6966, F1: 0.6815\n",
      "Stage 2 Validation - Acc: 0.6816, F1: 0.6793\n",
      "Overall Validation - Acc: 0.5110, F1: 0.5060\n",
      "Saved prediction results to checkpoints_2to2_adjust/hierarchical_bert-base-uncased_fp_b16_e50_len512_lr1e-05/predictions/train_predictions_epoch_2.json and checkpoints_2to2_adjust/hierarchical_bert-base-uncased_fp_b16_e50_len512_lr1e-05/predictions/val_predictions_epoch_2.json\n",
      "New best model saved with F1: 0.5060, Acc: 0.5110\n",
      "\n",
      "===== Epoch 3/50 =====\n",
      "Training Stage 1 (Yes vs Non-Yes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 1: 100%|█████████████████████████████████████| 123/123 [00:10<00:00, 11.62batch/s, loss=0.504]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 Training - Loss: 0.5572, Acc: 0.7058, F1: 0.6753\n",
      "Training Stage 2 (To some extent vs No)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 2: 100%|█████████████████████████████████████| 123/123 [00:05<00:00, 22.63batch/s, loss=0.746]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 Training - Loss: 0.5287, Acc: 0.7341, F1: 0.7339\n",
      "Overall Training - Acc: 0.7022, F1: 0.6616\n",
      "Evaluating on validation set...\n",
      "Stage 1 Validation - Acc: 0.7186, F1: 0.6789\n",
      "Stage 2 Validation - Acc: 0.6617, F1: 0.6607\n",
      "Overall Validation - Acc: 0.6048, F1: 0.5322\n",
      "Saved prediction results to checkpoints_2to2_adjust/hierarchical_bert-base-uncased_fp_b16_e50_len512_lr1e-05/predictions/train_predictions_epoch_3.json and checkpoints_2to2_adjust/hierarchical_bert-base-uncased_fp_b16_e50_len512_lr1e-05/predictions/val_predictions_epoch_3.json\n",
      "New best model saved with F1: 0.5322, Acc: 0.6048\n",
      "\n",
      "===== Epoch 4/50 =====\n",
      "Training Stage 1 (Yes vs Non-Yes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 1: 100%|█████████████████████████████████████| 123/123 [00:10<00:00, 11.61batch/s, loss=0.637]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 Training - Loss: 0.5203, Acc: 0.7337, F1: 0.7170\n",
      "Training Stage 2 (To some extent vs No)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 2: 100%|█████████████████████████████████████| 123/123 [00:05<00:00, 22.52batch/s, loss=0.406]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 Training - Loss: 0.4554, Acc: 0.7954, F1: 0.7953\n",
      "Overall Training - Acc: 0.7744, F1: 0.7091\n",
      "Evaluating on validation set...\n",
      "Stage 1 Validation - Acc: 0.7345, F1: 0.6921\n",
      "Stage 2 Validation - Acc: 0.6667, F1: 0.6626\n",
      "Overall Validation - Acc: 0.6747, F1: 0.5327\n",
      "Saved prediction results to checkpoints_2to2_adjust/hierarchical_bert-base-uncased_fp_b16_e50_len512_lr1e-05/predictions/train_predictions_epoch_4.json and checkpoints_2to2_adjust/hierarchical_bert-base-uncased_fp_b16_e50_len512_lr1e-05/predictions/val_predictions_epoch_4.json\n",
      "New best model saved with F1: 0.5327, Acc: 0.6747\n",
      "\n",
      "===== Epoch 5/50 =====\n",
      "Training Stage 1 (Yes vs Non-Yes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 1: 100%|█████████████████████████████████████| 123/123 [00:10<00:00, 11.59batch/s, loss=0.466]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 Training - Loss: 0.4510, Acc: 0.7891, F1: 0.7797\n",
      "Training Stage 2 (To some extent vs No)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 2: 100%|█████████████████████████████████████| 123/123 [00:05<00:00, 22.54batch/s, loss=0.625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 Training - Loss: 0.4201, Acc: 0.8092, F1: 0.8092\n",
      "Overall Training - Acc: 0.7937, F1: 0.7254\n",
      "Evaluating on validation set...\n",
      "Stage 1 Validation - Acc: 0.7126, F1: 0.6613\n",
      "Stage 2 Validation - Acc: 0.6716, F1: 0.6693\n",
      "Overall Validation - Acc: 0.6846, F1: 0.5216\n",
      "Saved prediction results to checkpoints_2to2_adjust/hierarchical_bert-base-uncased_fp_b16_e50_len512_lr1e-05/predictions/train_predictions_epoch_5.json and checkpoints_2to2_adjust/hierarchical_bert-base-uncased_fp_b16_e50_len512_lr1e-05/predictions/val_predictions_epoch_5.json\n",
      "\n",
      "===== Epoch 6/50 =====\n",
      "Training Stage 1 (Yes vs Non-Yes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 1: 100%|█████████████████████████████████████| 123/123 [00:10<00:00, 11.59batch/s, loss=0.453]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 Training - Loss: 0.4000, Acc: 0.8196, F1: 0.8131\n",
      "Training Stage 2 (To some extent vs No)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 2: 100%|█████████████████████████████████████| 123/123 [00:05<00:00, 22.66batch/s, loss=0.403]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 Training - Loss: 0.3404, Acc: 0.8532, F1: 0.8531\n",
      "Overall Training - Acc: 0.8562, F1: 0.8200\n",
      "Evaluating on validation set...\n",
      "Stage 1 Validation - Acc: 0.7126, F1: 0.6726\n",
      "Stage 2 Validation - Acc: 0.6915, F1: 0.6898\n",
      "Overall Validation - Acc: 0.6926, F1: 0.5753\n",
      "Saved prediction results to checkpoints_2to2_adjust/hierarchical_bert-base-uncased_fp_b16_e50_len512_lr1e-05/predictions/train_predictions_epoch_6.json and checkpoints_2to2_adjust/hierarchical_bert-base-uncased_fp_b16_e50_len512_lr1e-05/predictions/val_predictions_epoch_6.json\n",
      "New best model saved with F1: 0.5753, Acc: 0.6926\n",
      "\n",
      "===== Epoch 7/50 =====\n",
      "Training Stage 1 (Yes vs Non-Yes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 1: 100%|█████████████████████████████████████| 123/123 [00:10<00:00, 11.58batch/s, loss=0.339]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 Training - Loss: 0.3424, Acc: 0.8557, F1: 0.8513\n",
      "Training Stage 2 (To some extent vs No)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 2: 100%|█████████████████████████████████████| 123/123 [00:05<00:00, 22.68batch/s, loss=0.156]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 Training - Loss: 0.2959, Acc: 0.8799, F1: 0.8798\n",
      "Overall Training - Acc: 0.9080, F1: 0.8908\n",
      "Evaluating on validation set...\n",
      "Stage 1 Validation - Acc: 0.6886, F1: 0.6743\n",
      "Stage 2 Validation - Acc: 0.6567, F1: 0.6533\n",
      "Overall Validation - Acc: 0.6248, F1: 0.5409\n",
      "Saved prediction results to checkpoints_2to2_adjust/hierarchical_bert-base-uncased_fp_b16_e50_len512_lr1e-05/predictions/train_predictions_epoch_7.json and checkpoints_2to2_adjust/hierarchical_bert-base-uncased_fp_b16_e50_len512_lr1e-05/predictions/val_predictions_epoch_7.json\n",
      "\n",
      "===== Epoch 8/50 =====\n",
      "Training Stage 1 (Yes vs Non-Yes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 1: 100%|█████████████████████████████████████| 123/123 [00:10<00:00, 11.56batch/s, loss=0.607]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 Training - Loss: 0.3034, Acc: 0.8740, F1: 0.8707\n",
      "Training Stage 2 (To some extent vs No)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 2: 100%|█████████████████████████████████████| 123/123 [00:05<00:00, 22.50batch/s, loss=0.129]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 Training - Loss: 0.2246, Acc: 0.9155, F1: 0.9154\n",
      "Overall Training - Acc: 0.9162, F1: 0.8977\n",
      "Evaluating on validation set...\n",
      "Stage 1 Validation - Acc: 0.6846, F1: 0.6778\n",
      "Stage 2 Validation - Acc: 0.6766, F1: 0.6761\n",
      "Overall Validation - Acc: 0.5988, F1: 0.5408\n",
      "Saved prediction results to checkpoints_2to2_adjust/hierarchical_bert-base-uncased_fp_b16_e50_len512_lr1e-05/predictions/train_predictions_epoch_8.json and checkpoints_2to2_adjust/hierarchical_bert-base-uncased_fp_b16_e50_len512_lr1e-05/predictions/val_predictions_epoch_8.json\n",
      "\n",
      "===== Epoch 9/50 =====\n",
      "Training Stage 1 (Yes vs Non-Yes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 1: 100%|█████████████████████████████████████| 123/123 [00:10<00:00, 11.55batch/s, loss=0.113]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 Training - Loss: 0.2418, Acc: 0.9080, F1: 0.9060\n",
      "Training Stage 2 (To some extent vs No)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 2: 100%|█████████████████████████████████████| 123/123 [00:05<00:00, 22.57batch/s, loss=0.144]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 Training - Loss: 0.1868, Acc: 0.9375, F1: 0.9374\n",
      "Overall Training - Acc: 0.9543, F1: 0.9448\n",
      "Evaluating on validation set...\n",
      "Stage 1 Validation - Acc: 0.6926, F1: 0.6736\n",
      "Stage 2 Validation - Acc: 0.6418, F1: 0.6414\n",
      "Overall Validation - Acc: 0.6367, F1: 0.5423\n",
      "Saved prediction results to checkpoints_2to2_adjust/hierarchical_bert-base-uncased_fp_b16_e50_len512_lr1e-05/predictions/train_predictions_epoch_9.json and checkpoints_2to2_adjust/hierarchical_bert-base-uncased_fp_b16_e50_len512_lr1e-05/predictions/val_predictions_epoch_9.json\n",
      "\n",
      "===== Epoch 10/50 =====\n",
      "Training Stage 1 (Yes vs Non-Yes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 1: 100%|█████████████████████████████████████| 123/123 [00:10<00:00, 11.56batch/s, loss=0.204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 Training - Loss: 0.2164, Acc: 0.9111, F1: 0.9094\n",
      "Training Stage 2 (To some extent vs No)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 2: 100%|█████████████████████████████████████| 123/123 [00:05<00:00, 22.45batch/s, loss=0.123]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 Training - Loss: 0.1504, Acc: 0.9387, F1: 0.9387\n",
      "Overall Training - Acc: 0.9416, F1: 0.9321\n",
      "Evaluating on validation set...\n",
      "Stage 1 Validation - Acc: 0.7026, F1: 0.6688\n",
      "Stage 2 Validation - Acc: 0.6468, F1: 0.6439\n",
      "Overall Validation - Acc: 0.6607, F1: 0.5345\n",
      "Saved prediction results to checkpoints_2to2_adjust/hierarchical_bert-base-uncased_fp_b16_e50_len512_lr1e-05/predictions/train_predictions_epoch_10.json and checkpoints_2to2_adjust/hierarchical_bert-base-uncased_fp_b16_e50_len512_lr1e-05/predictions/val_predictions_epoch_10.json\n",
      "\n",
      "===== Epoch 11/50 =====\n",
      "Training Stage 1 (Yes vs Non-Yes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 1: 100%|█████████████████████████████████████| 123/123 [00:10<00:00, 11.56batch/s, loss=0.370]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 Training - Loss: 0.1721, Acc: 0.9329, F1: 0.9315\n",
      "Training Stage 2 (To some extent vs No)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 2: 100%|█████████████████████████████████████| 123/123 [00:05<00:00, 22.59batch/s, loss=0.058]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 Training - Loss: 0.1279, Acc: 0.9561, F1: 0.9560\n",
      "Overall Training - Acc: 0.9629, F1: 0.9553\n",
      "Evaluating on validation set...\n",
      "Stage 1 Validation - Acc: 0.6906, F1: 0.6652\n",
      "Stage 2 Validation - Acc: 0.6567, F1: 0.6545\n",
      "Overall Validation - Acc: 0.6427, F1: 0.5218\n",
      "Saved prediction results to checkpoints_2to2_adjust/hierarchical_bert-base-uncased_fp_b16_e50_len512_lr1e-05/predictions/train_predictions_epoch_11.json and checkpoints_2to2_adjust/hierarchical_bert-base-uncased_fp_b16_e50_len512_lr1e-05/predictions/val_predictions_epoch_11.json\n",
      "\n",
      "===== Epoch 12/50 =====\n",
      "Training Stage 1 (Yes vs Non-Yes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 1: 100%|█████████████████████████████████████| 123/123 [00:10<00:00, 11.53batch/s, loss=0.079]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 Training - Loss: 0.1532, Acc: 0.9365, F1: 0.9353\n",
      "Training Stage 2 (To some extent vs No)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 2: 100%|█████████████████████████████████████| 123/123 [00:05<00:00, 22.48batch/s, loss=0.326]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 Training - Loss: 0.1001, Acc: 0.9619, F1: 0.9618\n",
      "Overall Training - Acc: 0.9807, F1: 0.9758\n",
      "Evaluating on validation set...\n",
      "Stage 1 Validation - Acc: 0.7126, F1: 0.6935\n",
      "Stage 2 Validation - Acc: 0.6418, F1: 0.6418\n",
      "Overall Validation - Acc: 0.6427, F1: 0.5447\n",
      "Saved prediction results to checkpoints_2to2_adjust/hierarchical_bert-base-uncased_fp_b16_e50_len512_lr1e-05/predictions/train_predictions_epoch_12.json and checkpoints_2to2_adjust/hierarchical_bert-base-uncased_fp_b16_e50_len512_lr1e-05/predictions/val_predictions_epoch_12.json\n",
      "\n",
      "===== Epoch 13/50 =====\n",
      "Training Stage 1 (Yes vs Non-Yes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 1: 100%|█████████████████████████████████████| 123/123 [00:10<00:00, 11.52batch/s, loss=0.203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 Training - Loss: 0.1251, Acc: 0.9543, F1: 0.9535\n",
      "Training Stage 2 (To some extent vs No)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 2: 100%|█████████████████████████████████████| 123/123 [00:05<00:00, 22.53batch/s, loss=0.077]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 Training - Loss: 0.0955, Acc: 0.9630, F1: 0.9629\n",
      "Overall Training - Acc: 0.9863, F1: 0.9827\n",
      "Evaluating on validation set...\n",
      "Stage 1 Validation - Acc: 0.7046, F1: 0.6822\n",
      "Stage 2 Validation - Acc: 0.6517, F1: 0.6453\n",
      "Overall Validation - Acc: 0.6487, F1: 0.5363\n",
      "Saved prediction results to checkpoints_2to2_adjust/hierarchical_bert-base-uncased_fp_b16_e50_len512_lr1e-05/predictions/train_predictions_epoch_13.json and checkpoints_2to2_adjust/hierarchical_bert-base-uncased_fp_b16_e50_len512_lr1e-05/predictions/val_predictions_epoch_13.json\n",
      "\n",
      "===== Epoch 14/50 =====\n",
      "Training Stage 1 (Yes vs Non-Yes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 1: 100%|█████████████████████████████████████| 123/123 [00:10<00:00, 11.55batch/s, loss=0.112]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 Training - Loss: 0.0926, Acc: 0.9665, F1: 0.9659\n",
      "Training Stage 2 (To some extent vs No)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 2: 100%|█████████████████████████████████████| 123/123 [00:05<00:00, 22.42batch/s, loss=0.050]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 Training - Loss: 0.0945, Acc: 0.9746, F1: 0.9746\n",
      "Overall Training - Acc: 0.9919, F1: 0.9901\n",
      "Evaluating on validation set...\n",
      "Stage 1 Validation - Acc: 0.6707, F1: 0.6616\n",
      "Stage 2 Validation - Acc: 0.6418, F1: 0.6342\n",
      "Overall Validation - Acc: 0.5948, F1: 0.5117\n",
      "Saved prediction results to checkpoints_2to2_adjust/hierarchical_bert-base-uncased_fp_b16_e50_len512_lr1e-05/predictions/train_predictions_epoch_14.json and checkpoints_2to2_adjust/hierarchical_bert-base-uncased_fp_b16_e50_len512_lr1e-05/predictions/val_predictions_epoch_14.json\n",
      "Early stopping triggered after 14 epochs.\n",
      "\n",
      "Training complete!\n",
      "Best validation accuracy: 0.6926\n",
      "Best validation F1 score: 0.5753\n"
     ]
    }
   ],
   "source": [
    "def train_hierarchical(configs):\n",
    "    # 设置随机种子\n",
    "    random.seed(configs.seed)\n",
    "    np.random.seed(configs.seed)\n",
    "    torch.manual_seed(configs.seed)\n",
    "    \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # 创建检查点目录\n",
    "    checkpoint_dir = os.path.join(configs.checkpoint_dir, configs.exp_name)\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # 为保存混淆矩阵创建目录 - 分别为训练集和验证集创建\n",
    "    train_plot_dir = os.path.join(checkpoint_dir, 'plots', 'train')\n",
    "    val_plot_dir = os.path.join(checkpoint_dir, 'plots', 'val')\n",
    "    os.makedirs(train_plot_dir, exist_ok=True)\n",
    "    os.makedirs(val_plot_dir, exist_ok=True)\n",
    "    \n",
    "    # 创建保存预测结果的目录\n",
    "    predictions_dir = os.path.join(checkpoint_dir, 'predictions')\n",
    "    os.makedirs(predictions_dir, exist_ok=True)\n",
    "    \n",
    "    # 加载数据集\n",
    "    train_dataset = BAE2025Dataset(configs.data_path)\n",
    "    val_dataset = BAE2025Dataset(configs.val_data_path)    \n",
    "\n",
    "    # 创建数据加载器\n",
    "    train_dataloader = BAE2025DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=configs.batch_size,\n",
    "        max_length=configs.max_length,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        device=configs.device,\n",
    "        tokenizer_name=configs.model_name\n",
    "    )\n",
    "\n",
    "    val_dataloader = BAE2025DataLoader(\n",
    "        dataset=val_dataset,\n",
    "        batch_size=configs.batch_size,\n",
    "        max_length=configs.max_length,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        device=configs.device,\n",
    "        tokenizer_name=configs.model_name\n",
    "    )\n",
    "    \n",
    "    # 创建分层分类模型\n",
    "    model = HierarchicalBertClassifier(\n",
    "        pretrained_model_name=configs.model_name,\n",
    "        freeze_pooler=configs.freeze_pooler,\n",
    "        dropout=configs.dropout\n",
    "    ).to(configs.device)\n",
    "\n",
    "    # 定义两个阶段的损失函数\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # 定义优化器\n",
    "    optimizer = AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=configs.lr\n",
    "    )\n",
    "\n",
    "    # 初始化最佳验证指标\n",
    "    best_val_acc = 0.0\n",
    "    best_val_f1 = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # 定义原始类别名称和阶段类别名称\n",
    "    class_names = ['Yes', 'To some extent', 'No']\n",
    "    stage1_names = ['Yes', 'Non-Yes']\n",
    "    stage2_names = ['To some extent', 'No']\n",
    "    \n",
    "    # 添加计算所需的库\n",
    "    from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import json\n",
    "    import torch.nn.functional as F  # 用于获取概率值\n",
    "    \n",
    "    # 训练循环\n",
    "    for epoch in range(configs.epochs):\n",
    "        print(f\"\\n===== Epoch {epoch + 1}/{configs.epochs} =====\")\n",
    "        \n",
    "        # 用于保存训练和验证结果的字典\n",
    "        train_results = {\n",
    "            \"stage1\": {\"probs\": [], \"preds\": [], \"labels\": []},\n",
    "            \"stage2\": {\"probs\": [], \"preds\": [], \"labels\": []},\n",
    "            \"full\": {\"probs\": [], \"preds\": [], \"labels\": []}\n",
    "        }\n",
    "        \n",
    "        val_results = {\n",
    "            \"stage1\": {\"probs\": [], \"preds\": [], \"labels\": []},\n",
    "            \"stage2\": {\"probs\": [], \"preds\": [], \"labels\": []},\n",
    "            \"full\": {\"probs\": [], \"preds\": [], \"labels\": []}\n",
    "        }\n",
    "        \n",
    "        # ======== 训练第一阶段模型：Yes vs 非Yes ========\n",
    "        model.train()\n",
    "        stage1_train_loss = 0.0\n",
    "        stage1_train_preds = []\n",
    "        stage1_train_labels = []\n",
    "        \n",
    "        print(\"Training Stage 1 (Yes vs Non-Yes)...\")\n",
    "        with tqdm(train_dataloader, total=len(train_dataloader), desc=\"Stage 1\", unit=\"batch\", ncols=100) as pbar:\n",
    "            for input_ids, attention_mask, token_type_ids, labels in pbar:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # 将原始标签转换为二分类标签：0(Yes) 或 1(非Yes)\n",
    "                stage1_labels = (labels > 0).long()  # Yes=0, 其他=1\n",
    "                \n",
    "                # 前向传播第一阶段\n",
    "                stage1_logits = model(input_ids, attention_mask, token_type_ids, stage=1)\n",
    "                \n",
    "                # 计算损失\n",
    "                loss = criterion(stage1_logits, stage1_labels)\n",
    "                \n",
    "                # 反向传播\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # 获取预测概率\n",
    "                stage1_probs = F.softmax(stage1_logits, dim=1).detach().cpu().numpy().tolist()\n",
    "                \n",
    "                # 收集预测和标签\n",
    "                preds = torch.argmax(stage1_logits, dim=1)\n",
    "                stage1_train_preds.extend(preds.cpu().numpy())\n",
    "                stage1_train_labels.extend(stage1_labels.cpu().numpy())\n",
    "                \n",
    "                # 保存预测概率和标签\n",
    "                train_results[\"stage1\"][\"probs\"].extend(stage1_probs)\n",
    "                train_results[\"stage1\"][\"preds\"].extend(preds.cpu().numpy().tolist())\n",
    "                train_results[\"stage1\"][\"labels\"].extend(stage1_labels.cpu().numpy().tolist())\n",
    "                \n",
    "                stage1_train_loss += loss.item()\n",
    "                \n",
    "                # 更新进度条\n",
    "                pbar.set_postfix(loss=f'{loss.item():.3f}')\n",
    "        \n",
    "        # 计算第一阶段训练指标\n",
    "        stage1_train_loss /= len(train_dataloader)\n",
    "        stage1_train_acc = accuracy_score(stage1_train_labels, stage1_train_preds)\n",
    "        stage1_train_f1 = f1_score(stage1_train_labels, stage1_train_preds, average='macro')\n",
    "        \n",
    "        print(f\"Stage 1 Training - Loss: {stage1_train_loss:.4f}, Acc: {stage1_train_acc:.4f}, F1: {stage1_train_f1:.4f}\")\n",
    "        \n",
    "        # 创建并保存第一阶段训练混淆矩阵\n",
    "        cm = confusion_matrix(stage1_train_labels, stage1_train_preds)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=stage1_names, yticklabels=stage1_names)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title(f'Train: Stage 1 (Yes vs Non-Yes)\\nAcc: {stage1_train_acc:.4f}, F1: {stage1_train_f1:.4f}')\n",
    "        matrix_path = os.path.join(train_plot_dir, f'stage1_cm_epoch_{epoch+1}.png')\n",
    "        plt.savefig(matrix_path)\n",
    "        plt.close()\n",
    "        \n",
    "        # ======== 训练第二阶段模型：To some extent vs No ========\n",
    "        # 筛选出标签为To some extent或No的样本索引\n",
    "        stage2_train_loss = 0.0\n",
    "        stage2_train_preds = []\n",
    "        stage2_train_labels = []\n",
    "        stage2_sample_count = 0\n",
    "        \n",
    "        print(\"Training Stage 2 (To some extent vs No)...\")\n",
    "        with tqdm(train_dataloader, total=len(train_dataloader), desc=\"Stage 2\", unit=\"batch\", ncols=100) as pbar:\n",
    "            for input_ids, attention_mask, token_type_ids, labels in pbar:\n",
    "                # 筛选非Yes样本的索引\n",
    "                non_yes_indices = (labels > 0).nonzero(as_tuple=True)[0]\n",
    "                \n",
    "                if len(non_yes_indices) == 0:\n",
    "                    continue  # 如果批次中没有非Yes样本，跳过\n",
    "                \n",
    "                # 提取非Yes样本的数据\n",
    "                non_yes_input_ids = input_ids[non_yes_indices]\n",
    "                non_yes_attention_mask = attention_mask[non_yes_indices]\n",
    "                non_yes_token_type_ids = token_type_ids[non_yes_indices]\n",
    "                non_yes_labels = labels[non_yes_indices]\n",
    "                \n",
    "                # 将原始标签转换为二分类标签：0(To some extent) 或 1(No)\n",
    "                # 原始：0=Yes, 1=To some extent, 2=No\n",
    "                # 现在：0=To some extent, 1=No\n",
    "                stage2_labels = (non_yes_labels == 2).long()  # To some extent=0, No=1\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # 前向传播第二阶段\n",
    "                stage2_logits = model(non_yes_input_ids, non_yes_attention_mask, non_yes_token_type_ids, stage=2)\n",
    "                \n",
    "                # 计算损失\n",
    "                loss = criterion(stage2_logits, stage2_labels)\n",
    "                \n",
    "                # 反向传播\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # 获取预测概率\n",
    "                stage2_probs = F.softmax(stage2_logits, dim=1).detach().cpu().numpy().tolist()\n",
    "                \n",
    "                # 收集预测和标签\n",
    "                preds = torch.argmax(stage2_logits, dim=1)\n",
    "                stage2_train_preds.extend(preds.cpu().numpy())\n",
    "                stage2_train_labels.extend(stage2_labels.cpu().numpy())\n",
    "                \n",
    "                # 保存预测概率和标签\n",
    "                train_results[\"stage2\"][\"probs\"].extend(stage2_probs)\n",
    "                train_results[\"stage2\"][\"preds\"].extend(preds.cpu().numpy().tolist())\n",
    "                train_results[\"stage2\"][\"labels\"].extend(stage2_labels.cpu().numpy().tolist())\n",
    "                \n",
    "                stage2_train_loss += loss.item()\n",
    "                stage2_sample_count += 1\n",
    "                \n",
    "                # 更新进度条\n",
    "                pbar.set_postfix(loss=f'{loss.item():.3f}')\n",
    "        \n",
    "        # 计算第二阶段训练指标\n",
    "        if stage2_sample_count > 0:\n",
    "            stage2_train_loss /= stage2_sample_count\n",
    "            stage2_train_acc = accuracy_score(stage2_train_labels, stage2_train_preds)\n",
    "            stage2_train_f1 = f1_score(stage2_train_labels, stage2_train_preds, average='macro')\n",
    "            \n",
    "            print(f\"Stage 2 Training - Loss: {stage2_train_loss:.4f}, Acc: {stage2_train_acc:.4f}, F1: {stage2_train_f1:.4f}\")\n",
    "            \n",
    "            # 创建并保存第二阶段训练混淆矩阵\n",
    "            cm = confusion_matrix(stage2_train_labels, stage2_train_preds)\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=stage2_names, yticklabels=stage2_names)\n",
    "            plt.xlabel('Predicted')\n",
    "            plt.ylabel('True')\n",
    "            plt.title(f'Train: Stage 2 (To some extent vs No)\\nAcc: {stage2_train_acc:.4f}, F1: {stage2_train_f1:.4f}')\n",
    "            matrix_path = os.path.join(train_plot_dir, f'stage2_cm_epoch_{epoch+1}.png')\n",
    "            plt.savefig(matrix_path)\n",
    "            plt.close()\n",
    "        \n",
    "        # ======== 训练集整体评估 ========\n",
    "        model.eval()\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for input_ids, attention_mask, token_type_ids, labels in train_dataloader:\n",
    "                # 完整两阶段预测\n",
    "                logits = model(input_ids, attention_mask, token_type_ids)\n",
    "                \n",
    "                # 获取预测概率\n",
    "                probs = F.softmax(logits, dim=1).cpu().numpy().tolist()\n",
    "                \n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                \n",
    "                train_preds.extend(preds.cpu().numpy())\n",
    "                train_labels.extend(labels.cpu().numpy())\n",
    "                \n",
    "                # 保存预测概率和标签\n",
    "                train_results[\"full\"][\"probs\"].extend(probs)\n",
    "                train_results[\"full\"][\"preds\"].extend(preds.cpu().numpy().tolist())\n",
    "                train_results[\"full\"][\"labels\"].extend(labels.cpu().numpy().tolist())\n",
    "        \n",
    "        # 计算整体训练集指标\n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        train_f1 = f1_score(train_labels, train_preds, average='macro')\n",
    "        \n",
    "        print(f\"Overall Training - Acc: {train_acc:.4f}, F1: {train_f1:.4f}\")\n",
    "        \n",
    "        # 创建完整的训练集混淆矩阵\n",
    "        cm_full = confusion_matrix(train_labels, train_preds, labels=[0, 1, 2])\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm_full, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title(f'Train: Full Hierarchical Confusion Matrix\\nAcc: {train_acc:.4f}, F1: {train_f1:.4f}')\n",
    "        matrix_path = os.path.join(train_plot_dir, f'full_cm_epoch_{epoch+1}.png')\n",
    "        plt.savefig(matrix_path)\n",
    "        plt.close()\n",
    "        \n",
    "        # ======== 验证集评估 ========\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        stage1_val_preds = []\n",
    "        stage1_val_labels = []\n",
    "        stage2_val_preds = []\n",
    "        stage2_val_labels = []\n",
    "        \n",
    "        print(\"Evaluating on validation set...\")\n",
    "        with torch.no_grad():\n",
    "            for input_ids, attention_mask, token_type_ids, labels in val_dataloader:\n",
    "                # 第一阶段评估\n",
    "                stage1_logits = model(input_ids, attention_mask, token_type_ids, stage=1)\n",
    "                stage1_probs = F.softmax(stage1_logits, dim=1).cpu().numpy().tolist()\n",
    "                stage1_preds = torch.argmax(stage1_logits, dim=1)\n",
    "                stage1_labels_binary = (labels > 0).long()\n",
    "                \n",
    "                stage1_val_preds.extend(stage1_preds.cpu().numpy())\n",
    "                stage1_val_labels.extend(stage1_labels_binary.cpu().numpy())\n",
    "                \n",
    "                # 保存第一阶段预测概率和标签\n",
    "                val_results[\"stage1\"][\"probs\"].extend(stage1_probs)\n",
    "                val_results[\"stage1\"][\"preds\"].extend(stage1_preds.cpu().numpy().tolist())\n",
    "                val_results[\"stage1\"][\"labels\"].extend(stage1_labels_binary.cpu().numpy().tolist())\n",
    "                \n",
    "                # 找出非Yes样本\n",
    "                non_yes_indices = (labels > 0).nonzero(as_tuple=True)[0]\n",
    "                \n",
    "                if len(non_yes_indices) > 0:\n",
    "                    # 第二阶段评估\n",
    "                    non_yes_input_ids = input_ids[non_yes_indices]\n",
    "                    non_yes_attention_mask = attention_mask[non_yes_indices]\n",
    "                    non_yes_token_type_ids = token_type_ids[non_yes_indices]\n",
    "                    non_yes_labels = labels[non_yes_indices]\n",
    "                    \n",
    "                    stage2_logits = model(non_yes_input_ids, non_yes_attention_mask, non_yes_token_type_ids, stage=2)\n",
    "                    stage2_probs = F.softmax(stage2_logits, dim=1).cpu().numpy().tolist()\n",
    "                    stage2_preds = torch.argmax(stage2_logits, dim=1)\n",
    "                    \n",
    "                    # 转换为二分类标签：0=To some extent, 1=No\n",
    "                    stage2_labels_binary = (non_yes_labels == 2).long()\n",
    "                    \n",
    "                    stage2_val_preds.extend(stage2_preds.cpu().numpy())\n",
    "                    stage2_val_labels.extend(stage2_labels_binary.cpu().numpy())\n",
    "                    \n",
    "                    # 保存第二阶段预测概率和标签\n",
    "                    val_results[\"stage2\"][\"probs\"].extend(stage2_probs)\n",
    "                    val_results[\"stage2\"][\"preds\"].extend(stage2_preds.cpu().numpy().tolist())\n",
    "                    val_results[\"stage2\"][\"labels\"].extend(stage2_labels_binary.cpu().numpy().tolist())\n",
    "                \n",
    "                # 完整两阶段预测\n",
    "                logits = model(input_ids, attention_mask, token_type_ids)\n",
    "                probs = F.softmax(logits, dim=1).cpu().numpy().tolist()\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                \n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "                \n",
    "                # 保存整体预测概率和标签\n",
    "                val_results[\"full\"][\"probs\"].extend(probs)\n",
    "                val_results[\"full\"][\"preds\"].extend(preds.cpu().numpy().tolist())\n",
    "                val_results[\"full\"][\"labels\"].extend(labels.cpu().numpy().tolist())\n",
    "        \n",
    "        # 计算验证集指标\n",
    "        # 阶段1\n",
    "        stage1_val_acc = accuracy_score(stage1_val_labels, stage1_val_preds)\n",
    "        stage1_val_f1 = f1_score(stage1_val_labels, stage1_val_preds, average='macro')\n",
    "        \n",
    "        print(f\"Stage 1 Validation - Acc: {stage1_val_acc:.4f}, F1: {stage1_val_f1:.4f}\")\n",
    "        \n",
    "        # 创建阶段1验证混淆矩阵\n",
    "        cm = confusion_matrix(stage1_val_labels, stage1_val_preds)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=stage1_names, yticklabels=stage1_names)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title(f'Val: Stage 1 (Yes vs Non-Yes)\\nAcc: {stage1_val_acc:.4f}, F1: {stage1_val_f1:.4f}')\n",
    "        matrix_path = os.path.join(val_plot_dir, f'stage1_cm_epoch_{epoch+1}.png')\n",
    "        plt.savefig(matrix_path)\n",
    "        plt.close()\n",
    "        \n",
    "        # 阶段2\n",
    "        if len(stage2_val_labels) > 0:\n",
    "            stage2_val_acc = accuracy_score(stage2_val_labels, stage2_val_preds)\n",
    "            stage2_val_f1 = f1_score(stage2_val_labels, stage2_val_preds, average='macro')\n",
    "            \n",
    "            print(f\"Stage 2 Validation - Acc: {stage2_val_acc:.4f}, F1: {stage2_val_f1:.4f}\")\n",
    "            \n",
    "            # 创建阶段2验证混淆矩阵\n",
    "            cm = confusion_matrix(stage2_val_labels, stage2_val_preds)\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=stage2_names, yticklabels=stage2_names)\n",
    "            plt.xlabel('Predicted')\n",
    "            plt.ylabel('True')\n",
    "            plt.title(f'Val: Stage 2 (To some extent vs No)\\nAcc: {stage2_val_acc:.4f}, F1: {stage2_val_f1:.4f}')\n",
    "            matrix_path = os.path.join(val_plot_dir, f'stage2_cm_epoch_{epoch+1}.png')\n",
    "            plt.savefig(matrix_path)\n",
    "            plt.close()\n",
    "        \n",
    "        # 整体验证集评估\n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        val_f1 = f1_score(val_labels, val_preds, average='macro')\n",
    "        \n",
    "        print(f\"Overall Validation - Acc: {val_acc:.4f}, F1: {val_f1:.4f}\")\n",
    "        \n",
    "        # 创建完整验证集混淆矩阵\n",
    "        cm_full = confusion_matrix(val_labels, val_preds, labels=[0, 1, 2])\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm_full, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title(f'Val: Full Hierarchical Confusion Matrix\\nAcc: {val_acc:.4f}, F1: {val_f1:.4f}')\n",
    "        matrix_path = os.path.join(val_plot_dir, f'full_cm_epoch_{epoch+1}.png')\n",
    "        plt.savefig(matrix_path)\n",
    "        plt.close()\n",
    "        \n",
    "        # 绘制两两类别的验证集混淆矩阵\n",
    "        class_pairs = [\n",
    "            ([0, 1], ['Yes', 'To some extent']),  # Yes vs To some extent\n",
    "            ([0, 2], ['Yes', 'No']),              # Yes vs No\n",
    "            ([1, 2], ['To some extent', 'No'])    # To some extent vs No\n",
    "        ]\n",
    "        \n",
    "        for classes_idx, classes_names in class_pairs:\n",
    "            # 筛选出对应两个类别的预测和标签\n",
    "            mask = np.isin(np.array(val_labels), classes_idx)\n",
    "            filtered_preds = np.array(val_preds)[mask]\n",
    "            filtered_labels = np.array(val_labels)[mask]\n",
    "            \n",
    "            # 计算此对类别的准确率和F1分数\n",
    "            if len(filtered_labels) > 0:\n",
    "                pair_acc = accuracy_score(filtered_labels, filtered_preds)\n",
    "                # 计算二分类F1分数\n",
    "                pair_f1 = f1_score(filtered_labels, filtered_preds, average='macro')\n",
    "                \n",
    "                # 创建混淆矩阵\n",
    "                cm = confusion_matrix(filtered_labels, filtered_preds, labels=classes_idx)\n",
    "                \n",
    "                # 绘制混淆矩阵\n",
    "                plt.figure(figsize=(8, 6))\n",
    "                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                            xticklabels=[classes_names[i == classes_idx[1]] for i in classes_idx],\n",
    "                            yticklabels=[classes_names[i == classes_idx[1]] for i in classes_idx])\n",
    "                plt.xlabel('Predicted')\n",
    "                plt.ylabel('True')\n",
    "                plt.title(f'Val: {classes_names[0]} vs {classes_names[1]}\\nAcc: {pair_acc:.4f}, F1: {pair_f1:.4f}')\n",
    "                \n",
    "                # 保存图表\n",
    "                matrix_path = os.path.join(val_plot_dir, f'cm_{classes_names[0].replace(\" \", \"_\")}_{classes_names[1].replace(\" \", \"_\")}_epoch_{epoch+1}.png')\n",
    "                plt.savefig(matrix_path)\n",
    "                plt.close()\n",
    "        \n",
    "        # 保存训练和验证结果到JSON文件\n",
    "        train_json_path = os.path.join(predictions_dir, f'train_predictions_epoch_{epoch+1}.json')\n",
    "        val_json_path = os.path.join(predictions_dir, f'val_predictions_epoch_{epoch+1}.json')\n",
    "        \n",
    "        # 添加一些元数据到结果字典\n",
    "        train_metadata = {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"stage1_acc\": stage1_train_acc,\n",
    "            \"stage1_f1\": stage1_train_f1,\n",
    "            \"full_acc\": train_acc,\n",
    "            \"full_f1\": train_f1\n",
    "        }\n",
    "        \n",
    "        val_metadata = {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"stage1_acc\": stage1_val_acc,\n",
    "            \"stage1_f1\": stage1_val_f1,\n",
    "            \"full_acc\": val_acc,\n",
    "            \"full_f1\": val_f1\n",
    "        }\n",
    "        \n",
    "        if stage2_sample_count > 0:\n",
    "            train_metadata[\"stage2_acc\"] = stage2_train_acc\n",
    "            train_metadata[\"stage2_f1\"] = stage2_train_f1\n",
    "        \n",
    "        if len(stage2_val_labels) > 0:\n",
    "            val_metadata[\"stage2_acc\"] = stage2_val_acc\n",
    "            val_metadata[\"stage2_f1\"] = stage2_val_f1\n",
    "        \n",
    "        # 将元数据添加到结果字典\n",
    "        train_results[\"metadata\"] = train_metadata\n",
    "        val_results[\"metadata\"] = val_metadata\n",
    "        \n",
    "        # 保存JSON文件\n",
    "        with open(train_json_path, 'w') as f:\n",
    "            json.dump(train_results, f, indent=2)\n",
    "        \n",
    "        with open(val_json_path, 'w') as f:\n",
    "            json.dump(val_results, f, indent=2)\n",
    "        \n",
    "        print(f\"Saved prediction results to {train_json_path} and {val_json_path}\")\n",
    "        \n",
    "        # 检查是否保存模型并判断是否需要早停\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            best_val_acc = val_acc\n",
    "            \n",
    "            # 保存模型\n",
    "            # torch.save(model.state_dict(), os.path.join(checkpoint_dir, 'best_hierarchical_model.pt'))\n",
    "            print(f'New best model saved with F1: {best_val_f1:.4f}, Acc: {best_val_acc:.4f}')\n",
    "            \n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= configs.patience:\n",
    "                print(f'Early stopping triggered after {epoch+1} epochs.')\n",
    "                break\n",
    "        \n",
    "        # 返回训练状态\n",
    "        model.train()\n",
    "    \n",
    "    print(\"\\nTraining complete!\")\n",
    "    print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
    "    print(f\"Best validation F1 score: {best_val_f1:.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 修改主函数\n",
    "if __name__ == '__main__':\n",
    "    # 判断是否在Jupyter环境中运行\n",
    "    try:\n",
    "        # 检查是否在Jupyter中运行\n",
    "        get_ipython = globals().get('get_ipython', None)\n",
    "        if get_ipython and 'IPKernelApp' in get_ipython().config:\n",
    "            # 在Jupyter环境中运行，使用默认配置\n",
    "            print(\"Running in Jupyter environment, using default configs\")\n",
    "            configs = get_default_configs()\n",
    "        else:\n",
    "            # 在命令行环境中运行，使用argparse\n",
    "            configs = argparser()\n",
    "    except:\n",
    "        # 任何异常都使用argparse处理\n",
    "        configs = argparser()\n",
    "    \n",
    "    # 设置实验名称\n",
    "    if configs.name is None:\n",
    "        configs.exp_name = \\\n",
    "            f'hierarchical_{os.path.basename(configs.model_name)}' + \\\n",
    "            f'{\"_fp\" if configs.freeze_pooler else \"\"}' + \\\n",
    "            f'_b{configs.batch_size}_e{configs.epochs}' + \\\n",
    "            f'_len{configs.max_length}_lr{configs.lr}'\n",
    "    else:\n",
    "        configs.exp_name = configs.name\n",
    "    \n",
    "    # 设置设备\n",
    "    if configs.device is None:\n",
    "        configs.device = torch.device(\n",
    "            'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        )\n",
    "    \n",
    "    # 调用分层训练函数\n",
    "    trained_model = train_hierarchical(configs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
