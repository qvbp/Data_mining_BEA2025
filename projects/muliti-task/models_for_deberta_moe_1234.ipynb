{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# 设置环境变量，只让程序看到 GPU 2\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import BertModel, AutoModel\n",
    "from transformers import AdamW\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "\n",
    "class BAE2025Dataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            data_path,\n",
    "            label_types=[\"Mistake_Identification\", \"Mistake_Location\", \"Providing_Guidance\", \"Actionability\"],\n",
    "            labels={\n",
    "                \"Yes\": 0,\n",
    "                \"To some extent\": 1, \n",
    "                \"No\": 2,\n",
    "            }\n",
    "    ):\n",
    "        self.data_path = data_path\n",
    "        self.label_types = label_types\n",
    "        self.labels = labels\n",
    "        self._get_data()\n",
    "    \n",
    "    def _get_data(self):\n",
    "        with open(self.data_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        self.data = []\n",
    "        for item in data:\n",
    "            sent1 = item['conversation_history']\n",
    "            sent2 = item['response']\n",
    "            \n",
    "            label_values = []\n",
    "            # 逐个标签提取并转换\n",
    "            for label_type in self.label_types:\n",
    "                label = item.get(label_type)\n",
    "                if label not in self.labels:\n",
    "                    break  # 如果有任何一个标签缺失或无效，就跳过这个样本\n",
    "                label_values.append(self.labels[label])\n",
    "            else:\n",
    "                # 只有在所有标签都成功提取时才添加到数据集中\n",
    "                self.data.append(((sent1, sent2), label_values))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def get_labels(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据加载函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoConfig\n",
    "from transformers import DebertaV2Tokenizer\n",
    "\n",
    "class BAE2025DataLoader:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        batch_size=16,\n",
    "        max_length=512,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        device=None,\n",
    "        # tokenizer_name='chinese-bert-wwm-ext'\n",
    "        # tokenizer_name='chinese-roberta-wwm-ext'\n",
    "        # tokenizer_name='chinese-roberta-wwm-ext-large'\n",
    "        # tokenizer_name='/mnt/cfs/huangzhiwei/pykt-moekt/SBM/bge-large-en-v1.5'\n",
    "        tokenizer_name='/mnt/cfs/huangzhiwei/BAE2025/models/deberta-v3-base'\n",
    "        # tokenizer_name='/mnt/cfs/huangzhiwei/BAE2025/models/roberta-base'\n",
    "    ):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        self.tokenizer.truncation_side = 'left'  # 设置截断方向为左侧,即从句子开头开始截断,假设一个句子过长，则从句子开头开始截断，保留句子结尾的部分\n",
    "        print(\"当前使用的 tokenizer 类型：\", type(self.tokenizer))\n",
    "        \n",
    "        # config = AutoConfig.from_pretrained(tokenizer_name)\n",
    "        # self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, config=config, use_fast=True)\n",
    "        \n",
    "        \n",
    "        # self.tokenizer = DebertaV2Tokenizer.from_pretrained(tokenizer_name)\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "        self.shuffle = shuffle\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "        if device is None:\n",
    "            self.device = torch.device(\n",
    "                'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            )\n",
    "        else:\n",
    "            self.device = device\n",
    "\n",
    "        self.loader = DataLoader(\n",
    "            dataset=self.dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=self.collate_fn,\n",
    "            shuffle=self.shuffle,\n",
    "            drop_last=self.drop_last\n",
    "        )\n",
    "\n",
    "    def collate_fn(self, data):\n",
    "        sents = [i[0] for i in data]\n",
    "        labels = [i[1] for i in data]\n",
    "\n",
    "        # 修改这里，处理两个句子的情况\n",
    "        data = self.tokenizer.batch_encode_plus(\n",
    "            batch_text_or_text_pairs=[(sent[0], sent[1]) for sent in sents],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt',\n",
    "            return_length=True\n",
    "        )\n",
    "        input_ids = data['input_ids'].to(self.device)\n",
    "        attention_mask = data['attention_mask'].to(self.device)\n",
    "        # token_type_ids = data['token_type_ids'].to(self.device)\n",
    "        # labels = torch.LongTensor(labels).to(self.device)\n",
    "        \n",
    "        # 将 label 列表变成 tensor，自动处理为二维\n",
    "        labels = torch.tensor(labels, dtype=torch.long).to(self.device)\n",
    "\n",
    "        # return input_ids, attention_mask, token_type_ids, labels\n",
    "        return input_ids, attention_mask, labels\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        for data in self.loader:\n",
    "            yield data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel\n",
    "\n",
    "\n",
    "class ExpertLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, input_size]\n",
    "        output, (hidden, _) = self.lstm(x)\n",
    "        # 返回最后一个时间步的隐藏状态\n",
    "        return hidden[-1]  # [batch_size, hidden_size]\n",
    "\n",
    "\n",
    "class ExpertBiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size // 2,  # 因为是双向的，所以每个方向的隐藏层大小减半\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, input_size]\n",
    "        output, (hidden, _) = self.bilstm(x)\n",
    "        # 拼接最后一层的正向和反向隐藏状态\n",
    "        # hidden shape: [num_layers * num_directions, batch_size, hidden_size//2]\n",
    "        hidden_forward = hidden[-2]  # 正向的最后一层 [batch_size, hidden_size//2]\n",
    "        hidden_backward = hidden[-1]  # 反向的最后一层 [batch_size, hidden_size//2]\n",
    "        hidden_concat = torch.cat([hidden_forward, hidden_backward], dim=1)  # [batch_size, hidden_size]\n",
    "        return hidden_concat\n",
    "\n",
    "\n",
    "class ExpertRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, input_size]\n",
    "        _, hidden = self.rnn(x)\n",
    "        # 返回最后一个时间步的隐藏状态\n",
    "        return hidden[-1]  # [batch_size, hidden_size]\n",
    "\n",
    "\n",
    "class ExpertGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, input_size]\n",
    "        _, hidden = self.gru(x)\n",
    "        # 返回最后一个时间步的隐藏状态\n",
    "        return hidden[-1]  # [batch_size, hidden_size]\n",
    "\n",
    "\n",
    "class ExpertLinear(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size * 2),\n",
    "            nn.LayerNorm(hidden_size * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size * 2, hidden_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, input_size]\n",
    "        # 我们需要把序列信息压缩为一个向量，可以使用平均池化\n",
    "        pooled = torch.mean(x, dim=1)  # [batch_size, input_size]\n",
    "        return self.linear(pooled)  # [batch_size, hidden_size]\n",
    "\n",
    "\n",
    "class BertClassificationHead(nn.Module):\n",
    "    def __init__(self, hidden_size=1024, num_classes=3, dropout_prob=0.3):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.out_proj = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, features):\n",
    "        # 提取 [CLS] 标记的表示\n",
    "        x = features[:, 0, :]  # 使用第一个标记([CLS])\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MoERouter(nn.Module):\n",
    "    \"\"\"专家路由器，学习为每个样本分配专家权重\"\"\"\n",
    "    def __init__(self, input_size, num_experts):\n",
    "        super().__init__()\n",
    "        self.router = nn.Linear(input_size, num_experts)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, input_size]\n",
    "        # 计算每个专家的权重 (使用softmax确保权重和为1)\n",
    "        router_logits = self.router(x)\n",
    "        router_probs = F.softmax(router_logits, dim=-1)\n",
    "        return router_probs  # [batch_size, num_experts]\n",
    "\n",
    "\n",
    "class DeBERTaMoEClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        pretrained_model_name, \n",
    "        num_classes=3, \n",
    "        freeze_pooler=0,\n",
    "        expert_hidden_size=256,\n",
    "        dropout=0.3,\n",
    "        num_rnn_layers=1,\n",
    "        num_tasks=4  # 新增参数，指定任务数量\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_tasks = num_tasks  # 存储任务数量\n",
    "        \n",
    "        # 使用 AutoModel 加载 DeBERTa 模型\n",
    "        self.bert = AutoModel.from_pretrained(pretrained_model_name)\n",
    "        \n",
    "        # 获取 bert 隐藏层大小\n",
    "        self.bert_hidden_size = self.bert.config.hidden_size\n",
    "        \n",
    "        # 为每个任务创建独立的分类头\n",
    "        self.task_classifiers = nn.ModuleList([\n",
    "            BertClassificationHead(\n",
    "                hidden_size=self.bert_hidden_size,\n",
    "                num_classes=num_classes,\n",
    "                dropout_prob=dropout\n",
    "            ) for _ in range(num_tasks)\n",
    "        ])\n",
    "        \n",
    "        # 创建多个专家模型\n",
    "        self.experts = nn.ModuleDict({\n",
    "            'lstm': ExpertLSTM(\n",
    "                input_size=self.bert_hidden_size, \n",
    "                hidden_size=expert_hidden_size,\n",
    "                num_layers=num_rnn_layers,\n",
    "                dropout=dropout\n",
    "            ),\n",
    "            'bilstm': ExpertBiLSTM(\n",
    "                input_size=self.bert_hidden_size, \n",
    "                hidden_size=expert_hidden_size,\n",
    "                num_layers=num_rnn_layers,\n",
    "                dropout=dropout\n",
    "            ),\n",
    "            'rnn': ExpertRNN(\n",
    "                input_size=self.bert_hidden_size, \n",
    "                hidden_size=expert_hidden_size,\n",
    "                num_layers=num_rnn_layers,\n",
    "                dropout=dropout\n",
    "            ),\n",
    "            'gru': ExpertGRU(\n",
    "                input_size=self.bert_hidden_size, \n",
    "                hidden_size=expert_hidden_size,\n",
    "                num_layers=num_rnn_layers,\n",
    "                dropout=dropout\n",
    "            ),\n",
    "            'linear': ExpertLinear(\n",
    "                input_size=self.bert_hidden_size, \n",
    "                hidden_size=expert_hidden_size\n",
    "            ),\n",
    "        })\n",
    "        \n",
    "        # 创建路由器 (使用[CLS]标记表示作为路由的输入)\n",
    "        self.router = MoERouter(self.bert_hidden_size, len(self.experts))\n",
    "        \n",
    "        # 为每个任务创建专家输出层\n",
    "        self.expert_outputs = nn.ModuleList([\n",
    "            nn.ModuleDict({\n",
    "                expert_name: nn.Linear(expert_hidden_size, num_classes)\n",
    "                for expert_name in self.experts.keys()\n",
    "            }) for _ in range(num_tasks)\n",
    "        ])\n",
    "        \n",
    "        # 为每个任务创建融合层\n",
    "        combined_dim = num_classes * (1 + len(self.experts))\n",
    "        self.final_classifiers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(combined_dim, combined_dim // 2),\n",
    "                nn.LayerNorm(combined_dim // 2),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(combined_dim // 2, num_classes)\n",
    "            ) for _ in range(num_tasks)\n",
    "        ])\n",
    "        \n",
    "        # 创建可学习的损失权重参数\n",
    "        # 初始化Track 4 (Actionability)的权重略大，表示更重视这个任务\n",
    "        # self.loss_weights = nn.Parameter(torch.ones(num_tasks))\n",
    "        # # 设置初始权重，使Track 4的权重初始值更大\n",
    "        # with torch.no_grad():\n",
    "        #     # 假设Track 4是索引3\n",
    "        #     self.loss_weights[3] = 1.5  # 给Track 4一个更高的初始权重\n",
    "        \n",
    "        # 方法1：使用固定的四个权重值\n",
    "        self.loss_weights = nn.Parameter(torch.tensor([0.1, 0.1, 0.1, 0.7]))\n",
    "\n",
    "        # self.gate = \n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # DeBERTa 编码\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # 获取序列隐藏状态\n",
    "        hidden_states = outputs.last_hidden_state  # [batch_size, seq_len, hidden_size]\n",
    "        \n",
    "        # 获取路由权重\n",
    "        cls_embedding = hidden_states[:, 0]  # [batch_size, hidden_size]\n",
    "        routing_weights = self.router(cls_embedding)  # [batch_size, num_experts]\n",
    "        \n",
    "        # 处理每个任务\n",
    "        final_logits_list = []\n",
    "        \n",
    "        for task_idx in range(self.num_tasks):\n",
    "            # 获取当前任务的原始分类头结果\n",
    "            original_logits = self.task_classifiers[task_idx](hidden_states)  # [batch_size, num_classes]\n",
    "            \n",
    "            # 获取各专家结果\n",
    "            expert_logits_list = [original_logits]  # 包含原始分类头\n",
    "            \n",
    "            for expert_name, expert in self.experts.items():\n",
    "                # 获取专家输出\n",
    "                expert_hidden = expert(hidden_states)  # [batch_size, expert_hidden_size]\n",
    "                # 映射到类别空间\n",
    "                expert_logits = self.expert_outputs[task_idx][expert_name](expert_hidden)  # [batch_size, num_classes]\n",
    "                # 添加到列表\n",
    "                expert_logits_list.append(expert_logits)\n",
    "            \n",
    "            # 拼接所有结果 [batch_size, (1+num_experts)*num_classes]\n",
    "            combined_logits = torch.cat(expert_logits_list, dim=1)\n",
    "            \n",
    "            # 通过最终分类器输出当前任务的最终结果\n",
    "            task_final_logits = self.final_classifiers[task_idx](combined_logits)\n",
    "            \n",
    "            final_logits_list.append(task_final_logits)\n",
    "        \n",
    "        # 返回所有任务的预测结果以及损失权重\n",
    "        return {\n",
    "            'logits': final_logits_list,  # 每个任务的预测结果列表\n",
    "            'loss_weights': F.softmax(self.loss_weights, dim=0)  # 归一化的损失权重\n",
    "        }\n",
    "\n",
    "    def get_normalized_loss_weights(self):\n",
    "        \"\"\"获取归一化后的损失权重\"\"\"\n",
    "        return F.softmax(self.loss_weights, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from transformers import AutoModel\n",
    "\n",
    "\n",
    "# class ExpertLSTM(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.1):\n",
    "#         super().__init__()\n",
    "#         self.lstm = nn.LSTM(\n",
    "#             input_size=input_size,\n",
    "#             hidden_size=hidden_size,\n",
    "#             num_layers=num_layers,\n",
    "#             batch_first=True,\n",
    "#             dropout=dropout if num_layers > 1 else 0,\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # x: [batch_size, seq_len, input_size]\n",
    "#         output, (hidden, _) = self.lstm(x)\n",
    "#         # 返回最后一个时间步的隐藏状态\n",
    "#         return hidden[-1]  # [batch_size, hidden_size]\n",
    "\n",
    "\n",
    "# class ExpertBiLSTM(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.1):\n",
    "#         super().__init__()\n",
    "#         self.bilstm = nn.LSTM(\n",
    "#             input_size=input_size,\n",
    "#             hidden_size=hidden_size // 2,  # 因为是双向的，所以每个方向的隐藏层大小减半\n",
    "#             num_layers=num_layers,\n",
    "#             batch_first=True,\n",
    "#             bidirectional=True,\n",
    "#             dropout=dropout if num_layers > 1 else 0,\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # x: [batch_size, seq_len, input_size]\n",
    "#         output, (hidden, _) = self.bilstm(x)\n",
    "#         # 拼接最后一层的正向和反向隐藏状态\n",
    "#         # hidden shape: [num_layers * num_directions, batch_size, hidden_size//2]\n",
    "#         hidden_forward = hidden[-2]  # 正向的最后一层 [batch_size, hidden_size//2]\n",
    "#         hidden_backward = hidden[-1]  # 反向的最后一层 [batch_size, hidden_size//2]\n",
    "#         hidden_concat = torch.cat([hidden_forward, hidden_backward], dim=1)  # [batch_size, hidden_size]\n",
    "#         return hidden_concat\n",
    "\n",
    "\n",
    "# class ExpertRNN(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.1):\n",
    "#         super().__init__()\n",
    "#         self.rnn = nn.RNN(\n",
    "#             input_size=input_size,\n",
    "#             hidden_size=hidden_size,\n",
    "#             num_layers=num_layers,\n",
    "#             batch_first=True,\n",
    "#             dropout=dropout if num_layers > 1 else 0,\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # x: [batch_size, seq_len, input_size]\n",
    "#         _, hidden = self.rnn(x)\n",
    "#         # 返回最后一个时间步的隐藏状态\n",
    "#         return hidden[-1]  # [batch_size, hidden_size]\n",
    "\n",
    "\n",
    "# class ExpertGRU(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.1):\n",
    "#         super().__init__()\n",
    "#         self.gru = nn.GRU(\n",
    "#             input_size=input_size,\n",
    "#             hidden_size=hidden_size,\n",
    "#             num_layers=num_layers,\n",
    "#             batch_first=True,\n",
    "#             dropout=dropout if num_layers > 1 else 0,\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # x: [batch_size, seq_len, input_size]\n",
    "#         _, hidden = self.gru(x)\n",
    "#         # 返回最后一个时间步的隐藏状态\n",
    "#         return hidden[-1]  # [batch_size, hidden_size]\n",
    "\n",
    "\n",
    "# class ExpertLinear(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size):\n",
    "#         super().__init__()\n",
    "#         self.linear = nn.Sequential(\n",
    "#             nn.Linear(input_size, hidden_size * 2),\n",
    "#             nn.LayerNorm(hidden_size * 2),\n",
    "#             nn.GELU(),\n",
    "#             nn.Linear(hidden_size * 2, hidden_size)\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # x: [batch_size, seq_len, input_size]\n",
    "#         # 我们需要把序列信息压缩为一个向量，可以使用平均池化\n",
    "#         pooled = torch.mean(x, dim=1)  # [batch_size, input_size]\n",
    "#         return self.linear(pooled)  # [batch_size, hidden_size]\n",
    "\n",
    "\n",
    "# class BertClassificationHead(nn.Module):\n",
    "#     def __init__(self, hidden_size=1024, num_classes=3, dropout_prob=0.3):\n",
    "#         super().__init__()\n",
    "#         self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.dropout = nn.Dropout(dropout_prob)\n",
    "#         self.out_proj = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "#     def forward(self, features):\n",
    "#         # 提取 [CLS] 标记的表示\n",
    "#         x = features[:, 0, :]  # 使用第一个标记([CLS])\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.dense(x)\n",
    "#         x = torch.relu(x)\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.out_proj(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "# class MoERouter(nn.Module):\n",
    "#     \"\"\"专家路由器，学习为每个样本分配专家权重\"\"\"\n",
    "#     def __init__(self, input_size, num_experts):\n",
    "#         super().__init__()\n",
    "#         self.router = nn.Linear(input_size, num_experts)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # x: [batch_size, input_size]\n",
    "#         # 计算每个专家的权重 (使用softmax确保权重和为1)\n",
    "#         router_logits = self.router(x)\n",
    "#         router_probs = F.softmax(router_logits, dim=-1)\n",
    "#         return router_probs  # [batch_size, num_experts]\n",
    "\n",
    "\n",
    "# class DeBERTaMoEClassifier(nn.Module):\n",
    "#     def __init__(\n",
    "#         self, \n",
    "#         pretrained_model_name, \n",
    "#         num_classes=3, \n",
    "#         freeze_pooler=0,\n",
    "#         expert_hidden_size=256,\n",
    "#         dropout=0.3,\n",
    "#         num_rnn_layers=1,\n",
    "#         num_tasks=4  # 新增参数，指定任务数量\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.num_tasks = num_tasks  # 存储任务数量\n",
    "        \n",
    "#         # 使用 AutoModel 加载 DeBERTa 模型\n",
    "#         self.bert = AutoModel.from_pretrained(pretrained_model_name)\n",
    "        \n",
    "#         # 获取 bert 隐藏层大小\n",
    "#         self.bert_hidden_size = self.bert.config.hidden_size\n",
    "        \n",
    "#         # 为每个任务创建独立的分类头\n",
    "#         self.task_classifiers = nn.ModuleList([\n",
    "#             BertClassificationHead(\n",
    "#                 hidden_size=self.bert_hidden_size,\n",
    "#                 num_classes=num_classes,\n",
    "#                 dropout_prob=dropout\n",
    "#             ) for _ in range(num_tasks)\n",
    "#         ])\n",
    "        \n",
    "#         # 创建多个专家模型\n",
    "#         self.experts = nn.ModuleDict({\n",
    "#             'lstm': ExpertLSTM(\n",
    "#                 input_size=self.bert_hidden_size, \n",
    "#                 hidden_size=expert_hidden_size,\n",
    "#                 num_layers=num_rnn_layers,\n",
    "#                 dropout=dropout\n",
    "#             ),\n",
    "#             'bilstm': ExpertBiLSTM(\n",
    "#                 input_size=self.bert_hidden_size, \n",
    "#                 hidden_size=expert_hidden_size,\n",
    "#                 num_layers=num_rnn_layers,\n",
    "#                 dropout=dropout\n",
    "#             ),\n",
    "#             'rnn': ExpertRNN(\n",
    "#                 input_size=self.bert_hidden_size, \n",
    "#                 hidden_size=expert_hidden_size,\n",
    "#                 num_layers=num_rnn_layers,\n",
    "#                 dropout=dropout\n",
    "#             ),\n",
    "#             'gru': ExpertGRU(\n",
    "#                 input_size=self.bert_hidden_size, \n",
    "#                 hidden_size=expert_hidden_size,\n",
    "#                 num_layers=num_rnn_layers,\n",
    "#                 dropout=dropout\n",
    "#             ),\n",
    "#             'linear': ExpertLinear(\n",
    "#                 input_size=self.bert_hidden_size, \n",
    "#                 hidden_size=expert_hidden_size\n",
    "#             ),\n",
    "#         })\n",
    "        \n",
    "#         # 创建路由器 (使用[CLS]标记表示作为路由的输入)\n",
    "#         self.router = MoERouter(self.bert_hidden_size, len(self.experts))\n",
    "        \n",
    "#         # 为每个任务创建专家输出层\n",
    "#         self.expert_outputs = nn.ModuleList([\n",
    "#             nn.ModuleDict({\n",
    "#                 expert_name: nn.Linear(expert_hidden_size, self.bert_hidden_size)\n",
    "#                 for expert_name in self.experts.keys()\n",
    "#             }) for _ in range(num_tasks)\n",
    "#         ])\n",
    "        \n",
    "#         # 为每个任务创建融合层\n",
    "#         combined_dim = self.bert_hidden_size\n",
    "#         self.final_classifiers = nn.ModuleList([\n",
    "#             nn.Sequential(\n",
    "#                 # nn.Linear(combined_dim, combined_dim // 2),\n",
    "#                 nn.LayerNorm(combined_dim),\n",
    "#                 nn.Dropout(dropout),\n",
    "#                 nn.ReLU(),\n",
    "#                 nn.Linear(combined_dim, num_classes)\n",
    "#             ) for _ in range(num_tasks)\n",
    "#         ])\n",
    "        \n",
    "#         # 创建可学习的损失权重参数\n",
    "#         # 初始化Track 4 (Actionability)的权重略大，表示更重视这个任务\n",
    "#         # self.loss_weights = nn.Parameter(torch.ones(num_tasks))\n",
    "#         # # 设置初始权重，使Track 4的权重初始值更大\n",
    "#         # with torch.no_grad():\n",
    "#         #     # 假设Track 4是索引3\n",
    "#         #     self.loss_weights[3] = 1.5  # 给Track 4一个更高的初始权重\n",
    "        \n",
    "#         # 方法1：使用固定的四个权重值\n",
    "#         self.loss_weights = nn.Parameter(torch.tensor([0.1, 0.1, 0.2, 0.6]))\n",
    "\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         # DeBERTa 编码\n",
    "#         outputs = self.bert(\n",
    "#             input_ids=input_ids,\n",
    "#             attention_mask=attention_mask\n",
    "#         )\n",
    "        \n",
    "#         # 获取序列隐藏状态\n",
    "#         hidden_states = outputs.last_hidden_state  # [batch_size, seq_len, hidden_size]\n",
    "        \n",
    "#         # 获取路由权重\n",
    "#         cls_embedding = hidden_states[:, 0]  # [batch_size, hidden_size]\n",
    "#         routing_weights = self.router(cls_embedding)  # [batch_size, num_experts]\n",
    "        \n",
    "#         # 处理每个任务\n",
    "#         final_logits_list = []\n",
    "        \n",
    "#         for task_idx in range(self.num_tasks):\n",
    "        \n",
    "#             # 在每个任务循环内部重置专家输出列表\n",
    "#             expert_logits_list = []\n",
    "        \n",
    "#             for expert_name, expert in self.experts.items():\n",
    "#                 # 获取专家输出\n",
    "#                 expert_hidden = expert(hidden_states)  # [batch_size, expert_hidden_size]\n",
    "#                 # 映射到类别空间\n",
    "#                 expert_logits = self.expert_outputs[task_idx][expert_name](expert_hidden)  # [batch_size, num_classes]\n",
    "#                 # 添加到列表\n",
    "#                 expert_logits_list.append(expert_logits)\n",
    "            \n",
    "#             stack_logits = torch.stack(expert_logits_list, dim=1)  # [b, expert_num, h]\n",
    "#             moe_logits = torch.sum(routing_weights.unsqueeze(-1) * stack_logits, dim=1)\n",
    "            \n",
    "#             # 通过最终分类器输出当前任务的最终结果\n",
    "#             task_final_logits = self.final_classifiers[task_idx](moe_logits)\n",
    "            \n",
    "#             final_logits_list.append(task_final_logits)\n",
    "        \n",
    "#         # 返回所有任务的预测结果以及损失权重\n",
    "#         return {\n",
    "#             'logits': final_logits_list,  # 每个任务的预测结果列表\n",
    "#             'loss_weights': F.softmax(self.loss_weights, dim=0)  # 归一化的损失权重\n",
    "#         }\n",
    "\n",
    "#     def get_normalized_loss_weights(self):\n",
    "#         \"\"\"获取归一化后的损失权重\"\"\"\n",
    "#         return F.softmax(self.loss_weights, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "import random\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import AdamW\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "# 如果在Jupyter Notebook中运行，可以使用这个自定义参数函数替代argparser\n",
    "def get_default_configs():\n",
    "    \"\"\"在Jupyter环境中使用的默认配置，避免argparse解析错误\"\"\"\n",
    "    class Args:\n",
    "        def __init__(self):\n",
    "            # self.model_name = '/mnt/cfs/huangzhiwei/pykt-moekt/SBM/bge-large-en-v1.5'\n",
    "            # self.model_name = \"/mnt/cfs/huangzhiwei/BAE2025/models/ModernBERT-large\"\n",
    "            # self.model_name = '/mnt/cfs/huangzhiwei/pykt-moekt/SBM/xlm-roberta-large'\n",
    "            # self.model_name = '/mnt/cfs/huangzhiwei/BAE2025/models/bge-base-en-v1.5'\n",
    "            # self.model_name = '/mnt/cfs/huangzhiwei/BAE2025/models/bert-base-uncased'\n",
    "            self.model_name = '/mnt/cfs/huangzhiwei/BAE2025/models/deberta-v3-base'\n",
    "            # self.model_name = '/mnt/cfs/huangzhiwei/BAE2025/models/roberta-base'\n",
    "            self.num_classes = 3\n",
    "            self.dropout = 0.25\n",
    "            self.freeze_pooler = 8\n",
    "            self.batch_size = 16\n",
    "            self.max_length = 512\n",
    "            self.lr = 2e-5\n",
    "            self.epochs = 50\n",
    "            self.device = device\n",
    "            self.name = None\n",
    "            self.seed = 42\n",
    "            self.data_path = '../data_new/all.json'\n",
    "            self.val_data_path = '../data_new/val.json'\n",
    "            self.checkpoint_dir = '/mnt/cfs/huangzhiwei/BAE2025/projects/predict/1234'\n",
    "            self.patience = 8\n",
    "            self.expert_hidden_size = 512\n",
    "            self.num_rnn_layers = 1\n",
    "            self.warmup_ratio = 0.1\n",
    "            self.num_tasks = 4   # 新增参数，指定任务数量\n",
    "            self.exp_name = 'BAE2025_track4_bert'\n",
    "    return Args()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Jupyter environment, using default configs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前使用的 tokenizer 类型： <class 'transformers.models.deberta_v2.tokenization_deberta_v2_fast.DebertaV2TokenizerFast'>\n",
      "当前使用的 tokenizer 类型： <class 'transformers.models.deberta_v2.tokenization_deberta_v2_fast.DebertaV2TokenizerFast'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/50: 100%|█| 154/154 [01:28<00:00,  1.74batch/s, loss=0.943, lr=0.000004, weights=w1:0.21 w2:0.21 w3:0.21 w4:0.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.9897\n",
      "Task track1 - Loss: 0.8796, Acc: 0.6335, F1: 0.2966\n",
      "Task track2 - Loss: 1.0064, Acc: 0.5503, F1: 0.3337\n",
      "Task track3 - Loss: 1.0269, Acc: 0.5495, F1: 0.2627\n",
      "Task track4 - Loss: 1.0206, Acc: 0.4529, F1: 0.3125\n",
      "Current Loss Weights: [0.20745215 0.20736727 0.20735952 0.37782112]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|█| 154/154 [01:29<00:00,  1.72batch/s, loss=0.730, lr=0.000008, weights=w1:0.21 w2:0.21 w3:0.21 w4:0.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.9303\n",
      "Task track1 - Loss: 0.7400, Acc: 0.7358, F1: 0.3024\n",
      "Task track2 - Loss: 0.9328, Acc: 0.6031, F1: 0.2930\n",
      "Task track3 - Loss: 0.9954, Acc: 0.5686, F1: 0.2456\n",
      "Task track4 - Loss: 0.9979, Acc: 0.5004, F1: 0.3136\n",
      "Current Loss Weights: [0.20768104 0.20739464 0.20728901 0.37763524]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|█| 154/154 [01:29<00:00,  1.73batch/s, loss=0.822, lr=0.000012, weights=w1:0.21 w2:0.21 w3:0.21 w4:0.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.9247\n",
      "Task track1 - Loss: 0.7273, Acc: 0.7447, F1: 0.3174\n",
      "Task track2 - Loss: 0.9354, Acc: 0.5933, F1: 0.2941\n",
      "Task track3 - Loss: 0.9916, Acc: 0.5649, F1: 0.2519\n",
      "Task track4 - Loss: 0.9907, Acc: 0.5073, F1: 0.3062\n",
      "Current Loss Weights: [0.2080429  0.2074159  0.20715913 0.37738204]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|█| 154/154 [01:28<00:00,  1.73batch/s, loss=0.871, lr=0.000016, weights=w1:0.21 w2:0.21 w3:0.21 w4:0.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.9001\n",
      "Task track1 - Loss: 0.7091, Acc: 0.7386, F1: 0.3130\n",
      "Task track2 - Loss: 0.8937, Acc: 0.6185, F1: 0.3137\n",
      "Task track3 - Loss: 0.9771, Acc: 0.5686, F1: 0.2572\n",
      "Task track4 - Loss: 0.9668, Acc: 0.5357, F1: 0.3476\n",
      "Current Loss Weights: [0.20851013 0.20753069 0.20694102 0.37701815]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|█| 154/154 [01:28<00:00,  1.73batch/s, loss=0.896, lr=0.000020, weights=w1:0.21 w2:0.21 w3:0.21 w4:0.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.8551\n",
      "Task track1 - Loss: 0.6585, Acc: 0.7719, F1: 0.3492\n",
      "Task track2 - Loss: 0.8583, Acc: 0.6627, F1: 0.4051\n",
      "Task track3 - Loss: 0.9363, Acc: 0.5942, F1: 0.3481\n",
      "Task track4 - Loss: 0.9176, Acc: 0.6185, F1: 0.4370\n",
      "Current Loss Weights: [0.20914547 0.20762795 0.2066485  0.37657815]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|█| 154/154 [01:28<00:00,  1.74batch/s, loss=0.843, lr=0.000020, weights=w1:0.21 w2:0.21 w3:0.21 w4:0.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.8256\n",
      "Task track1 - Loss: 0.6208, Acc: 0.8263, F1: 0.5066\n",
      "Task track2 - Loss: 0.8426, Acc: 0.6859, F1: 0.4308\n",
      "Task track3 - Loss: 0.9210, Acc: 0.6177, F1: 0.3973\n",
      "Task track4 - Loss: 0.8780, Acc: 0.6757, F1: 0.4795\n",
      "Current Loss Weights: [0.2098579  0.20762862 0.20626982 0.3762437 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|█| 154/154 [01:27<00:00,  1.76batch/s, loss=0.835, lr=0.000020, weights=w1:0.21 w2:0.21 w3:0.21 w4:0.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.8034\n",
      "Task track1 - Loss: 0.5986, Acc: 0.8344, F1: 0.5298\n",
      "Task track2 - Loss: 0.8255, Acc: 0.7074, F1: 0.4538\n",
      "Task track3 - Loss: 0.9096, Acc: 0.6262, F1: 0.4181\n",
      "Task track4 - Loss: 0.8474, Acc: 0.6660, F1: 0.4751\n",
      "Current Loss Weights: [0.2105682  0.20758832 0.20583075 0.37601277]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|█| 154/154 [01:26<00:00,  1.78batch/s, loss=0.919, lr=0.000020, weights=w1:0.21 w2:0.21 w3:0.21 w4:0.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.7866\n",
      "Task track1 - Loss: 0.5919, Acc: 0.8320, F1: 0.5260\n",
      "Task track2 - Loss: 0.8110, Acc: 0.7127, F1: 0.4594\n",
      "Task track3 - Loss: 0.8964, Acc: 0.6299, F1: 0.4369\n",
      "Task track4 - Loss: 0.8221, Acc: 0.6968, F1: 0.4977\n",
      "Current Loss Weights: [0.21123397 0.20755833 0.20542131 0.37578642]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|█| 154/154 [01:27<00:00,  1.77batch/s, loss=0.787, lr=0.000020, weights=w1:0.21 w2:0.21 w3:0.20 w4:0.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.7686\n",
      "Task track1 - Loss: 0.5770, Acc: 0.8283, F1: 0.5169\n",
      "Task track2 - Loss: 0.7990, Acc: 0.7212, F1: 0.4699\n",
      "Task track3 - Loss: 0.8833, Acc: 0.6408, F1: 0.4585\n",
      "Task track4 - Loss: 0.7970, Acc: 0.7078, F1: 0.5070\n",
      "Current Loss Weights: [0.2118851  0.20745765 0.20496753 0.37568974]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|█| 154/154 [01:26<00:00,  1.79batch/s, loss=0.923, lr=0.000019, weights=w1:0.21 w2:0.21 w3:0.20 w4:0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.7546\n",
      "Task track1 - Loss: 0.5714, Acc: 0.8401, F1: 0.5278\n",
      "Task track2 - Loss: 0.7820, Acc: 0.7224, F1: 0.4701\n",
      "Task track3 - Loss: 0.8746, Acc: 0.6575, F1: 0.5173\n",
      "Task track4 - Loss: 0.7776, Acc: 0.7382, F1: 0.5311\n",
      "Current Loss Weights: [0.21250169 0.2073601  0.20454463 0.3755935 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|█| 154/154 [01:26<00:00,  1.79batch/s, loss=0.660, lr=0.000019, weights=w1:0.21 w2:0.21 w3:0.20 w4:0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.7401\n",
      "Task track1 - Loss: 0.5542, Acc: 0.8417, F1: 0.5359\n",
      "Task track2 - Loss: 0.7774, Acc: 0.7297, F1: 0.4790\n",
      "Task track3 - Loss: 0.8585, Acc: 0.6696, F1: 0.5353\n",
      "Task track4 - Loss: 0.7604, Acc: 0.7350, F1: 0.5280\n",
      "Current Loss Weights: [0.21310651 0.20721084 0.20412597 0.37555668]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|█| 154/154 [01:26<00:00,  1.79batch/s, loss=0.662, lr=0.000019, weights=w1:0.21 w2:0.21 w3:0.20 w4:0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.7280\n",
      "Task track1 - Loss: 0.5528, Acc: 0.8356, F1: 0.5455\n",
      "Task track2 - Loss: 0.7538, Acc: 0.7451, F1: 0.4960\n",
      "Task track3 - Loss: 0.8493, Acc: 0.6769, F1: 0.5504\n",
      "Task track4 - Loss: 0.7475, Acc: 0.7488, F1: 0.5478\n",
      "Current Loss Weights: [0.21368396 0.20713124 0.20368913 0.37549567]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|█| 154/154 [01:26<00:00,  1.79batch/s, loss=0.755, lr=0.000018, weights=w1:0.21 w2:0.21 w3:0.20 w4:0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.7043\n",
      "Task track1 - Loss: 0.5339, Acc: 0.8429, F1: 0.5510\n",
      "Task track2 - Loss: 0.7271, Acc: 0.7626, F1: 0.5119\n",
      "Task track3 - Loss: 0.8269, Acc: 0.6924, F1: 0.5714\n",
      "Task track4 - Loss: 0.7224, Acc: 0.7654, F1: 0.5685\n",
      "Current Loss Weights: [0.2142418  0.20705909 0.20331177 0.37538725]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|█| 154/154 [01:26<00:00,  1.78batch/s, loss=0.590, lr=0.000018, weights=w1:0.21 w2:0.21 w3:0.20 w4:0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6901\n",
      "Task track1 - Loss: 0.5166, Acc: 0.8523, F1: 0.5648\n",
      "Task track2 - Loss: 0.7077, Acc: 0.7739, F1: 0.5234\n",
      "Task track3 - Loss: 0.8085, Acc: 0.7001, F1: 0.5865\n",
      "Task track4 - Loss: 0.7156, Acc: 0.7654, F1: 0.5991\n",
      "Current Loss Weights: [0.21479398 0.20704433 0.20291424 0.37524745]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|█| 154/154 [01:26<00:00,  1.79batch/s, loss=0.547, lr=0.000018, weights=w1:0.22 w2:0.21 w3:0.20 w4:0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6725\n",
      "Task track1 - Loss: 0.5105, Acc: 0.8575, F1: 0.5824\n",
      "Task track2 - Loss: 0.6971, Acc: 0.7886, F1: 0.5355\n",
      "Task track3 - Loss: 0.7886, Acc: 0.7204, F1: 0.6172\n",
      "Task track4 - Loss: 0.6891, Acc: 0.7841, F1: 0.6284\n",
      "Current Loss Weights: [0.21531011 0.20696661 0.20255445 0.37516883]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|█| 154/154 [01:26<00:00,  1.79batch/s, loss=0.698, lr=0.000017, weights=w1:0.22 w2:0.21 w3:0.20 w4:0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6584\n",
      "Task track1 - Loss: 0.5142, Acc: 0.8543, F1: 0.5672\n",
      "Task track2 - Loss: 0.6834, Acc: 0.7877, F1: 0.5319\n",
      "Task track3 - Loss: 0.7659, Acc: 0.7273, F1: 0.6257\n",
      "Task track4 - Loss: 0.6694, Acc: 0.8003, F1: 0.6488\n",
      "Current Loss Weights: [0.2157625  0.20687877 0.20221183 0.37514693]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|█| 154/154 [01:26<00:00,  1.79batch/s, loss=0.717, lr=0.000017, weights=w1:0.22 w2:0.21 w3:0.20 w4:0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6427\n",
      "Task track1 - Loss: 0.5001, Acc: 0.8547, F1: 0.5557\n",
      "Task track2 - Loss: 0.6635, Acc: 0.8072, F1: 0.5492\n",
      "Task track3 - Loss: 0.7578, Acc: 0.7435, F1: 0.6562\n",
      "Task track4 - Loss: 0.6513, Acc: 0.8101, F1: 0.6727\n",
      "Current Loss Weights: [0.2161974  0.20681542 0.20185505 0.37513214]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|█| 154/154 [01:26<00:00,  1.78batch/s, loss=0.574, lr=0.000016, weights=w1:0.22 w2:0.21 w3:0.20 w4:0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6215\n",
      "Task track1 - Loss: 0.4773, Acc: 0.8653, F1: 0.5785\n",
      "Task track2 - Loss: 0.6389, Acc: 0.8239, F1: 0.5619\n",
      "Task track3 - Loss: 0.7470, Acc: 0.7472, F1: 0.6625\n",
      "Task track4 - Loss: 0.6275, Acc: 0.8231, F1: 0.6917\n",
      "Current Loss Weights: [0.21662502 0.2067537  0.20146444 0.37515685]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|█| 154/154 [01:26<00:00,  1.79batch/s, loss=0.619, lr=0.000016, weights=w1:0.22 w2:0.21 w3:0.20 w4:0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6099\n",
      "Task track1 - Loss: 0.4810, Acc: 0.8567, F1: 0.5722\n",
      "Task track2 - Loss: 0.6319, Acc: 0.8186, F1: 0.5580\n",
      "Task track3 - Loss: 0.7163, Acc: 0.7707, F1: 0.6983\n",
      "Task track4 - Loss: 0.6151, Acc: 0.8348, F1: 0.7102\n",
      "Current Loss Weights: [0.21700901 0.20668548 0.20115222 0.37515324]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|█| 154/154 [01:26<00:00,  1.79batch/s, loss=0.568, lr=0.000015, weights=w1:0.22 w2:0.21 w3:0.20 w4:0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6000\n",
      "Task track1 - Loss: 0.4748, Acc: 0.8649, F1: 0.5820\n",
      "Task track2 - Loss: 0.6130, Acc: 0.8304, F1: 0.5668\n",
      "Task track3 - Loss: 0.7130, Acc: 0.7768, F1: 0.7099\n",
      "Task track4 - Loss: 0.6047, Acc: 0.8377, F1: 0.7202\n",
      "Current Loss Weights: [0.21737087 0.20664188 0.20082982 0.37515748]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|█| 154/154 [01:26<00:00,  1.79batch/s, loss=0.477, lr=0.000014, weights=w1:0.22 w2:0.21 w3:0.20 w4:0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5903\n",
      "Task track1 - Loss: 0.4730, Acc: 0.8608, F1: 0.5678\n",
      "Task track2 - Loss: 0.6102, Acc: 0.8332, F1: 0.5683\n",
      "Task track3 - Loss: 0.6924, Acc: 0.7886, F1: 0.7238\n",
      "Task track4 - Loss: 0.5927, Acc: 0.8429, F1: 0.7397\n",
      "Current Loss Weights: [0.2177144  0.20658536 0.20053881 0.37516144]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|█| 154/154 [01:26<00:00,  1.79batch/s, loss=0.578, lr=0.000014, weights=w1:0.22 w2:0.21 w3:0.20 w4:0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5798\n",
      "Task track1 - Loss: 0.4624, Acc: 0.8689, F1: 0.5898\n",
      "Task track2 - Loss: 0.6080, Acc: 0.8304, F1: 0.5662\n",
      "Task track3 - Loss: 0.7067, Acc: 0.7723, F1: 0.7017\n",
      "Task track4 - Loss: 0.5648, Acc: 0.8486, F1: 0.7399\n",
      "Current Loss Weights: [0.21801776 0.20647296 0.20018995 0.37531927]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|█| 154/154 [01:26<00:00,  1.79batch/s, loss=0.491, lr=0.000013, weights=w1:0.22 w2:0.21 w3:0.20 w4:0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5592\n",
      "Task track1 - Loss: 0.4543, Acc: 0.8689, F1: 0.6055\n",
      "Task track2 - Loss: 0.5946, Acc: 0.8397, F1: 0.5736\n",
      "Task track3 - Loss: 0.6788, Acc: 0.7930, F1: 0.7301\n",
      "Task track4 - Loss: 0.5369, Acc: 0.8620, F1: 0.7664\n",
      "Current Loss Weights: [0.21828558 0.20632422 0.19985993 0.37553027]\n"
     ]
    }
   ],
   "source": [
    "def train(configs):\n",
    "    \n",
    "    # 设置随机种子\n",
    "    random.seed(configs.seed)\n",
    "    np.random.seed(configs.seed)\n",
    "    torch.manual_seed(configs.seed)\n",
    "    \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # 创建检查点目录\n",
    "    checkpoint_dir = os.path.join(configs.checkpoint_dir, configs.exp_name)\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # 为保存混淆矩阵创建目录 - 为每个任务分别创建\n",
    "    task_names = [\"track1\", \"track2\", \"track3\", \"track4\"]\n",
    "    plot_dirs = {}\n",
    "    \n",
    "    for task_name in task_names:\n",
    "        train_plot_dir = os.path.join(checkpoint_dir, 'plots', task_name, 'train')\n",
    "        val_plot_dir = os.path.join(checkpoint_dir, 'plots', task_name, 'val')\n",
    "        os.makedirs(train_plot_dir, exist_ok=True)\n",
    "        os.makedirs(val_plot_dir, exist_ok=True)\n",
    "        plot_dirs[task_name] = {\n",
    "            'train': train_plot_dir,\n",
    "            'val': val_plot_dir\n",
    "        }\n",
    "    \n",
    "    # 加载数据集\n",
    "    train_dataset = BAE2025Dataset(configs.data_path)\n",
    "    val_dataset = BAE2025Dataset(configs.val_data_path)    \n",
    "\n",
    "    # 创建数据加载器\n",
    "    train_dataloader = BAE2025DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=configs.batch_size,\n",
    "        max_length=configs.max_length,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        device=configs.device,\n",
    "        tokenizer_name=configs.model_name\n",
    "    )\n",
    "\n",
    "    val_dataloader = BAE2025DataLoader(\n",
    "        dataset=val_dataset,\n",
    "        batch_size=configs.batch_size,\n",
    "        max_length=configs.max_length,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        device=configs.device,\n",
    "        tokenizer_name=configs.model_name\n",
    "    )\n",
    "    \n",
    "    # 创建多任务模型\n",
    "    model = DeBERTaMoEClassifier(\n",
    "        pretrained_model_name=configs.model_name,\n",
    "        num_classes=configs.num_classes,\n",
    "        freeze_pooler=configs.freeze_pooler,\n",
    "        num_rnn_layers=configs.num_rnn_layers,\n",
    "        expert_hidden_size=configs.expert_hidden_size,\n",
    "        dropout=configs.dropout,\n",
    "        num_tasks=configs.num_tasks  # 指定4个任务\n",
    "    ).to(configs.device)\n",
    "\n",
    "    # 为每个任务定义交叉熵损失函数\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # 定义优化器\n",
    "    optimizer = AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=configs.lr\n",
    "    )\n",
    "    \n",
    "    # 添加Warmup + Cosine Decay学习率调度\n",
    "    from transformers import get_cosine_schedule_with_warmup\n",
    "    \n",
    "    # 计算总训练步数\n",
    "    total_steps = len(train_dataloader) * configs.epochs\n",
    "    \n",
    "    # 计算warmup步数 (默认总步数的10%，可通过configs.warmup_ratio调整)\n",
    "    warmup_ratio = getattr(configs, 'warmup_ratio', 0.1)  # 如果未定义，则使用默认值0.1\n",
    "    warmup_steps = int(warmup_ratio * total_steps)\n",
    "    \n",
    "    # 创建学习率调度器\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # 初始化最佳验证损失和早停计数器\n",
    "    best_val_acc = 0.0\n",
    "    best_val_f1 = 0.0\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # 定义类别名称\n",
    "    class_names = ['Yes', 'To some extent', 'No']\n",
    "    \n",
    "    # 添加F1计算所需的库\n",
    "    from sklearn.metrics import f1_score, confusion_matrix\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # 训练循环\n",
    "    for epoch in range(configs.epochs):\n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_task_losses = [0.0] * configs.num_tasks  # 记录每个任务的损失\n",
    "        train_task_accs = [0.0] * configs.num_tasks    # 记录每个任务的准确率\n",
    "        train_task_preds = [[] for _ in range(configs.num_tasks)]  # 每个任务的预测结果\n",
    "        train_task_labels = [[] for _ in range(configs.num_tasks)]  # 每个任务的真实标签\n",
    "        \n",
    "        with tqdm(\n",
    "            train_dataloader,\n",
    "            total=len(train_dataloader),\n",
    "            desc=f'Epoch {epoch + 1}/{configs.epochs}',\n",
    "            unit='batch',\n",
    "            ncols=120\n",
    "        ) as pbar:\n",
    "            for input_ids, attention_mask, labels in pbar:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # 前向传播\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                logits_list = outputs['logits']  # 每个任务的预测结果列表\n",
    "                loss_weights = outputs['loss_weights']  # 可学习的损失权重\n",
    "                \n",
    "                # 计算每个任务的损失并加权求和\n",
    "                batch_losses = []\n",
    "                for task_idx in range(configs.num_tasks):\n",
    "                    task_labels = labels[:, task_idx].long()\n",
    "                    task_loss = criterion(logits_list[task_idx], task_labels)\n",
    "                    batch_losses.append(task_loss)\n",
    "                    train_task_losses[task_idx] += task_loss.item()\n",
    "                    \n",
    "                    # 计算当前任务的准确率\n",
    "                    task_preds = logits_list[task_idx].argmax(dim=1)\n",
    "                    task_acc = (task_preds == task_labels).float().mean()\n",
    "                    train_task_accs[task_idx] += task_acc.item()\n",
    "                    \n",
    "                    # 收集预测结果和真实标签\n",
    "                    train_task_preds[task_idx].extend(task_preds.cpu().numpy())\n",
    "                    train_task_labels[task_idx].extend(task_labels.cpu().numpy())\n",
    "                \n",
    "                # 将每个损失与其权重相乘，然后求和\n",
    "                weighted_losses = [loss * weight for loss, weight in zip(batch_losses, loss_weights)]\n",
    "                final_loss = sum(weighted_losses)\n",
    "                \n",
    "                # 反向传播\n",
    "                final_loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # 更新学习率\n",
    "                \n",
    "                train_loss += final_loss.item()\n",
    "                \n",
    "                # 更新进度条显示\n",
    "                curr_lr = scheduler.get_last_lr()[0]\n",
    "                curr_weights = loss_weights.detach().cpu().numpy()\n",
    "                \n",
    "                # 格式化权重显示\n",
    "                weights_str = \" \".join([f\"w{i+1}:{w:.2f}\" for i, w in enumerate(curr_weights)])\n",
    "                \n",
    "                pbar.set_postfix(\n",
    "                    loss=f'{final_loss.item():.3f}',\n",
    "                    lr=f'{curr_lr:.6f}',\n",
    "                    weights=weights_str\n",
    "                )\n",
    "        \n",
    "        # 计算每个任务的平均损失和准确率\n",
    "        train_loss = train_loss / len(train_dataloader)\n",
    "        train_task_losses = [loss / len(train_dataloader) for loss in train_task_losses]\n",
    "        train_task_accs = [acc / len(train_dataloader) for acc in train_task_accs]\n",
    "        \n",
    "        # 计算每个任务的F1分数\n",
    "        train_task_f1s = [\n",
    "            f1_score(labels, preds, average='macro') \n",
    "            for labels, preds in zip(train_task_labels, train_task_preds)\n",
    "        ]\n",
    "        \n",
    "        # 打印训练结果\n",
    "        print(f'Training Loss: {train_loss:.4f}')\n",
    "        for task_idx, task_name in enumerate(task_names):\n",
    "            print(f'Task {task_name} - Loss: {train_task_losses[task_idx]:.4f}, '\n",
    "                  f'Acc: {train_task_accs[task_idx]:.4f}, '\n",
    "                  f'F1: {train_task_f1s[task_idx]:.4f}')\n",
    "        \n",
    "        # 打印当前的损失权重\n",
    "        print(f'Current Loss Weights: {loss_weights.detach().cpu().numpy()}')\n",
    "        \n",
    "        # 为每个任务创建混淆矩阵\n",
    "        for task_idx, task_name in enumerate(task_names):\n",
    "            # 创建完整的三分类混淆矩阵\n",
    "            cm_full = confusion_matrix(\n",
    "                train_task_labels[task_idx], \n",
    "                train_task_preds[task_idx], \n",
    "                labels=[0, 1, 2]\n",
    "            )\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(cm_full, annot=True, fmt='d', cmap='Blues',\n",
    "                        xticklabels=class_names,\n",
    "                        yticklabels=class_names)\n",
    "            plt.xlabel('Predicted')\n",
    "            plt.ylabel('True')\n",
    "            plt.title(f'Train: {task_name} Confusion Matrix\\n'\n",
    "                      f'Acc: {train_task_accs[task_idx]:.4f}, '\n",
    "                      f'F1: {train_task_f1s[task_idx]:.4f}')\n",
    "            \n",
    "            # 保存完整混淆矩阵\n",
    "            matrix_path = os.path.join(plot_dirs[task_name]['train'], f'cm_full_epoch_{epoch+1}.png')\n",
    "            plt.savefig(matrix_path)\n",
    "            plt.close()\n",
    "        \n",
    "        # # 验证阶段\n",
    "        # model.eval()\n",
    "        # val_loss = 0.0\n",
    "        # val_task_losses = [0.0] * configs.num_tasks\n",
    "        # val_task_corrects = [0.0] * configs.num_tasks\n",
    "        # val_task_preds = [[] for _ in range(configs.num_tasks)]\n",
    "        # val_task_labels = [[] for _ in range(configs.num_tasks)]\n",
    "\n",
    "        # with torch.no_grad():\n",
    "        #     for input_ids, attention_mask, labels in val_dataloader:\n",
    "        #         # 前向传播\n",
    "        #         outputs = model(input_ids, attention_mask)\n",
    "        #         logits_list = outputs['logits']\n",
    "        #         loss_weights = outputs['loss_weights']\n",
    "                \n",
    "        #         # 计算每个任务的损失\n",
    "        #         for task_idx in range(configs.num_tasks):\n",
    "        #             task_labels = labels[:, task_idx].long()\n",
    "        #             task_logits = logits_list[task_idx]\n",
    "                    \n",
    "        #             task_loss = criterion(task_logits, task_labels)\n",
    "        #             val_task_losses[task_idx] += task_loss.item()\n",
    "                    \n",
    "        #             task_preds = task_logits.argmax(dim=1)\n",
    "        #             task_corrects = (task_preds == task_labels).float().sum()\n",
    "        #             val_task_corrects[task_idx] += task_corrects.item()\n",
    "                    \n",
    "        #             # 收集预测结果和真实标签\n",
    "        #             val_task_preds[task_idx].extend(task_preds.cpu().numpy())\n",
    "        #             val_task_labels[task_idx].extend(task_labels.cpu().numpy())\n",
    "                \n",
    "        #         # 计算加权总损失\n",
    "        #         val_batch_losses = [criterion(logits_list[i], labels[:, i].long()) for i in range(configs.num_tasks)]\n",
    "        #         weighted_losses = [loss * weight for loss, weight in zip(val_batch_losses, loss_weights)]\n",
    "        #         val_loss += sum(weighted_losses).item()\n",
    "        \n",
    "        # # 计算验证损失和准确率\n",
    "        # val_loss = val_loss / len(val_dataloader)\n",
    "        # val_task_losses = [loss / len(val_dataloader) for loss in val_task_losses]\n",
    "        # val_task_accs = [correct / len(val_dataset) for correct in val_task_corrects]\n",
    "        \n",
    "        # # 计算每个任务的F1分数\n",
    "        # val_task_f1s = [\n",
    "        #     f1_score(labels, preds, average='macro') \n",
    "        #     for labels, preds in zip(val_task_labels, val_task_preds)\n",
    "        # ]\n",
    "        \n",
    "        # # 打印验证结果\n",
    "        # print(f'Validation Loss: {val_loss:.4f}')\n",
    "        # for task_idx, task_name in enumerate(task_names):\n",
    "        #     print(f'Task {task_name} - Loss: {val_task_losses[task_idx]:.4f}, '\n",
    "        #           f'Acc: {val_task_accs[task_idx]:.4f}, '\n",
    "        #           f'F1: {val_task_f1s[task_idx]:.4f}')\n",
    "        \n",
    "        # # 为每个任务创建验证集混淆矩阵\n",
    "        # for task_idx, task_name in enumerate(task_names):\n",
    "        #     # 创建完整的三分类混淆矩阵\n",
    "        #     cm_full = confusion_matrix(\n",
    "        #         val_task_labels[task_idx], \n",
    "        #         val_task_preds[task_idx], \n",
    "        #         labels=[0, 1, 2]\n",
    "        #     )\n",
    "        #     plt.figure(figsize=(10, 8))\n",
    "        #     sns.heatmap(cm_full, annot=True, fmt='d', cmap='Blues',\n",
    "        #                 xticklabels=class_names,\n",
    "        #                 yticklabels=class_names)\n",
    "        #     plt.xlabel('Predicted')\n",
    "        #     plt.ylabel('True')\n",
    "        #     plt.title(f'Val: {task_name} Confusion Matrix\\n'\n",
    "        #               f'Acc: {val_task_accs[task_idx]:.4f}, '\n",
    "        #               f'F1: {val_task_f1s[task_idx]:.4f}')\n",
    "            \n",
    "        #     # 保存完整混淆矩阵\n",
    "        #     matrix_path = os.path.join(plot_dirs[task_name]['val'], f'cm_full_epoch_{epoch+1}.png')\n",
    "        #     plt.savefig(matrix_path)\n",
    "        #     plt.close()\n",
    "        \n",
    "        # # 检查是否保存模型，使用Track 4 (Actionability)的F1分数作为主要指标\n",
    "        # # 这里我们特别关注Track 4的性能\n",
    "        # track4_idx = 3  # Track 4的索引\n",
    "        # if val_task_f1s[track4_idx] > best_val_f1:\n",
    "        #     best_val_f1 = val_task_f1s[track4_idx]\n",
    "        #     best_val_acc = val_task_accs[track4_idx]\n",
    "            \n",
    "        #     # 保存模型\n",
    "        #     state_dict = model.state_dict()\n",
    "        #     torch.save(state_dict, os.path.join(checkpoint_dir, 'best_model_f1.pt'))\n",
    "        #     print(f'New best model saved with Track 4 F1: {best_val_f1:.4f}, Acc: {best_val_acc:.4f}')\n",
    "            \n",
    "        #     patience_counter = 0\n",
    "        # else:\n",
    "        #     patience_counter += 1\n",
    "        #     if patience_counter >= configs.patience:\n",
    "        #         print(f'Early stopping triggered after {epoch+1} epochs.')\n",
    "        #         break\n",
    "\n",
    "        # model.train()\n",
    "        \n",
    "        # 保存每个epoch的损失权重\n",
    "        np.save(\n",
    "            os.path.join(checkpoint_dir, f'loss_weights_epoch_{epoch+1}.npy'), \n",
    "            loss_weights.detach().cpu().numpy()\n",
    "        )\n",
    "\n",
    "        if epoch == 22 :\n",
    "            state_dict = model.state_dict()\n",
    "            torch.save(state_dict, os.path.join(checkpoint_dir, 'best_model_f1.pt'))\n",
    "            break\n",
    "        \n",
    "# 在以下主函数中添加判断Jupyter环境的逻辑\n",
    "if __name__ == '__main__':\n",
    "    # 判断是否在Jupyter环境中运行\n",
    "    try:\n",
    "        # 检查是否在Jupyter中运行\n",
    "        get_ipython = globals().get('get_ipython', None)\n",
    "        if get_ipython and 'IPKernelApp' in get_ipython().config:\n",
    "            # 在Jupyter环境中运行，使用默认配置\n",
    "            print(\"Running in Jupyter environment, using default configs\")\n",
    "            configs = get_default_configs()\n",
    "        else:\n",
    "            # 在命令行环境中运行，使用argparse\n",
    "            configs = argparser()\n",
    "    except:\n",
    "        # 任何异常都使用argparse处理\n",
    "        configs = argparser()\n",
    "    \n",
    "    # 设置实验名称\n",
    "    if configs.name is None:\n",
    "        configs.exp_name = \\\n",
    "            f'{os.path.basename(configs.model_name)}' + \\\n",
    "            f'{\"_fp\" if configs.freeze_pooler else \"\"}' + \\\n",
    "            f'_b{configs.batch_size}_e{configs.epochs}' + \\\n",
    "            f'_len{configs.max_length}_lr{configs.lr}'\n",
    "    else:\n",
    "        configs.exp_name = configs.name\n",
    "    \n",
    "    # 设置设备\n",
    "    if configs.device is None:\n",
    "        configs.device = torch.device(\n",
    "            'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        )\n",
    "    \n",
    "    # 调用训练函数\n",
    "    train(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
